{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Sapsan!","text":"<p>Sapsan is a pipeline for Machine Learning (ML) based turbulence modeling. While turbulence is important in a wide range of mediums, the pipeline primarily focuses on astrophysical application. With Sapsan, one can create their own custom models or use either conventional or physics-informed ML approaches for turbulence modeling included with the pipeline (estimators). For example, Sapsan features ML models in its set of tools to accurately capture the turbulent nature applicable to Core-Collapse Supernovae.</p>","tags":["astro","supernovae","CCSN","turbulence","machine learning","los alamos","LANL","ucsc"]},{"location":"#purpose","title":"Purpose","text":"<p>Sapsan takes out all the hard work from data preparation and analysis in turbulence  and astrophysical applications, leaving you focused on ML model design, layer by layer.</p>","tags":["astro","supernovae","CCSN","turbulence","machine learning","los alamos","LANL","ucsc"]},{"location":"#website","title":"Website","text":"<p>Check out a website version with a few examples at sapsan.app. The interface is identical to the GUI of the local version of Sapsan, except lacking the ability to edit the model code on the fly and to use mlflow for tracking.</p>","tags":["astro","supernovae","CCSN","turbulence","machine learning","los alamos","LANL","ucsc"]},{"location":"#news-and-publications","title":"News and Publications","text":"<p>Physics-Informed Machine Learning for Modeling Turbulence in Supernovae  Astrophysical Journal (ApJ) - 2022</p> <p>Sapsan: Framework for Supernovae Turbulence Modeling with Machine Learning  Journal of Open Source Software (JOSS) - November 26, 2021</p> <p>Provectus Brings Machine Learning to Numerical Astrophysics, Helping Simulate Turbulence in Supernovae Models Provectus IT Press Release - March 9, 2021</p> <p>Machine Learning for Supernova Turbulence  Society for Industrial and Applied Mathematics (SIAM) News (CSE21) - March 4, 2021</p> License <p>Sapsan has a BSD-style license, as found in the LICENSE file.</p> <p>\u00a9 (or copyright) 2019. Triad National Security, LLC. All rights reserved. This program was produced under U.S. Government contract 89233218CNA000001 for Los Alamos National Laboratory (LANL), which is operated by Triad National Security, LLC for the U.S. Department of Energy/National Nuclear Security Administration. All rights in the program are reserved by Triad National Security, LLC, and the U.S. Department of Energy/National Nuclear Security Administration. The Government is granted for itself and others acting on its behalf a nonexclusive, paid-up, irrevocable worldwide license in this material to reproduce, prepare derivative works, distribute copies to the public, perform publicly and display publicly, and to permit others to do so.</p>","tags":["astro","supernovae","CCSN","turbulence","machine learning","los alamos","LANL","ucsc"]},{"location":"api/","title":"API Reference","text":""},{"location":"api/#glossary","title":"Glossary","text":"Variable Definition N # of Batches Cin # of input channels (i.e. features) D or Db Data or Batch depth (z) H or Hb Data or Batch height (y) W or Wb Data or Batch width (x)"},{"location":"api/#trainevaluate","title":"Train/Evaluate","text":""},{"location":"api/#train","title":"Train","text":"<p>CLASS</p> <p><code>sapsan.lib.experiments.train.Train</code><code>(model: Estimator, data_parameters: dict, backend = FakeBackend(), show_log = True, run_name = 'train')</code></p> Call <code>Train</code> to set up your run     Parameters Name Type Discription Default <code>model</code> object model to use for training <code>data_parameters</code> dict data parameters from the data loader, necessary for tracking <code>backend</code> object backend to track the experiment FakeBackend() <code>show_log</code> bool show the loss vs. epoch progress plot (it will be save in mlflow in either case) True <code>run_name</code> str 'run name' tag as recorded under MLflow train <p><code>sapsan.lib.experiments.train.Train.run()</code></p> Run the model Return Type Description pytorch or sklearn or custom type trained model"},{"location":"api/#evaluate","title":"Evaluate","text":"<p>CLASS</p> <p><code>sapsan.lib.experiments.evaluate.Evaluate</code><code>(model: Estimator, data_parameters: dict, backend = FakeBackend(), cmap: str = 'plasma', run_name: str = 'evaluate', **kwargs)</code></p> Call <code>Evaluate</code> to set up the testing of the trained model. Don't forget to update <code>estimator.loaders</code> with the new data for testing. Parameters Name Type Discription Default <code>model</code> object model to use for testing <code>data_parameters</code> dict data parameters from the data loader, necessary for tracking <code>backend</code> obejct backend to track the experiment FakeBackend() <code>cmap</code> str matplotlib colormap to use for slice plots plasma <code>run_name</code> str 'run name' tag as recorded under MLflow evaluate <code>pdf_xlim</code> tuple x-axis limits for the PDF plot <code>pdf_ylim</code> tuple y-axis limits for the PDF plot <p><code>sapsan.lib.experiments.evaluate.Evaluate.run()</code></p> Run the evaluation of the trained model Return Type Description dict{'target' : np.ndarray, 'predict' : np.ndarray} target and predicted data"},{"location":"api/#estimators","title":"Estimators","text":""},{"location":"api/#cnn3d","title":"CNN3d","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.CNN3d</code><code>(loaders, config, model)</code></p> A model based on Pytorch's 3D Convolutional Neural Network Parameters Name Type Discription Default <code>loaders</code> dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) CNN3dConfig() <code>config</code> class configuration to use for the model CNN3dConfig() <code>model</code> class the model itself - should not be adjusted CNN3dModel() <p><code>sapsan.lib.estimator.CNN3d.save</code><code>(path: str)</code></p> Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <p><code>sapsan.lib.estimator.CNN3d.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing <code>n_epochs</code> to keep training the model further. <code>load_saved_config</code> bool updates config parameters from <code>{path}/params.json</code>. False <p>Return</p> Type Description pytorch model loaded model <p>CLASS</p> <p><code>sapsan.lib.estimator.CNN3dConfig</code><code>(n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs)</code></p> Configuration for the CNN3d - based on pytorch and catalyst libraries Parameters Name Type Discription Default <code>n_epochs</code> int number of epochs 1 <code>patience</code> int number of epochs with no improvement after which training will be stopped. Default 10 <code>min_delta</code> float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement 1e-5 <code>log_dir</code> int path to store the logs ./logs/ <code>lr</code> float learning rate 1e-3 <code>min_lr</code> float a lower bound of the learning rate  for ReduceLROnPlateau lr*1e-2 <code>device</code> str specify the device to run the model on cuda (or switch to cpu) <code>loader_key</code> str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' <code>metric_key</code> str the metric to use for early stop 'loss' <code>ddp</code> bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs.  This is passed to Catalyst to activate the <code>ddp</code> flag in <code>runner</code> (see more Distributed Training Tutorial; the <code>runner</code> is set up in pytorch_estimator.py). Note: doesn't support jupyter notebooks - prepare a script! False"},{"location":"api/#pimlturb","title":"PIMLTurb","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.PIMLTurb</code><code>(activ, loss, loaders, ks_stop, ks_frac, ks_scale, l1_scale, l1_beta, sigma, config, model)</code></p> Physics-informed machine learning model to predict Reynolds-like stress tensor, \\(Re\\), for turbulence modeling. Learn more on the wiki: PIMLTurb A custom loss function was developed for this model combining spatial (SmoothL1) and statistical (Kolmogorov-Smirnov) losses. Parameters Name Type Discription Default <code>activ</code> str activation function to use from PyTorch Tanhshrink <code>loss</code> str loss function to use; accepts only custom SmoothL1_KSLoss <code>loaders</code> dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) <code>ks_stop</code> float early-stopping condition based on the KS loss value alone 0.1 <code>ks_frac</code> float fraction the KS loss contributes to the total loss 0.5 <code>ks_scale</code> float scale factor to prioritize KS loss over SmoothL1 (should not be altered) 1 <code>l1_scale</code> float scale factor to prioritize SmoothL1 loss over KS 1 <code>l1_beta</code> float \\(beta\\) threshold for smoothing the L1 loss 1 <code>sigma</code> float \\(sigma\\) for the last layer of the network that performs a filtering operation using a Gaussian kernel 1 <code>config</code> class configuration to use for the model PIMLTurbConfig() <code>model</code> class the model itself - should not be adjusted PIMLTurbModel() <p><code>sapsan.lib.estimator.PIMLTurb.save</code><code>(path: str)</code></p> Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <p><code>sapsan.lib.estimator.PIMLTurb.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing <code>n_epochs</code> to keep training the model further. <code>load_saved_config</code> bool updates config parameters from <code>{path}/params.json</code>. False <p>Return</p> Type Description pytorch model loaded model <p>CLASS</p> <p><code>sapsan.lib.estimator.PIMLTurbConfig</code><code>(n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs)</code></p> Configuration for the PIMLTurb - based on pytorch (catalyst is not used) Parameters Name Type Discription Default <code>n_epochs</code> int number of epochs 1 <code>patience</code> int number of epochs with no improvement after which training will be stopped (not used) 10 <code>min_delta</code> float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement (not used) 1e-5 <code>log_dir</code> int path to store the logs ./logs/ <code>lr</code> float learning rate 1e-3 <code>min_lr</code> float a lower bound of the learning rate  for ReduceLROnPlateau lr*1e-2 <code>device</code> str specify the device to run the model on cuda (or switch to cpu)"},{"location":"api/#pimlturb1d","title":"PIMLTurb1D","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.PIMLTurb1D</code><code>(activ, loss, loaders, ks_stop, ks_frac, ks_scale, l1_scale, l1_beta, sigma, config, model)</code></p> Physics-informed machine learning model to predict Reynolds-like stress tensor, \\(Re\\), for turbulence modeling. Learn more on the wiki: PIMLTurb A custom loss function was developed for this model combining spatial (SmoothL1) and statistical (Kolmogorov-Smirnov) losses. Parameters Name Type Discription Default <code>activ</code> str activation function to use from PyTorch Tanhshrink <code>loss</code> str loss function to use; accepts only custom SmoothL1_KSLoss <code>loaders</code> dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) <code>ks_stop</code> float early-stopping condition based on the KS loss value alone 0.1 <code>ks_frac</code> float fraction the KS loss contributes to the total loss 0.5 <code>ks_scale</code> float scale factor to prioritize KS loss over SmoothL1 (should not be altered) 1 <code>l1_scale</code> float scale factor to prioritize SmoothL1 loss over KS 1 <code>l1_beta</code> float \\(beta\\) threshold for smoothing the L1 loss 1 <code>sigma</code> float \\(sigma\\) for the last layer of the network that performs a filtering operation using a Gaussian kernel 1 <code>config</code> class configuration to use for the model PIMLTurb1DConfig() <code>model</code> class the model itself - should not be adjusted PIMLTurb1DModel() <p><code>sapsan.lib.estimator.PIMLTurb1D.save</code><code>(path: str)</code></p> Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <p><code>sapsan.lib.estimator.PIMLTurb1D.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing <code>n_epochs</code> to keep training the model further. <code>load_saved_config</code> bool updates config parameters from <code>{path}/params.json</code>. False <p>Return</p> Type Description pytorch model loaded model <p>CLASS</p> <p><code>sapsan.lib.estimator.PIMLTurb1DConfig</code><code>(n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs)</code></p> Configuration for the PIMLTurb1D - based on pytorch (catalyst is not used) Parameters Name Type Discription Default <code>n_epochs</code> int number of epochs 1 <code>patience</code> int number of epochs with no improvement after which training will be stopped (not used) 10 <code>min_delta</code> float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement (not used) 1e-5 <code>log_dir</code> int path to store the logs ./logs/ <code>lr</code> float learning rate 1e-3 <code>min_lr</code> float a lower bound of the learning rate  for ReduceLROnPlateau lr*1e-2 <code>device</code> str specify the device to run the model on cuda (or switch to cpu)"},{"location":"api/#picae","title":"PICAE","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.PICAE</code><code>(loaders, config, model)</code></p> Convolutional Auto Encoder with Divergence-Free Kernel and with periodic padding. Further details can be found on the PICAE page Parameters Name Type Discription Default <code>loaders</code> dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) <code>config</code> class configuration to use for the model PICAEConfig() <code>model</code> class the model itself - should not be adjusted PICAEModel() <p><code>sapsan.lib.estimator.PICAE.save</code><code>(path: str)</code></p> Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <p><code>sapsan.lib.estimator.PICAE.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing <code>n_epochs</code> to keep training the model further. <code>load_saved_config</code>&gt; bool updates config parameters from <code>{path}/params.json</code> False <p>Return</p> Type Description pytorch model loaded model <p>CLASS</p> <p><code>sapsan.lib.estimator.PICAEConfig</code><code>(n_epochs, patience, min_delta, logdir, lr, min_lr, weight_decay, nfilters, kernel_size, enc_nlayers, dec_nlayers, *args, **kwargs)</code></p> Configuration for the CNN3d - based on pytorch and catalyst libraries Parameters Name Type Discription Default <code>n_epochs</code> int number of epochs 1 <code>batch_dim</code> int dimension of a batch in each axis 64 <code>patience</code> int number of epochs with no improvement after which training will be stopped 10 <code>min_delta</code> float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement 1e-5 <code>log_dir</code> str path to store the logs ./logs/ <code>lr</code> float learning rate 1e-3 <code>min_lr</code> float a lower bound of the learning rate  for ReduceLROnPlateau lr*1e-2 <code>weight_decay</code> float weight decay (L2 penalty) 1e-5 <code>nfilters</code> int the output dim for each convolutional layer, which is the number of \"filters\" learned by that layer 6 <code>kernel_size</code> tuple size of the convolutional kernel (3,3,3) <code>enc_layers</code> int number of encoding layers 3 <code>dec_layers</code> int number of decoding layers 3 <code>device</code> str specify the device to run the model on cuda (or switch to cpu) <code>loader_key</code> str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' <code>metric_key</code> str the metric to use for early stop 'loss' <code>ddp</code> bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs.  This is passed to Catalyst to activate the <code>ddp</code> flag in <code>runner</code> (see more Distributed Training Tutorial; the <code>runner</code> is set up in pytorch_estimator.py). Note: doesn't support jupyter notebooks - prepare a script! False"},{"location":"api/#krr","title":"KRR","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.KRR</code><code>(loaders, config, model)</code></p> A model based on sk-learn Kernel Ridge Regression Parameters Name Type Discription Default <code>loaders</code> list contains input and target data <code>config</code> class configuration to use for the model KRRConfig() <code>model</code> class the model itself - should not be adjusted KRRModel() <p><code>sapsan.lib.estimator.KRR.save</code><code>(path: str)</code></p> Saves the model Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <p><code>sapsan.lib.estimator.KRR.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads the model Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing <code>n_epochs</code> to keep training the model further. <code>load_saved_config</code> bool updates config parameters from <code>{path}/params.json</code> False <p>Return</p> Type Description sklearn model loaded model <p>CLASS</p> <p><code>sapsan.lib.estimator.KRRConfig</code><code>(alpha, gamma)</code></p> Configuration for the KRR model Parameters Name Type Discription Default <code>alpha</code> float regularization term, hyperparameter None <code>gamma</code> float full-width at half-max for the RBF kernel, hyperparameter None"},{"location":"api/#load_estimator","title":"load_estimator","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.load_estimator</code><code>()</code></p> Dummy estimator to call <code>load()</code> to load the saved pytorch models <p><code>sapsan.lib.estimator.load_estimator.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing <code>n_epochs</code> to keep training the model further <code>load_saved_config</code> bool updates config parameters from <code>{path}/params.json</code> False <p>Return</p> Type Description pytorch model loaded model"},{"location":"api/#load_sklearn_estimator","title":"load_sklearn_estimator","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.load_sklearn_estimator</code><code>()</code></p> Dummy estimator to call <code>load()</code> to load the saved sklearn models <p><code>sapsan.lib.estimator.load_sklearn_estimator.load</code><code>(path: str, estimator, load_saved_config = False)</code></p> Loads model Parameters Name Type Discription Default <code>path</code> str save path of the model and its config parameters, <code>{path}/model.pt</code> and <code>{path}/params.json</code> respectively <code>estimator</code> estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup to keep training the model further <code>load_saved_config</code> bool updates config parameters from <code>{path}/params.json</code> False <p>Return</p> Type Description sklearn model loaded model"},{"location":"api/#torch-modules","title":"Torch Modules","text":""},{"location":"api/#gaussian","title":"Gaussian","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.torch_modules.Gaussian</code><code>(sigma: int)</code></p> [1D,3D] Applies a Guassian filter as a torch layer through a series of 3 separable 1D convolutions, utilizing torch.nn.funcitonal.conv3d. CUDA is supported. Parameters Name Type Discription Default <code>sigma</code> int standard deviation \\(\\sigma\\) for a Gaussian kernel 2 <p><code>sapsan.lib.estimator.torch_modules.Gaussian.forward</code><code>(tensor: torch.tensor)</code></p> Parameters Name Type Discription Default <code>tensor</code> torch.tensor input torch tensor of shape  [N, Cin, Din, Hin, Win] <p>Return</p> Type Description torch.tensor filtered 3D torch data"},{"location":"api/#interp1d","title":"Interp1d","text":"<p>CLASS</p> <p><code>sapsan.lib.estimator.torch_modules.Interp1d</code><code>()</code></p> Linear 1D interpolation done in native PyTorch. CUDA is supported. Forked from @aliutkus <p><code>sapsan.lib.estimator.torch_modules.Interp1d.forward</code><code>(x: torch.tensor, y: torch.tensor, xnew: torch.tensor, out: torch.tensor)</code></p> Parameters Name Type Discription Default <code>x</code> torch.tensor 1D or 2D tensor <code>y</code> torch.tensor 1D or 2D tensor; the length of <code>y</code> along its last dimension must be the same as that of <code>x</code> <code>xnew</code> torch.tensor 1D or 2D tensor of real values. <code>xnew</code> can only be 1D if both <code>x</code> and <code>y</code> are 1D. Otherwise, its length along the first dimension must be the same as that of whichever <code>x</code> and <code>y</code> is 2D. <code>out</code> torch.tensor Tensor for the output If None, allocated automatically <p>Return</p> Type Description torch.tensor interpolated tensor"},{"location":"api/#data-loaders","title":"Data Loaders","text":""},{"location":"api/#hdf5dataset","title":"HDF5Dataset","text":"<p>CLASS</p> <p><code>sapsan.lib.data.hdf5_dataset.HDF5Dataset</code><code>( path: str, features: List[str], target: List[str], checkpoints: List[int], batch_size: int = None, input_size: int = None, sampler: Optional[Sampling] = None, time_granularity: float = 1, features_label: Optional[List[str]] = None, target_label: Optional[List[str]] = None, flat: bool = False, shuffle: bool=False, train_fraction = None)</code></p> HDF5 data loader class Parameters Name Type Discription Default <code>path</code> str path to the data in the following format: <code>\"data/t_{checkpoint:1.0f}/{feature}_data.h5\"</code> <code>features</code> List[str] list of train features to load ['not_specified_data'] <code>target</code> List[str] list of target features to load None <code>checkpoints</code> List[int] list of checkpoints to load (they will be appended as batches) <code>input_size</code> int dimension of the loaded data in each axis <code>batch_size</code> int dimension of a batch in each axis. If batch_size != input_size, the datacube will be evenly splitted input_size (doesn't work with sampler) <code>batch_num</code> int the number of batches to be loaded at a time 1 <code>sampler</code> object data sampler to use (ex: EquidistantSampling()) None <code>time_granularity</code> float what is the time separation (dt) between checkpoints 1 <code>features_label</code> List[str] hdf5 data label for the train features list(file.keys())[-1], i.e. last one in hdf5 file <code>target_label</code> List[str] hdf5 data label for the target features list(file.keys())[-1], i.e. last one in hdf5 file <code>flat</code> bool flatten the data into [Cin, D*H*W]. Required for sk-learn models False <code>shuffle</code> bool shuffle the dataset False <code>train_fraction</code> float or int a fraction of the dataset to be used for training (accessed through loaders['train']). The rest will be used for validation (accessed through loaders['valid']). If int is provided, then that number of batches will be used for training. If float is provided, then it will try to split the data either by batch or by actually slicing the data cube into smaller chunks None - training data will be used for validation, effectively skipping the latter <p><code>sapsan.lib.data.hdf5_dataset.HDF5Dataset.load_numpy()</code></p> HDF5 data loader method - call it to load the data as a numpy array. If targets are not specified, than only features will be loaded (hence you can just load 1 dataset at a time). Return Type Description np.ndarray, np.ndarray loaded a dataset as a numpy array <p><code>sapsan.lib.data.hdf5_dataset.HDF5Dataset.convert_to_torch([x, y])</code></p> Splits numpy arrays into batches and converts to torch dataloader Parameters Name Type Discription Default <code>[x, y]</code> list or np.ndarray a list of input datasets to batch and convert to torch loaders <p>Return</p> Type Description OrderedDict{'train' : DataLoader, 'valid' : DataLoader } Data in Torch Dataloader format ready for training <p><code>sapsan.lib.data.hdf5_dataset.HDF5Dataset.load()</code></p> Loads, splits into batches, and converts into torch dataloader. Effectively combines .load_numpy and .convert_to_torch Return Type Description np.ndarray, np.ndarray loaded train and target features: x, y"},{"location":"api/#get_loader_shape","title":"get_loader_shape","text":"<p><code>sapsan.lib.data.data_functions.get_loader_shape()</code></p> Returns the shape of the loaded tensors - the loaded data that has been split into <code>train</code> and <code>valid</code> datasets. Parameters Name Type Discription Default <code>loaders</code> torch DataLoader the loader of tensors passed for training <code>name</code> str name of the dataset in the loaders; usually either <code>train</code> or <code>valid</code> None - chooses the first entry in loaders <p>Return</p> Type Description np.ndarray shape of the tensor"},{"location":"api/#data-manipulation","title":"Data Manipulation","text":""},{"location":"api/#equidistantsampling","title":"EquidistantSampling","text":"<p>CLASS</p> <p><code>sapsan.lib.data.sampling.EquidistantSampling</code><code>(target_dim)</code></p> Samples the data to a lower dimension, keeping separation between the data points equally distant Parameters Name Type Discription Default <code>target_dim</code> np.ndarray new shape of the input in the form [D, H, W] <p><code>sapsan.lib.data.sampling.EquidistantSampling.sample</code><code>(data)</code></p> Performs sampling of the data Parameters Name Type Discription Default <code>data</code> np.ndarray input data to be sampled - has the shape of [axis, D, H, W] <p>Return</p> Type Description np.ndarray Sampled data with the shape [axis, D, H, W]"},{"location":"api/#split_data_by_batch","title":"split_data_by_batch","text":"<p><code>sapsan.utils.shapes.split_data_by_batch</code><code>(data: np.ndarray, size: int, batch_size: int, n_features: int, axis: int)</code></p> [2D, 3D]: splits data into smaller cubes or squares of batches Parameters Name Type Discription Default <code>data</code> np.ndarray input 2D or 3D data, [Cin, D, H, W] <code>size</code> int dimensionality of the data in each axis <code>batch_size</code> int dimensionality of the batch in each axis <code>n_features</code> int number of channels of the input data <code>axis</code> int number of axes, 2 or 3 <p>Return</p> Type Description np.ndarray batched data: [N, Cin, Db, Hb, Wb]"},{"location":"api/#combine_data","title":"combine_data","text":"<p><code>sapsan.utils.shapes.combine_data</code><code>(data: np.ndarray, input_size: tuple, batch_size: tuple, axis: int)</code></p> [2D, 3D] - reverse of <code>split_data_by_batch</code> function Parameters Name Type Discription Default <code>data</code> np.ndarray input 2D or 3D data, [N, Cin, Db, Hb, Wb] <code>input_size</code> tuple dimensionality of the original data in each axis <code>batch_size</code> tuple dimensionality of the batch in each axis <code>axis</code> int number of axes, 2 or 3 <p>Return</p> Type Description np.ndarray reassembled data: [Cin, D, H, W]"},{"location":"api/#slice_of_cube","title":"slice_of_cube","text":"<p><code>sapsan.utils.shapes.slice_of_cube</code><code>(data: np.ndarray, feature: Optional[int] = None, n_slice: Optional[int] = None)</code></p> Select a slice of a cube (to plot later) Parameters Name Type Discription Default <code>data</code> np.ndarray input 3D data, [Cin, D, H, W] <code>feature</code> int feature to take the slice of, i.e. the value of Cin 1 <code>n_slice</code> int what slice to select, i.e. the value of D 1 <p>Return</p> Type Description np.ndarray data slice: [H, W]"},{"location":"api/#filter","title":"Filter","text":""},{"location":"api/#spectral","title":"spectral","text":"<p><code>sapsan.utils.filter.spectral</code><code>(im: np.ndarray, fm: int)</code></p> [2D, 3D] apply a spectral filter Parameters Name Type Discription Default <code>im</code> np.ndarray input dataset (ex: [Cin, D, H, W]) <code>fm</code> int number of Fourier modes to filter down to <p>Return</p> Type Description np.ndarray filtered dataset"},{"location":"api/#box","title":"box","text":"<p><code>sapsan.utils.filter.box</code><code>(im: np.ndarray, ksize)</code></p> [2D] apply a box filter Parameters Name Type Discription Default <code>im</code> np.ndarray input dataset (ex: [Cin, H, W]) <code>ksize</code> tupple kernel size (ex: ksize = (2,2)) <p>Return</p> Type Description np.ndarray filtered dataset"},{"location":"api/#gaussian_1","title":"gaussian","text":"<p><code>sapsan.utils.filter.gaussian</code><code>(im: np.ndarray, sigma)</code></p> [2D, 3D] apply a gaussian filter Note: Guassian filter assumes dx=1 between the points. Adjust sigma accordingly. Parameters Name Type Discription Default <code>im</code> np.ndarray input dataset (ex: [H, W] or [D, H, W]) <code>sigma</code> float or tuple of floats standard deviation for Gaussian kernel. Sigma can be defined for each axis individually. <p>Return</p> Type Description np.ndarray filtered dataset"},{"location":"api/#backend-tracking","title":"Backend (Tracking)","text":""},{"location":"api/#mlflowbackend","title":"MLflowBackend","text":"<p>CLASS</p> <p><code>sapsan.lib.backends.mlflow.MLflowBackend</code><code>(name, host, port)</code></p> Initilizes mlflow and starts up mlflow ui at a given host:port Parameters Name Type Discription Default <code>name</code> str name under which to record the experiment \"experiment\" <code>host</code> str host of mlflow ui \"localhost\" <code>port</code> int port of mlflow ui 5000 <p><code>sapsan.lib.backends.mlflow.MLflowBackend.start_ui</code><code>()</code></p> starts MLflow ui at a specified host and port <p><code>sapsan.lib.backends.mlflow.MLflowBackend.start</code><code>(run_name: str, nested = False, run_id = None)</code></p> Starts a tracking run Parameters Name Type Discription Default <code>run_name</code> str name of the run \"train\" for <code>Train()</code>, \"evaluate\" for <code>Evaluate()</code> <code>nested</code> bool whether or not to nest the recorded run False for <code>Train()</code>, True for <code>Evaluate()</code> <code>run_id</code> str run id None - a new will be generated <p>Return</p> Type Description str run_id <p><code>sapsan.lib.backends.mlflow.MLflowBackend.resume</code><code>(run_id, nested = True)</code></p> Resumes a previous run, so you can record extra parameters Parameters Name Type Discription Default <code>run_id</code> str id of the run to resume <code>nested</code> bool whether or not to nest the recorded run True, since it will usually be an <code>Evaluate()</code> run <p><code>sapsan.lib.backends.mlflow.MLflowBackend.log_metric</code><code>()</code></p> Logs a metric <p><code>sapsan.lib.backends.mlflow.MLflowBackend.log_parameter</code><code>()</code></p> Logs a parameter <p><code>sapsan.lib.backends.mlflow.MLflowBackend.log_artifact</code><code>()</code></p> Logs an artifact (any saved file such, e.g. .png, .txt) <p><code>sapsan.lib.backends.mlflow.MLflowBackend.log_model</code><code>()</code></p> Log a PyTorch model as an MLflow artifact for the current run. Corresponds to mlflow.pytorch.log_model() <p><code>sapsan.lib.backends.mlflow.MLflowBackend.load_model</code><code>()</code></p> Load a PyTorch model from a local file or a run. Corresponds to mlflow.pytorch.load_model() <p><code>sapsan.lib.backends.mlflow.MLflowBackend.close_active_run</code><code>()</code></p> Closes all active MLflow runs <p><code>sapsan.lib.backends.mlflow.MLflowBackend.end</code><code>()</code></p> Ends the most recent MLflow run"},{"location":"api/#fakebackend","title":"FakeBackend","text":"<p>CLASS</p> <p><code>sapsan.lib.backends.fake.FakeBackend()</code></p> Pass to <code>train</code> in order to disable backend (tracking)"},{"location":"api/#plotting","title":"Plotting","text":""},{"location":"api/#plot_params","title":"plot_params","text":"<p><code>sapsan.utils.plot.plot_params()</code></p> Contains the matplotlib parameters that format all of the plots (<code>font.size</code>, <code>axes.labelsize</code>, etc.) Return Type Description dict matplotlib parameters Default Parameters <pre><code>def plot_params():\nparams = {'font.size': 14, 'legend.fontsize': 14, \n'axes.labelsize': 20, 'axes.titlesize': 24,\n'xtick.labelsize': 17,'ytick.labelsize': 17,\n'axes.linewidth': 1, 'patch.linewidth': 3, \n'lines.linewidth': 3,\n'xtick.major.width': 1.5,'ytick.major.width': 1.5,\n'xtick.minor.width': 1.25,'ytick.minor.width': 1.25,\n'xtick.major.size': 7,'ytick.major.size': 7,\n'xtick.minor.size': 4,'ytick.minor.size': 4,\n'xtick.direction': 'in','ytick.direction': 'in',              \n'axes.formatter.limits': [-7, 7],'axes.grid': True, \n'grid.linestyle': ':','grid.color': '#999999',\n'text.usetex': False,}\nreturn params\n</code></pre>"},{"location":"api/#pdf_plot","title":"pdf_plot","text":"<p><code>sapsan.utils.plot.pdf_plot</code><code>(series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, figsize: tuple, dpi: int, ax: matplotlib.axes, style: str)</code></p> Plot a probability density function (PDF) of a single or multiple datasets Parameters Name Type Discription Default <code>series</code> List[np.ndarray] input datasets <code>bins</code> int number of bins to use for the dataset to generate the pdf 100 <code>label</code> List[str] list of names to use as labels in the legend None <code>figsize</code> tuple figure size as passed to matplotlib figure (6,6) <code>dpi</code> int resolution of the figure 60 <code>ax</code> matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure <code>style</code> str accepts matplotlib styles 'tableau-colorblind10' <p>Return</p> Type Description matplotlib.axes object ax"},{"location":"api/#cdf_plot","title":"cdf_plot","text":"<p><code>sapsan.utils.plot.cdf_plot</code><code>(series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, figsize: tuple, dpi: int, ax: matplotlib.axes, ks: bool, style: str)</code></p> Plot a cumulative distribution function (CDF) of a single or multiple datasets Parameters Name Type Discription Default <code>series</code> List[np.ndarray] input datasets <code>bins</code> int number of bins to use for the dataset to generate the pdf 100 <code>label</code> List[str] list of names to use as labels in the legend None <code>figsize</code> tuple figure size as passed to matplotlib figure (6,6) <code>dpi</code> int resolution of the figure 60 <code>ax</code> matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure <code>ks</code> bool if True prints out on the plot itself the Kolomogorov-Smirnov Statistic. It will also be returned along with the ax object False <code>style</code> str accepts matplotlib styles 'tableau-colorblind10' <p>Return</p> Type Description matplotlib.axes object, float (if ks==True) ax, ks (if ks==True)"},{"location":"api/#line_plot","title":"line_plot","text":"<p><code>sapsan.utils.plot.line_plot</code><code>(series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, plot_type: str, figsize: tuple, dpi: int, ax: matplotlib.axes, style: str)</code></p> Plot linear data of x vs y - same matplotlib formatting will be used as the other plots Parameters Name Type Discription Default <code>series</code> List[np.ndarray] input datasets <code>bins</code> int number of bins to use for the dataset to generate the pdf 100 <code>label</code> List[str] list of names to use as labels in the legend None <code>plot_type</code> str axis type of the matplotlib plot; options = ['plot', 'semilogx', 'semilogy', 'loglog'] 'plot' <code>figsize</code> tuple figure size as passed to matplotlib figure (6,6) <code>linestyle</code> List[str] list of linestyles to use for each profile for the matplotlib figure ['-'] (solid line) <code>dpi</code> int resolution of the figure 60 <code>ax</code> matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure <code>style</code> str accepts matplotlib styles 'tableau-colorblind10' <p>Return</p> Type Description matplotlib.axes object ax"},{"location":"api/#slice_plot","title":"slice_plot","text":"<p><code>sapsan.utils.plot.slice_plot</code><code>(series: List[np.ndarray], label: Optional[List[str]] = None, cmap = 'plasma', figsize: tuple, dpi: int, ax: matplotlib.axes)</code></p> Plot 2D spatial distributions (slices) of your target and prediction datasets. Colorbar limits for both slices are set based on the minimum and maximum of the 2nd (target) provided dataset. Parameters Name Type Discription Default <code>series</code> List[np.ndarray] input datasets <code>label</code> List[str] list of names to use as labels in the legend None <code>cmap</code> str matplotlib colormap to use 'viridis' <code>figsize</code> tuple figure size as passed to matplotlib figure (6,6) <code>dpi</code> int resolution of the figure 60 <code>ax</code> matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots)  WARNING: only works if a single image is supplied to <code>slice_plot()</code>, otherwise will be ignored None - creates a separate figure <p>Return</p> Type Description matplotlib.axes object ax"},{"location":"api/#log_plot","title":"log_plot","text":"<p><code>sapsan.utils.plot.log_plot</code><code>(show_log = True, log_path = 'logs/logs/train.csv', valid_log_path = 'logs/logs/valid.csv', delimiter=',', train_name = 'train_loss', valid_name = 'valid_loss', train_column = 1, valid_column = 1, epoch_column = 0)</code></p> Plots an interactive training log of train_loss vs. epoch with plotly Parameters Name Type Discription Default <code>show_log</code> bool show the loss vs. epoch progress plot (it will be save in mlflow in either case) True <code>log_path</code> str path to training log produced by the catalyst wrapper 'logs/logs/train.csv' <code>valid_log_path</code> str path to validation log produced by the catalyst wrapper 'logs/logs/valid.csv' <code>delimiter</code> str delimiter to use for numpy.genfromtxt data loading ',' <code>train_name</code> str name for the training label 'train_loss' <code>valid_name</code> str name for the validation label 'valid_loss' <code>train_column</code> int column to load for training data from <code>log_path</code> 1 <code>valid_column</code> int column to load for validation data from <code>valid_log_path</code> 1 <code>epoch_column</code> int column to load the epoch index from <code>log_path</code>. If None, then epoch will be generated fro the number of entries 0 <p>Return</p> Type Description plotly.express object plot figure"},{"location":"api/#model_graph","title":"model_graph","text":"<p><code>sapsan.utils.plot.model_graph</code><code>(model, shape: np.array, transforms)</code></p> Creates a graph of the ML model (needs graphviz to be installed). A tutorial is available on the wiki: Model Graph The method is based on hiddenlayer originally written by Waleed Abdulla. Parameters Name Type Discription Default <code>model</code> object initialized pytorch or tensorflow model <code>shape</code> np.array shape of the input array in the form [N, Cin, Db, Hb, Wb], where Cin=1 <code>transforms</code> list[methods] a list of hiddenlayer transforms to be applied (Fold, FoldId, Prune, PruneBranch, FoldDuplicates, Rename), defined in transforms.py See below Default Parameters <pre><code>import sapsan.utils.hiddenlayer as hl\ntransforms = [\nhl.transforms.Fold(\"Conv &gt; MaxPool &gt; Relu\", \"ConvPoolRelu\"),\nhl.transforms.Fold(\"Conv &gt; MaxPool\", \"ConvPool\"),    \nhl.transforms.Prune(\"Shape\"),\nhl.transforms.Prune(\"Constant\"),\nhl.transforms.Prune(\"Gather\"),\nhl.transforms.Prune(\"Unsqueeze\"),\nhl.transforms.Prune(\"Concat\"),\nhl.transforms.Rename(\"Cast\", to=\"Input\"),\nhl.transforms.FoldDuplicates()\n]\n</code></pre> <p>Return</p> Type Description graphviz.Digraph object SVG graph of a model"},{"location":"api/#physics","title":"Physics","text":""},{"location":"api/#reynoldsstress","title":"ReynoldsStress","text":"<p><code>sapsan.utils.physics.ReynoldsStress</code><code>(u, filt, filt_size, only_x_components=False)</code></p> Calculates a stress tensor of the form  \\[ \\tau_{ij} = \\widetilde{u_i u_j} - \\tilde{u}_i\\tilde{u}_j \\] where \\(\\tilde{u}\\) is the filtered \\(u\\) Parameters Name Type Discription Default <code>u</code> np.ndarray input velocity in 3D - [axis, D, H, W] <code>filt</code> sapsan.utils.filters the type of filter to use (spectral, box, gaussian). Pass the filter itself by loading the appropriate one from <code>sapsan.utils.filters</code> gaussian <code>filt_size</code> int or float size of the filter to apply. For different filter types, the size is defined differently. Spectral - fourier mode to filter to, Box - k_size (box size), Gaussian - sigma 2 (sigma=2 for gaussian) <code>only_x_component</code> bool calculates and outputs only x components of the tensor in shape [row, D, H, W] - calculating all 9 can be taxing on memory False <p>Return</p> Type Description np.ndarray stress tensor of shape [column, row, D, H, W]"},{"location":"api/#powerspectrum","title":"PowerSpectrum","text":"<p>CLASS</p> <p><code>sapsan.utils.physics.PowerSpectrum</code><code>(u: np.ndarray)</code></p> Sets up to produce a power spectrum Parameters Name Type Discription Default <code>u</code> np.ndarray input velocity  (first dimension must be the axis=[1, 2, or 3],  e.g. the shape for 3D velocity should be: [axis, D, H, W]) <p><code>sapsan.utils.physics.PowerSpectrum.calculate()</code></p> Calculates the power spectrum Return Type Description np.ndarray, np.ndarray k_bins (fourier modes), Ek_bins (E(k)) <p><code>sapsan.utils.physics.PowerSpectrum.spectrum_plot</code><code>(k_bins, Ek_bins, kolmogorov=True, kl_a)</code></p> Plots the calculated power spectrum Parameters Name Type Discription Default <code>k_bins</code> np.ndarray fourier mode values along x-axis <code>Ek_bins</code> np.ndarray energy as a function of k: E(k) <code>kolmogorov</code> bool plots scaled Kolmogorov's -5/3 spectrum alongside the calculated one True <code>kl_A</code> float scaling factor of Kolmogorov's law np.amax(self.Ek_bins)*1e1 <p>Return</p> Type Description matplotlib.axes object spectrum plot"},{"location":"api/#gradientmodel","title":"GradientModel","text":"<p>CLASS</p> <p><code>sapsan.utils.physics.GradientModel</code><code>(u: np.ndarray, filter_width, delta_u = 1)</code></p> sets up to apply a gradient turbulence subgrid model:  \\[ \\tau_{ij} = \\frac{1}{12} \\Delta^2 \\,\\delta_k u^*_i \\,\\delta_k u^*_j \\] where \\(\\Delta\\) is the filter width and \\(u^*\\) is the filtered \\(u\\) Parameters Name Type Discription Default <code>u</code> np.ndarray input filtered quantity in 3D - [axis, D, H, W] <code>filter_width</code> float width of the filter which was applied onto <code>u</code> <code>delta_u</code> distance between the points on the grid to use for scaling 1 <p><code>sapsan.utils.physics.GradientModel.gradient()</code></p> calculated the gradient of the given input data from GradientModel Return Type Description np.ndarray gradient with shape [column, row, D, H, W] <p><code>sapsan.utils.physics.GradientModel.model()</code></p> calculates the gradient model tensor with shape [column, row, D, H, W] Return Type Description np.ndarray gradient model tensor"},{"location":"api/#dynamicsmagorinskymodel","title":"DynamicSmagorinskyModel","text":"<p>CLASS</p> <p><code>sapsan.utils.physics.DynamicSmagorinskyModel</code><code>(u: np.ndarray, filt, original_filt_size, filt_ratio, du, delta_u)</code></p> sets up to apply a Dynamic Smagorinsky (DS) turbulence subgrid model:  \\[ \\tau_{ij} = -2(C_s\\Delta^*)^2|S^*|S^*_{ij} \\] where \\(\\Delta\\) is the filter width and \\(S^*\\) is the filtered \\(u\\) Parameters Name Type Discription Default <code>u</code> np.ndarray input filtered quantity either in 3D [axis, D, H, W] or 2D [axis, D, H] <code>du</code> np.ndarray gradient of <code>u</code> None*: if <code>du</code> is not provided, then it will be calculated with <code>np.gradient()</code> <code>filt</code> sapsan.utils.filters the type of filter to use (spectral, box, gaussian). Pass the filter itself by loading the appropriate one from <code>sapsan.utils.filters</code> spectral <code>original_fil_size</code> int width of the filter which was applied onto <code>u</code> 15 (spectral, fourier modes = 15) <code>delta_u</code> float distance between the points on the grid to use for scaling 1 <code>filt_ratio</code> float the ratio of additional filter that will be applied on the data to find the slope for Dynamic Smagorinsky extrapolation over <code>original_filt_size</code> 0.5 <p><code>sapsan.utils.physics.DynamicSmagorinskyModel.model()</code></p> calculates the DS model tensor with shape [column, row, D, H, W] Return Type Description np.ndarray DS model tensor"},{"location":"details/estimators/","title":"Estimators","text":"<p>Sapsan has several models in its arsenal to get started.</p>","boost":15},{"location":"details/estimators/#convolution-neural-network-cnn","title":"Convolution Neural Network (CNN)","text":"<p>Example: cnn_example.ipynb  Estimator: cnn3d_estimator.py</p> <p>The network is based around Conv3d and MaxPool3d layers, reducing the spatial dimensions down to 1 by increasing the number of features. In order to do that, the network iterates over the following NN block:</p> <p><pre><code>def nn_block():\ntorch.nn.Conv3d(D_in, D_in*2, kernel_size=2, stride=2, padding=1)\ntorch.nn.MaxPool3d(kernel_size=2, padding=1)\n</code></pre> where D_in is the input dimension.</p> <p>As final layers, ReLU activation function is used and the data is linearized. An example model graph for the input data with the spatial dimensions of [16, 16, 16] split into 8 batches is provided below.</p>","boost":15},{"location":"details/estimators/#physics-informed-cnn-for-turbulence-modeling-pimlturb","title":"Physics-Informed CNN for Turbulence Modeling (PIMLTurb)","text":"<p>Example: pimlturb_diagonal_example.ipynb  Estimator: pimlturb_diagonal_estimator.py</p> <p>The estimator is based on Physics-Informed Machine Learning for Modeling Turbulence in Supernovae by P.I.Karpov et al. (2022). The model is based on a 3D convolutional network with some additions to enforce a realizability constraint (\\(Re_{ii} &gt; 0\\), where \\(Re\\) is the Reynolds stress tensor and \\(i\\) is the component index). Its overall schematic and graph are shown below.</p> <p>The method also utilizes a custom loss that combines statistical (Kolmogorov-Smirnov Statistic) and spatial (Smooth L1) losses. The full description can be found in the paper linked above.</p> <p>For the example included in Sapsan, the data included is from the same dataset as the publication, but it has been heavily sampled (down to \\(17^3\\)). To achieve comparable published results, the model will need to be trained for 3000-4000 epochs.</p> <p> </p>","boost":15},{"location":"details/estimators/#physics-informed-cnn-for-1d-turbulence-modeling-pimlturb1d","title":"Physics-Informed CNN for 1D Turbulence Modeling (PIMLTurb1D)","text":"<p>Example: pimlturb1d_example.ipynb  Estimator: pimlturb1d_estimator.py</p> <p>The estimator is based on Machine Learning for Core-Collapse Supernovae: 1D Models by P.I.Karpov et al. (2023, in prep), and it is similar to the 3D implementation above. The model was adopted for 1D data using 1D convolutional network with some additions to enforce a realizability constraint (\\(Re_{ii} &gt; 0\\), where \\(Re\\) is the Reynolds stress tensor and \\(i\\) is the component index) and a smoothing Gaussian layer.</p> <p>The method also utilizes a custom loss that combines statistical (KS) and spatial (Smooth L1) losses. The full description can be found in the paper linked above.</p> <p>For the example included in Sapsan, only the mapped 3D-to-1D 12 \\(M_{\\odot}\\) is provided, stripped of all features not used in training. That being said, it is exactly the same as the data used for publication-ready results for the 12 \\(M_{\\odot}\\) model. To achieve comparable results, the model will need to be trained for ~1000 epochs. The original 3D dataset was provided by Adam Burrow, produced with FORNAX, and published by Burrows et al. (2020).</p> <p>The trained ML models were used to inference turbulent pressure at runtime in a 1D Fortran-based CCSN code called COLLAPSO1D. To make it work, we integrated a PyTorch wrapper for Fortran, implementation of which can be found on its GitHub. COLLAPSO1D's documentation also contains instructions on integrating said wrapper into other Fortran codebases.</p>","boost":15},{"location":"details/estimators/#physics-informed-convolutional-autoencoder-picae","title":"Physics-Informed Convolutional Autoencoder (PICAE)","text":"<p>Example: picae_example.ipynb  Estimator: picae_estimator.py</p> <p>Note: The estimator is based on Embedding Hard Physical Constraints in Neural Network Coarse-Graining of 3D Turbulence by M.T.Arvind et al.</p> <p>The model consists of 2 main parts: 1. Convolutional Auto-Encoder (trainable) 2. Static layers enforcing divergence-free condition (constant)</p> <p>Thus, the latter force the CAE portion of the model to adjust to the curl of \\(A\\) to be 0. Through this, we are effectively enforcing the conservation of mass. A schematic of the model is shown below.</p> <p> </p>","boost":15},{"location":"details/estimators/#kernel-ridge-regression-krr","title":"Kernel Ridge Regression (KRR)","text":"<p>Example: krr_example.ipynb  Estimator: krr_estimator.py</p> <p>We have included one of the classic regression-based methods used in machine learning - Kernel Ridge Regression. The model has two hyperparameters to be tuned: regularization term \\(\\alpha\\) and full-width at half-max \\(\\sigma\\). KRR has the following form:</p> \\[ y^\u2032\u2004=\u2004y(K\u2005+\u2005\\alpha I)^{\u2212\u20051}k \\] <p>where \\(K\\) is the kernel, chosen to be the radial basis function (gaussian):</p> \\[ K(x,\u2006x^\u2032)\u2004=\u2004exp\\left(\u2005-\\frac{||x\u2005\u2212\u2005x^\u2032||^2}{2\\sigma^2}\\right) \\]","boost":15},{"location":"details/structure/","title":"Structure","text":""},{"location":"details/structure/#dependencies","title":"Dependencies","text":"<p>Sapsan is a python-based framework. Dependencies can be associated with logical modules of the project. The core module does not have any particular dependencies as all classes are implemented using native Python. Lib modules relying heavily on PyTorch with a Catalyst wrapper, as well as scikit-learn for regression-based ML models. CLI module depends on the click library for implementing command line interfaces.</p> <p>Sapsan is integrated with MLflow to provide for easy and automatic tracking during the experiment stage, as well as saving the trained model. This gives direct access to run history and performance, which in turn gives the user ability to analyze and further tweak their model.</p>"},{"location":"details/structure/#structure-flexibility","title":"Structure &amp; flexibility","text":"<p>To provide flexibility and scalability of the project a number of abstraction classes were introduced. Core abstractions include:</p> Core Abstraction Description Experiment main abstraction which encapsulates execution of algorithms, experiments, tests, etc. Algorithm a base class which all models are extended from BaseAlgorithm base class for all algorithms that do not need to be trained and has only run method Estimator an algorithm that has train and predict methods, like regression model or classifier Dataset high level wrapped over dataset loaders <p>Next Sapsan has utility abstractions responsible for all-things tracking:</p> Utility Abstractions Description Metric a single instance of metric emitted during the experiment run Parameter a parameter used in the experiment Artifact artifacts for an algorithm (model weights, images, etc.) TrackingBackend adapter for tracking systems to save metrics, parameters, and artifact <p>The project is built around those abstractions to make it easier to reason about. In order to extend the project with new models/algorithms, the user will inherit from Estimator(or BaseAlgorithm) and implement required methods.</p>"},{"location":"other/community/","title":"Community Guidelines","text":"<p>Sapsan welcomes contributions from the community, looking to improve the pipeline and to grow its library of models. Let's make ML models more accessible together!</p>"},{"location":"other/community/#general-suggestions","title":"General Suggestions","text":"<p>Please feel free to post any bugs you run into or make suggestions through the Issues. I will do my best to address them as soon as possible.</p> <p>If you would like to contribute directly, then a Pull Request would be the most straightforward way to do so. Once approved, you will be added as a contributor to Sapsan on GitHub.</p>"},{"location":"other/community/#adding-a-model","title":"Adding a Model","text":"<p>You would like to contribute to Sapsan's 'model zoo'? That's great! Here are the steps to do so 1. Create a new folder under <code>sapsan/lib/estimator</code> with the name to reflect your model (<code>custom_model</code> for now). 2. Place your python script with the model into that folder, adhering to the format outlined in the template (see Custom Estimator for details)    * make sure you initialize the model with <code>sapsan/lib/estimator/custom_model/__init__.py</code>    * add to <code>sapsan/lib/estimator/__init__.py</code> a line to access your model, such as <pre><code>from .custom_model.custom_model import Custom_Model, Custom_ModelConfig\n</code></pre> 3. Set up a Jupyter notebook example and include it under <code>sapsan/examples</code>. Make sure the example data is either randomly generated, provided in a small batch, or can be auto-downloaded. 4. Write a short description of your model for the Estimators' page on the Wiki. It is a good idea to provide a graph to show the structure of your model (graph example), along with the links to any publications of the model if such exist. 5. Pull Request it!</p> <p>Once approved, your model will be included in automatic testing on push for all future Sapsan releases.</p>"},{"location":"other/community/#analytical-tools","title":"Analytical Tools","text":"<p>We use a huge variety of tools to analyze our results depending on the problem at hand. Sapsan certainly won't be able to cover everything, but it tries to cover the most general ones (e.g. power spectrum). If there is something major missing, please write about it in the Issues or create a Pull Request. For the latter, the tools should be added into <code>sapsan/utils</code>. You can further add to either 1. <code>plot.py</code> as a separate <code>function</code> if it is a visual analysis (e.g. plotting probability density function)  2. <code>physics.py</code> as a separate <code>Class</code> for any type of physics-based calculations 3. Anything custom is fine too</p>"},{"location":"other/community/#analytical-turbulence-models","title":"Analytical Turbulence Models","text":"<p>I am looking to expand a library of analytical turbulence models (i.e. gradient model) to compare ones results with. There are lots of flavors of such, hence a pull request would be highly appreciated. Analytical models should be added as a separate <code>Class</code> in <code>sapsan/utils/physics.py</code>. In addition, please prepare a short description of it for the Wiki.</p>"},{"location":"overview/getting_started/","title":"Getting Started","text":"","boost":10},{"location":"overview/getting_started/#command-line-interface-cli-jupyter-notebooks","title":"Command Line Interface (CLI) &amp; Jupyter Notebooks","text":"<p>CLI allows users to create new projects leveraging the structure and abstractions of Sapsan providing a unified interface of interaction with the experiments. In addition, you can test your installation and play around with a few included examples.</p>","boost":10},{"location":"overview/getting_started/#testing","title":"Testing","text":"<p>To make sure everything is working correctly and Sapsan was installed without issues, run: <pre><code>sapsan test\n</code></pre></p>","boost":10},{"location":"overview/getting_started/#running-examples","title":"Running Examples","text":"<p>To get started and familiarize yourself with the Jupyter Notebook interface, feel free to run the included examples (CNN, PICAE, PIMLTurb on 3D data, PIMLTurb1D on 1D data, and KRR on 2D data). There is also a notebook with examples of plotting routines and ML network visualization. To copy the examples, type:</p> <p><pre><code>sapsan get_examples\n</code></pre> This will create a folder <code>./sapsan_examples</code> with appropriate example jupyter notebooks.</p>","boost":10},{"location":"overview/getting_started/#custom-projects","title":"Custom Projects","text":"<p>In order to get started on your own project, proceed as follows:</p> <p><pre><code>sapsan create --name {name}\n</code></pre> where <code>{name}</code> should be replaced with your custom project name. This will result in creation of the following file structure:</p> <pre><code>Project Folder:             {name}/\nData Folder:                {name}/data/\nEstimator Template:         {name}/{name}_estimator.py\nJupyter Notebook Template:  {name}/{name}.ipynb\nDocker Template:            {name}/Dockerfile  \nDocker Makefile:            {name}/Makefile  \n</code></pre> <p>This structure allows you to focus on the designing your network structure itself in <code>{name}_estimator.py</code>. At the same time, you can quickly jump into Jupyter Notebook and start running your custom setup. Lastly, <code>Dockerfile</code> is already pre-filled to easily share your work with your collaborators or as part of a publication.</p> <p></p>","boost":10},{"location":"overview/getting_started/#graphical-user-interface-gui-beta","title":"Graphical User Interface (GUI) - beta","text":"<p>In the aim to provide a user-friendly experience, best suited for demonstrations of your models at talks and conferences, while attempting to not sacrifice too much on customization we have designed a GUI for Sapsan. By utilizing Streamlit, a python library to build web applications, Sapsan can be fully interacted in the browser running locally. A user can tweak the parameters, edit the portion of the code responsible for the ML model, perform visual layer-by-layer analysis, train/validate, analyze the results, and more.</p> <p>Lastly, Sapsan can be tried out in the demo-mode directly on the website - sapsan.app. There, one has limited editing capabilities but can explore the hyper-parameters and get a general understanding of what the framework is capable of.</p> <p>Offline</p> <p>sapsan.app is temporarily offline while transitioning to a new hosting service. Please refer to local GUI example for the demo.</p>","boost":10},{"location":"overview/getting_started/#running-gui","title":"Running GUI","text":"<p>In order to run it type in the following and follow the instructions - the interface will be opened in your browser <pre><code>sapsan get_examples\nstreamlit run ./sapsan-examples/GUI/st_intro.py\n</code></pre></p> <p>Learn more at GUI Examples.</p>","boost":10},{"location":"overview/getting_started/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter the following error when launching streamlit</p> <pre><code>upper limit on inotify watches reached!\n</code></pre> <p>Then follow the following instruction by Shivani Bhardwaj to increase the watchdog limit (it won't hog your RAM)</p>","boost":10},{"location":"overview/installation/","title":"Installation","text":"","boost":5},{"location":"overview/installation/#required","title":"Required","text":"","boost":5},{"location":"overview/installation/#1-install-pytorch-prerequisite","title":"1. Install PyTorch (prerequisite)","text":"<p>Sapsan can be run on both cpu and gpu. Below are the requirements for each version</p> Device CPU torch&gt;=1.9.0 torchvision&gt;=0.10.0 GPU torch&gt;=1.9.0+cu111 torchvision&gt;=0.10.0+cu111 <p>Please follow the instructions on PyTorch to install either version. <code>CUDA&gt;=11.1</code> can be installed directly with PyTorch as well.</p>","boost":5},{"location":"overview/installation/#install-cuda-toolkit","title":"Install CUDA-Toolkit","text":"<p>If you are planning to train on GPU, then you need to check that you have an <code>nvidia-cuda-toolkit</code> installed: <pre><code>nvcc --version\n</code></pre> if it is missing, you can install it via: <pre><code>sudo apt install nvidia-cuda-toolkit\n</code></pre></p> <p>Device</p> <p>If GPU is available, Sapsan will always try to run on a GPU by default (that includes the tests). However, you can specify the <code>device='cpu'</code> in model config.</p>","boost":5},{"location":"overview/installation/#2a-install-via-pip-recommended","title":"2a. Install via pip (recommended)","text":"<pre><code>pip install sapsan\n</code></pre>","boost":5},{"location":"overview/installation/#2b-clone-from-github-alternative","title":"2b. Clone from github (alternative)","text":"<pre><code>git clone https://github.com/pikarpov-LANL/Sapsan.git\ncd Sapsan/\npython setup.py install\n</code></pre> <p>If you experience any issues, you can try installing packages individually with: <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Version</p> <p>Make sure you are using the latest release version!</p>","boost":5},{"location":"overview/installation/#optional","title":"Optional","text":"","boost":5},{"location":"overview/installation/#install-graphviz","title":"Install Graphviz","text":"<p>Info</p> <p>pip version of graphviz that installs with Sapsan is not enough!</p> <p>In order to create model graphs, Sapsan is using graphviz. If you would like to utilize this functionality, pip graphviz wrapper is not enough. To install the core package for graphviz: <pre><code>conda install graphviz\n</code></pre> or <pre><code>sudo apt-get install graphviz\n</code></pre></p>","boost":5},{"location":"overview/installation/#install-docker","title":"Install Docker","text":"<p>In order to run Sapsan through Docker or build your own container to share, you will need to install it <pre><code>pip install docker\n</code></pre></p> <p>Next, you can build a docker setup with the following:</p> <pre><code>make build-container\n</code></pre> <p>this will create a container named <code>sapsan-docker</code>.</p> <p>If you want to run the container, type:</p> <p><pre><code>make run-container\n</code></pre> a Jupyter notebook will be launched at <code>localhost:7654</code></p>","boost":5},{"location":"overview/installation/#troubleshooting","title":"Troubleshooting","text":"","boost":5},{"location":"overview/installation/#libglso-error","title":"libGL.so error","text":"<p>If you get the following error: <pre><code>ImportError: libGL.so.1: cannot open shared object file: No such file or directory\n</code></pre> your <code>opencv-python</code> package has some dependency issues. To resolve, try the following: <pre><code>apt-get update\napt-get install ffmpeg libsm6 libxext6  -y\n</code></pre></p>","boost":5},{"location":"overview/installation/#my-kernel-is-dying","title":"My Kernel is Dying!","text":"<p>If when attempting to train the model (executing <code>Train.run()</code>) the kernel is dying without any errors, check if you have a GPU on your machine while not having a <code>nvidia-cuda-toolkit</code> installed. To resolve, you can either install it: <pre><code>sudo apt install nvidia-cuda-toolkit\n</code></pre> or specify <code>device='cpu'</code> in your model config. By defaul Sapsan always tries to train the models on a GPU if one is available.</p>","boost":5},{"location":"overview/examples/local_examples/","title":"Built-in Examples","text":""},{"location":"overview/examples/local_examples/#jupyter-notebook-examples","title":"Jupyter Notebook Examples","text":"<p>You can run the included examples (CNN, PICAE, PIMLTurb on 3D data, PIMLTurb1D,and KRR on 2D data). There is also a notebook with examples of plotting routines and ML network visualization. To copy the examples, type:</p> <p><pre><code>sapsan get_examples\n</code></pre> This will create a folder <code>./sapsan_examples</code> with appropriate example jupyter notebooks and GUI. For starters, to launch a CNN example:</p> <pre><code>jupyter notebook ./sapsan_examples/cnn_example.ipynb\n</code></pre>"},{"location":"overview/examples/local_examples/#gui-examples","title":"GUI Examples","text":"<p>In order to try out Sapsan's GUI, start a streamlit instance to open in a browser. After the examples have been compied into your working directory as described above, you will be able to find the GUI example. The entry point:</p> <pre><code>streamlit run ./sapsan_examples/GUI/Welcome.py\n</code></pre> <p>The scripts for the pages you see (welcome and examples) are located in the subsequent directory: <code>./sapsan_examples/GUI/pages/</code>. </p> <p>If you want to build your own demo, then look into Examples.py to get started. Ideally, you would only need to import your <code>Estimator</code>, <code>EstimatorConfig</code>, <code>EstimatorModel</code> and adjust the <code>run_experiment()</code> function, which has a nearly identical setup to a standard Sapsan's jupyter notebook interface.</p> <p> </p>"},{"location":"overview/examples/local_examples/#sample-data","title":"Sample Data","text":"<p>The data for the CNN and KRR examples has been sourced from JHTDB. Specifically the Forced MHD Dataset (10243) has been used as a starting point.</p> Data Description u_dim128_2d.h5 velocity field sampled down to [128,128,128] and using the 1st slice u_dim32_fm15.h5 velocity field sampled down to [32,32,32], and spectrally filtered down to 15 modes"},{"location":"overview/examples/web_examples/","title":"Online Examples","text":""},{"location":"overview/examples/web_examples/#jupyter-notebook-on-google-colab","title":"Jupyter Notebook on Google-Colab","text":"<p>You can play around with the CNN example on the google-colab. Besides the initial setup in the first cell, the rest of the notebook is identical to the one on github.</p> <p>Proceed via the link cnn_example.ipynb on google-colab.</p>"},{"location":"overview/examples/web_examples/#gui-web-demo","title":"GUI Web Demo","text":"<p>Sapsan's GUI is powered by streamlit. A short demo with the CNN example has been setup on google cloud. </p> <p>Try it out on sapsan.app</p> <p> </p>"},{"location":"tutorials/custom_docker/","title":"Custom Docker","text":"<p>The best way to release your code along with the publication is through Docker. This will ensure the reproducibility of your paper's results, making your methods easily accessible to the readers and general public.</p> <p>While you can write and handle Docker containers in any fashion you want, Sapsan includes a ready-to-go template to make this process easier. Here are the steps in the Docker template:</p> <ol> <li>Setup a virtual environment </li> <li>Install requirements (Sapsan)</li> <li>Launch a Jupyter notebook to reproduce the results</li> </ol> <p>That's it! Now there won't be any struggle or emails to you, the author, about the setup and configuration of your methods!</p> <p>In order to make this work, we will need to set up a Dockerfile, build a container, and run it. The latter steps are combined into a Makefile. When it comes to publishing your Docker, share the Docker setup files for the container to be built on-site. In this article, we will first discuss the Docker setup and then the release options.</p> <p>Note</p> <p>Make sure Docker is installed on your machine (Installation)</p>","boost":5},{"location":"tutorials/custom_docker/#docker-setup","title":"Docker Setup","text":"","boost":5},{"location":"tutorials/custom_docker/#dockerfile","title":"Dockerfile","text":"<p>The template below is will be created when starting a project via <code>sapsan create -n {name}</code>, where <code>{name}</code> is your custom project name. Feel free to edit it to your liking, such as adding further packages to install outside of Sapsan, name of working directories and etc.</p> <p><pre><code>FROM python:3.8.5-slim\n\n# remember to expose the port your app will run on\nEXPOSE 7654\nENV GIT_PYTHON_REFRESH=quiet\nRUN pip install -U pip\n\nRUN pip install sapsan=={version}\n# copy the notebook and data into a directory of its own (so it isn't in the top-level dir)\nCOPY {name}_estimator.py {name}_docker/\nCOPY {name}.ipynb {name}_docker/\nCOPY ./data/ {name}_docker/data/\nWORKDIR /{name}_docker\n\n# run it!\nENTRYPOINT [\"jupyter\", \"notebook\", \"{name}.ipynb\", \"--port=7654\", \"--ip=0.0.0.0\", \"--allow-root\", \"--NotebookApp.token=''\", \"--NotebookApp.password=''\", \"--no-browser\"]\n</code></pre> Here is a working Dockerfile to dockerize the Sapsan's included CNN example.</p>","boost":5},{"location":"tutorials/custom_docker/#makefile-to-build-and-run-the-container","title":"Makefile to build and run the container","text":"<p>The <code>Makefile</code> is also created upon initializing a project. It makes it straightforward to build and run your Docker container, launching a Jupyter Notebook as a result.</p> <pre><code># to build and start the container \nbuild-container:\n    @docker build . -t {name}-docker\n\n# to run existing the container created above\n# (jupyter notebook will be started at --port==7654)\nrun-container:\n    @docker run -p 7654:7654 {name}-docker:latest\n</code></pre> <p>Thus, the user will need to type the following to build and run the Docker container: <pre><code>make build-container\nmake run-container\n</code></pre> Here is a working Makefile for Sapsan's included CNN example.</p>","boost":5},{"location":"tutorials/custom_docker/#release-your-docker","title":"Release Your Docker","text":"","boost":5},{"location":"tutorials/custom_docker/#provide-the-setup-files","title":"Provide the Setup Files","text":"<p>In order for someone to reproduce your results, you will need to provide:</p> <ol> <li>Dockerfile</li> <li>Makefile</li> <li>Jupyter Notebook</li> <li>Training Data</li> </ol> <p>The virtual environment will be built from the ground up on the user's local machine. Besides the training data, the other files won't weigh anything. The only pre-requisite is to have the Docker installed, which can be done through <code>pip</code>.</p>","boost":5},{"location":"tutorials/custom_estimator/","title":"Custom Estimator","text":"<p>Sapsan makes it easy to get started on designing your own ML model layer-by-layer.</p>","boost":5},{"location":"tutorials/custom_estimator/#command-line-interface-cli","title":"Command-line Interface (CLI)","text":"<p>Here is the easiest way to get started, where you should replace <code>{name}</code> with your custom project name.</p> <pre><code>sapsan create -n {name}\n</code></pre> <p>This will create the full structure for your project, but in a template form. You will primarily focus on the designing your ML model (estimator). You will find the template for it in</p> <pre><code>{name}/{name}_estimator.py\n</code></pre> <p>The template is structured to utilize a custom backend <code>sapsan.lib.estimator.torch_backend.py</code>, hence it revolves around using PyTorch. In the template, you will define the layers your want to use, the order in which they should be executed, and a few custom model parameters (Optimizer, Loss Function, Scheduler). Since we are talking about PyTorch, refer to its API to define your layers.</p>","boost":5},{"location":"tutorials/custom_estimator/#estimator-template","title":"Estimator Template","text":"<ol> <li>{name}Model<ol> <li>define your ML layers</li> <li>forward function (layer order)</li> </ol> </li> <li>{name}Config<ol> <li>set parameters (e.g. number of epochs) - usually set through a high-level interface (e.g. a jupyter notebook)</li> <li>add custom parameters to be tracked by MLflow</li> </ol> </li> <li>{name}<ol> <li>Set the Optimizer</li> <li>Set the Loss Functions</li> <li>Set the Scheduler</li> <li>Set the Model (based on {name}Model &amp; {name}Config)</li> </ol> </li> </ol> <pre><code>\"\"\"\nEstimator Template\nPlease replace everything between triple quotes to create\nyour custom estimator.\n\"\"\"\nimport json\nimport numpy as np\nimport torch\nfrom sapsan.core.models import EstimatorConfig\nfrom sapsan.lib.estimator.torch_backend import TorchBackend\nfrom sapsan.lib.data import get_loader_shape\nclass {name_upper}Model(torch.nn.Module):\n# input channels, output channels can be the input to define the layers\ndef __init__(self):\nsuper({name_upper}Model, self).__init__()\n# define your layers\n\"\"\"\n        self.layer_1 = torch.nn.Linear(4, 8)\n        self.layer_2 = torch.nn.Linear(8, 16)\n        \"\"\"\ndef forward(self, x): \n# set the layer order here\n\"\"\"\n        l1 = self.layer_1(x)\n        output = self.layer_2(l1)\n        \"\"\"\nreturn output\nclass {name_upper}Config(EstimatorConfig):\n# set defaults to your liking, add more parameters\ndef __init__(self,\nn_epochs: int = 1,\nbatch_dim: int = 64,\npatience: int = 10,\nmin_delta: float = 1e-5, \nlogdir: str = \"./logs/\",\nlr: float = 1e-3,\nmin_lr = None,                 \n*args, **kwargs):\nself.n_epochs = n_epochs\nself.batch_dim = batch_dim\nself.logdir = logdir\nself.patience = patience\nself.min_delta = min_delta\nself.lr = lr\nif min_lr==None: self.min_lr = lr*1e-2\nelse: self.min_lr = min_lr\nself.kwargs = kwargs\n#everything in self.parameters will get recorded by MLflow\n#by default, all 'self' variables will get recorded\nself.parameters = {{f'model - {{k}}': v for k, v in self.__dict__.items() if k != 'kwargs'}}\nif bool(self.kwargs): self.parameters.update({{f'model - {{k}}': v for k, v in self.kwargs.items()}})\nclass {name_upper}(TorchBackend):\n# Set your optimizer, loss function, and scheduler here\ndef __init__(self, loaders,\nconfig = {name_upper}Config(), \nmodel = {name_upper}Model()):\nsuper().__init__(config, model)\nself.config = config\nself.loaders = loaders\n#uncomment if you need dataloader shapes for model input\n#x_shape, y_shape = get_shape(loaders)\nself.model = {name_upper}Model()\nself.optimizer = \"\"\" optimizer \"\"\"\nself.loss_func = \"\"\" loss function \"\"\"\nself.scheduler = \"\"\" scheduler \"\"\"        \ndef train(self):\ntrained_model = self.torch_train(self.loaders, self.model, \nself.optimizer, self.loss_func, self.scheduler, \nself.config)\nreturn trained_model\n</code></pre>","boost":5},{"location":"tutorials/custom_estimator/#editing-catalyst-runner","title":"Editing Catalyst Runner","text":"<p>For majority of applications, you won't need to touch Catalyst Runner settings, which located in <code>torch_backend.py</code>. However, in case you would like to dig further into more unique loss functions, optimizers, data distribution setups, then you can copy the <code>torch_backend.py</code> via <code>--get_torch_backend</code> or shorthand <code>--gtb</code> flag during the creation of the project:</p> <p><pre><code>sapsan create --gtb -n {name}\n</code></pre> or just copy it to your current directory by:</p> <pre><code>sapsan gtb\n</code></pre> <p>For <code>runner</code> types and extensive options please refer to Catalyst Documentation.</p> <p>As for runner adjustments to parallele your training, Sapsan's Wiki includes a page on Parallel GPU Training.</p>","boost":5},{"location":"tutorials/custom_estimator/#loss","title":"Loss","text":"<p>Catalyst includes a more extensive list of losses, i.e. criterions, than the standard PyTorch. Their implementations might require to include some extra <code>Callback</code>s to be specified in the runner (Criterion Documentation). Please refer to Catalyst examples to create your own loss functions.</p>","boost":5},{"location":"tutorials/custom_estimator/#optimizer","title":"Optimizer","text":"<p>Similar deal is with the optimizer. While using standard (i.e. Adam) can be specified within Estimator Template, for a more complex or custom setup you will need to refer to the runner (Optimizer Documentation).</p>","boost":5},{"location":"tutorials/mlflow/","title":"MLflow Tracking","text":"","boost":5},{"location":"tutorials/mlflow/#default-mlflow-tracking-in-sapsan","title":"Default MLflow Tracking in Sapsan","text":"","boost":5},{"location":"tutorials/mlflow/#starting-mlflow-server","title":"Starting MLflow Server","text":"<p><code>mlflow ui</code> server will automatically start locally if a designated port is open. If not, Sapsan assumes the <code>mLflow ui</code> server is already running on that local port and will direct mlflow to write to it. Also you can start <code>mlflow ui</code> manually via: <pre><code>mlflow ui --host localhost --port 5000\n</code></pre></p>","boost":5},{"location":"tutorials/mlflow/#structure","title":"Structure","text":"<p>By default, Sapsan will keep the following structure in MLflow:</p> <ul> <li>Train 1<ul> <li>Evaluate 1</li> <li>Evaluate 2</li> </ul> </li> <li>Train 2<ul> <li>Evaluate 1</li> <li>Evaluate 2</li> </ul> </li> </ul> <p>where all evaluation runs are nested under the trained run entry. This way all evaluations are grouped together under the model that was just trained.</p> <p>Every Train checks for other active runs, terminates them, and starts a new run. At the end of the Train method, the run does not terminate, awaiting Evaluate runs to be nested under it. Thus, Evaluate runs start and end at the end of the method. However, one can still add extra metrics, artifacts and etc by resuming the previously closed run and writing to it, as discussed in the later section.</p>","boost":5},{"location":"tutorials/mlflow/#tracked-parameters","title":"Tracked Parameters","text":"<p>Evaluation runs include the training model parameters and metrics to make it easier to parse through. Here is a complete list of what is tracked by default after running Train or Evaluate loop.</p> Parameter Train Evaluate Everything passed to <code>ModelConfig()</code>  (including new parameters passed to <code>kwargs</code>) <code>model - {parameter}</code> - device, logdir, lr, min_delta,  min_lr, n_epochs, patience <code>data - {parameter}</code> - features, features_label, target, target_label, axis, path, path, shuffle <code>chkpnt - {parameter}</code> - initial_size, sample to size, batch_size, batch_num, time, time_granularity <p>Since Train metrics are recorded for Evaluate runs, they are prefixed as <code>train - {metric}</code>. Subsequently, all Evaluate metrics are written as <code>eval - {metric}</code></p> Metrics Train Evaluate <code>eval - MSE Loss</code> - Mean Squared Error (if the target is provided) <code>eval - KS Stat</code> - Kolmogorov-Smirnov Statistic (if the target is provided) <code>train - final epoch</code> - final training epoch All model metrics <code>model.metrics()</code> (provided by Catalyst and PyTorch) Runtime **Artifacts ** Train Evaluate <code>model_details.txt</code> - model layer init &amp; optimizer settings <code>model_forward.txt</code> - Model.forward() function <code>runtime_log.html</code> - loss vs. epoch training progress <code>pdf_cdf.png</code> - Probability Density Function (PDF) &amp; Cummulative Distribution Function (CDF) plots <code>slices_plot.png</code> - 2D Spatial Distribution (slice snapshots)","boost":5},{"location":"tutorials/mlflow/#adding-extra-parameters","title":"Adding extra parameters","text":"","boost":5},{"location":"tutorials/mlflow/#before-training","title":"Before Training","text":"<p>In order to add a <code>new_parameter</code> to be tracked with MLflow per your run, simply pass it to config as such: <code>ModelConfig(new_parameter=value)</code>. </p> <p>Since it will be initialized under <code>ModelConfig().kwargs['new_parameter']</code>, the parameter name can be anything. You will see it in MLflow recorded as <code>model - new_parameter</code>.</p> <p>Internally, everything in <code>ModelConfig().parameters</code> gets recorded to MLflow. By default, all <code>ModelConfig()</code> variables, including <code>kwargs</code> are passed to it. Here is the implementation from the CNN3d estimator.</p> <pre><code>class CNN3dConfig(EstimatorConfig):\ndef __init__(self,\nn_epochs: int = 1,\npatience: int = 10,\nmin_delta: float = 1e-5,\nlogdir: str = \"./logs/\",\nlr: float = 1e-3,\nmin_lr = None,\n*args, **kwargs):\nself.n_epochs = n_epochs\nself.logdir = logdir\nself.patience = patience\nself.min_delta = min_delta\nself.lr = lr\nif min_lr==None: self.min_lr = lr*1e-2\nelse: self.min_lr = min_lr\nself.kwargs = kwargs\n#everything in self.parameters will get recorded by MLflow\n#by default, all 'self' variables will get recorded\nself.parameters = {f'model - {k}': v for k, v in self.__dict__.items() if k != 'kwargs'}\nif bool(self.kwargs): self.parameters.update({f'model - {k}': v for k, v in self.kwargs.items()})\n</code></pre> <p>Note: MLflow doesn't like labels that contain <code>/</code> symbol. Please avoid or you might encounter an error.</p>","boost":5},{"location":"tutorials/mlflow/#after-training-or-evaluation","title":"After Training or Evaluation","text":"<p>If you want to perform some extra analysis on your model or predictions and record additional metrics after you have called <code>Train.run()</code> or <code>Evaluation.run()</code>, Sapsan has an interface to do so in 3 steps:</p> <ol> <li>Resume MLflow run</li> <li> <p>Since MLflow run is closed at the end of <code>Evaluation.run()</code>, it will need to be resumed first before attempting to write to it. For that reason, both Train and Evaluate classes have a parameter <code>run_id</code> which contains the MLflow run_id. You can use it to resume the run, and record new metrics.</p> </li> <li> <p>Record new parameters</p> </li> <li> <p>To add extra parameters to the most recent Train or Evaluate entry in MLflow, simply use either the <code>backend()</code> interface or the standard MLflow interface.</p> </li> <li> <p>End the run</p> </li> <li>In order to keep MLflow tidy, it is advised to call <code>backend.end()</code> after you are done.</li> </ol> <p><pre><code>eval = Evaluate(...)\ncubes = eval.run()\n#do something with the prediction and/or target cube\nnew_metric = np.amax(cubes['pred_cube'] / cubes['target_cube'])\nbackend.resume(run_id = eval.run_id)\nbackend.log_metric('new_metric', new_metric) #or use backend.log_parameter() or backend.log_artifact()\nbackend.end()\n</code></pre> Feel free to review the full API Reference: Backend (Tracking) for the full description of MLflow-related functions built into Sapsan. </p>","boost":5},{"location":"tutorials/model_graph/","title":"Model Graph","text":""},{"location":"tutorials/model_graph/#how-to-construct-a-graph-of-the-model","title":"How to construct a graph of the model","text":"<p>This a page describing in detail how to construct nice-looking graphs of your model automatically.</p>"},{"location":"tutorials/model_graph/#example","title":"Example","text":"<p>There is a <code>model_graph</code> example in the plotting_examples.ipynb notebook (you can get all examples by running <code>sapsan get_examples</code>). That being said, a brief overview of how it works is below:</p> <p><pre><code>from sapsan.lib.estimator.cnn.cnn3d_estimator import CNN3d, CNN3dConfig\nfrom sapsan.utils.plot import model_graph\nfrom sapsan.lib.data import get_loader_shape\n# load your data into torch loaders\nestimator = CNN3d(config = CNN3dConfig(),\nloaders = loaders)\nshape_x, shape_y = get_loader_shape(loaders)\nmodel_graph(model = estimator.model, shape = shape_x)\n</code></pre> Considering that <code>shape_x = (8,1,8,8,8)</code>, the following graph will be produced:</p> <p> </p> <p> </p>"},{"location":"tutorials/model_graph/#details","title":"Details","text":"<p><code>shape</code> of the input data is in the format [N, Cin, Db, Hb, Wb]. You can either grab it from the loader as shown above or provide your own, as long as the number of channels Cin matches the data your model was initialized with.</p> <p><code>transforms</code> allow you to adjust the graph to your liking. For example, they can allow you to combine layers to be displayed in a single box, instead of separate. Please refer to the API of model_graph to see what options are available for transformations.</p> <p>Info</p> <p>Order of transforms in the list matters!</p>"},{"location":"tutorials/model_graph/#limitations","title":"Limitations","text":"<ul> <li><code>model</code> input param must be a PyTorch, TensorFlow, or Keras-with-TensorFlow-backend model.</li> </ul>"},{"location":"tutorials/model_graph/#api-for-model_graph","title":"API for model_graph","text":"<code>sapsan.utils.plot.model_graph</code><code>(model, shape: np.array, transforms)</code> <p>Creates a graph of the ML model (needs graphviz to be installed). The method is based on hiddenlayer originally written by Waleed Abdulla.</p> <code>Parameters</code> <p>model (object) - initialized pytorch or tensorflow model</p> <p>shape (np.array) - shape of the input array in the form [N, Cin, Db, Hb, Wb], where Cin=1</p> <p>transforms (list[methods]) - a list of hiddenlayer transforms to be applied (Fold, FoldId, Prune, PruneBranch, FoldDuplicates, Rename), defined in transforms.py. Default: <pre><code>&gt; import sapsan.utils.hiddenlayer as hl\n&gt; transforms = [\nhl.transforms.Fold(\"Conv &gt; MaxPool &gt; Relu\", \"ConvPoolRelu\"),\nhl.transforms.Fold(\"Conv &gt; MaxPool\", \"ConvPool\"),    \nhl.transforms.Prune(\"Shape\"),\nhl.transforms.Prune(\"Constant\"),\nhl.transforms.Prune(\"Gather\"),\nhl.transforms.Prune(\"Unsqueeze\"),\nhl.transforms.Prune(\"Concat\"),\nhl.transforms.Rename(\"Cast\", to=\"Input\"),\nhl.transforms.FoldDuplicates()\n]\n</code></pre></p> <code>Return</code> <p>SVG graph of a model</p> <code>Return type</code> <p>graphviz.Digraph object</p>"},{"location":"tutorials/parallelgpu/","title":"Parallel GPU Training","text":""},{"location":"tutorials/parallelgpu/#automatic","title":"Automatic","text":"<p>Sapsan relies on Catalyst to implement Distributed Data Parallel (DDP). You can specify <code>ddp=True</code> in <code>ModelConfig</code>, which in turn will set <code>ddp=True</code> parameter for the Catalyst runner.train(). Let's take a look at how it could be done by adjusting cnn_example:</p> cnn_example.ipynb<pre><code>estimator = CNN3d(config = CNN3dConfig(n_epochs=5, \npatience=10, \nmin_delta=1e-5, \nddp=True),\nloaders = loaders)\n</code></pre> <p>DDP is not supported on Jupyter Notebooks! You will have to prepare a script.</p> <p>Thus, it is advised to start off developing and testing your model on a single CPU or GPU in a jupyter notebook, then downloading it as a python script to run on multiple GPUs locally or on HPC. In addition, you will have to add the following statement to the beginning of your script in order for torch.multiprocessing to work correctly:</p> <pre><code>if __name__ == '__main__':\n</code></pre> <p>Even though Training will be performed on the GPUs, evaluation will be done on the CPU.</p>"},{"location":"tutorials/parallelgpu/#customizing","title":"Customizing","text":"<p>For more information and further customization of your parallel setup, see DDP Tutorial from Catalyst. It might come in useful if you want, among other things, to take control over what portion of the data is copied onto which node. The runner itself, torch_backend.py, can be copied to the project directory and accessed when creating a new project by invoking <code>--get_torch_backend</code> or <code>--gtb</code> flag as such:</p> <pre><code>sapsan create --gtb -n {name}\n</code></pre> <p>The <code>torch_backend.py</code> contains lots of important functions, but for customizing <code>DDP</code> you will need to focus on <code>TorchBackend.torch_train()</code> as shown below. Most likely you will need to adjust <code>self.runner</code> to either another Catalyst runner or your own, custom runner. Next, you will need to edit <code>self.runner.train()</code> parameters accordingly.</p> torch_backend.py<pre><code>class TorchBackend(Estimator):\ndef __init__(self, config: EstimatorConfig, model):\nsuper().__init__(config)\nself.runner = SupervisedRunner() # (1)\n.\n.\n.\ndef torch_train(self):\n.\n.\n.\nself.runner.train(model=model,\ncriterion=self.loss_func,\noptimizer=self.optimizer,\nscheduler=self.scheduler,\nloaders=loaders,\nlogdir=self.config.logdir,\nnum_epochs=self.config.n_epochs,\ncallbacks=[EarlyStoppingCallback(patience=self.config.patience,\nmin_delta=self.config.min_delta,\nloader_key=self.loader_key,\nmetric_key=self.metric_key,\nminimize=True),\nSchedulerCallback(loader_key=self.loader_key,\nmetric_key=self.metric_key,),\nSkipCheckpointCallback(logdir=self.config.logdir)\n],\nverbose=False,\ncheck=False,\nengine=DeviceEngine(self.device),\nddp=self.ddp # (2)\n)\n</code></pre> <ol> <li>Adjust the Runner here. Check Catalyst's documentation</li> <li>Controls automatic Distributed Data Parallel (DDP)</li> </ol>"},{"location":"tutorials/savenload/","title":"Save &amp; Load Models","text":"<p>All estimators in Sapsan depend either on <code>torch_backend</code> or <code>sklearn_backend</code>, depending on the model architecture. Both backends have save and load functions. Thus, no matter whether you are using included estimators or designing your own, both methods will be available. Loaded models can be used either for evaluation or to continue training. In the case of the latter either old or new config parameters can be set.</p>"},{"location":"tutorials/savenload/#saving-the-model","title":"Saving the Model","text":"<p>To save the model, call:</p> <pre><code>estimator.save(path = {save_path})\n</code></pre> <p>For PyTorch, the states of the model and optimizer will be saved, along with the last epoch and loss in <code>{save_path}/model.pt</code>. The config parameters will be saved in <code>{save_path}/params.json</code></p> <p>For Sklearn, only the model itself will be saved, in <code>{save_path}/model.pt</code></p>"},{"location":"tutorials/savenload/#loading-the-model","title":"Loading the Model","text":"<p>Even though all Sapsan estimators have <code>load</code> method, you can use a dummy estimator to load your model.</p>"},{"location":"tutorials/savenload/#pytorch","title":"PyTorch","text":"<p>Import load_estimator() to load your  PyTorch model. You can pass new ModelConfig() parameters as well if you intend to continue training your model.</p> <pre><code>from sapsan.lib.estimator import load_estimator\nestimator = CNN3d(config = CNN3dConfig(n_epoch=100),\nloaders = loaders)\nloaded_estimator = load_estimator.load({path_to_model}, \nestimator = estimator)\n</code></pre>"},{"location":"tutorials/savenload/#sklearn","title":"Sklearn","text":"<p>Sklearn uses a different interface, so you will need to call load_sklearn_estimator()</p> <pre><code>from sapsan.lib.estimator import load_sklearn_estimator\nestimator = KRR(config = KRRConfig(),\nloaders = loaders)\nloaded_estimator = load_sklearn.estimator.load({path_to_model}, \nestimator = estimator)\n</code></pre>"}]}
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","tags":["astro","supernovae","CCSN","turbulence","machine learning","los alamos","LANL","ucsc"],"text":"Welcome to Sapsan! Sapsan is a pipeline for Machine Learning (ML) based turbulence modeling. While turbulence is important in a wide range of mediums, the pipeline primarily focuses on astrophysical application. With Sapsan, one can create their own custom models or use either conventional or physics-informed ML approaches for turbulence modeling included with the pipeline ( estimators ). For example, Sapsan features ML models in its set of tools to accurately capture the turbulent nature applicable to Core-Collapse Supernovae. Purpose Sapsan takes out all the hard work from data preparation and analysis in turbulence and astrophysical applications, leaving you focused on ML model design, layer by layer. Website Check out a website version with a few examples at sapsan.app . The interface is identical to the GUI of the local version of Sapsan, except lacking the ability to edit the model code on the fly and to use mlflow for tracking. News and Publications Physics-Informed Machine Learning for Modeling Turbulence in Supernovae Astrophysical Journal (ApJ) - 2022 Sapsan: Framework for Supernovae Turbulence Modeling with Machine Learning Journal of Open Source Software (JOSS) - November 26, 2021 Provectus Brings Machine Learning to Numerical Astrophysics, Helping Simulate Turbulence in Supernovae Models Provectus IT Press Release - March 9, 2021 Machine Learning for Supernova Turbulence Society for Industrial and Applied Mathematics (SIAM) News (CSE21) - March 4, 2021 License Sapsan has a BSD-style license, as found in the LICENSE file. \u00a9 (or copyright) 2019. Triad National Security, LLC. All rights reserved. This program was produced under U.S. Government contract 89233218CNA000001 for Los Alamos National Laboratory (LANL), which is operated by Triad National Security, LLC for the U.S. Department of Energy/National Nuclear Security Administration. All rights in the program are reserved by Triad National Security, LLC, and the U.S. Department of Energy/National Nuclear Security Administration. The Government is granted for itself and others acting on its behalf a nonexclusive, paid-up, irrevocable worldwide license in this material to reproduce, prepare derivative works, distribute copies to the public, perform publicly and display publicly, and to permit others to do so.","title":"Welcome to Sapsan!"},{"location":"#welcome-to-sapsan","text":"Sapsan is a pipeline for Machine Learning (ML) based turbulence modeling. While turbulence is important in a wide range of mediums, the pipeline primarily focuses on astrophysical application. With Sapsan, one can create their own custom models or use either conventional or physics-informed ML approaches for turbulence modeling included with the pipeline ( estimators ). For example, Sapsan features ML models in its set of tools to accurately capture the turbulent nature applicable to Core-Collapse Supernovae.","title":"Welcome to Sapsan!"},{"location":"#purpose","text":"Sapsan takes out all the hard work from data preparation and analysis in turbulence and astrophysical applications, leaving you focused on ML model design, layer by layer.","title":"Purpose"},{"location":"#website","text":"Check out a website version with a few examples at sapsan.app . The interface is identical to the GUI of the local version of Sapsan, except lacking the ability to edit the model code on the fly and to use mlflow for tracking.","title":"Website"},{"location":"#news-and-publications","text":"Physics-Informed Machine Learning for Modeling Turbulence in Supernovae Astrophysical Journal (ApJ) - 2022 Sapsan: Framework for Supernovae Turbulence Modeling with Machine Learning Journal of Open Source Software (JOSS) - November 26, 2021 Provectus Brings Machine Learning to Numerical Astrophysics, Helping Simulate Turbulence in Supernovae Models Provectus IT Press Release - March 9, 2021 Machine Learning for Supernova Turbulence Society for Industrial and Applied Mathematics (SIAM) News (CSE21) - March 4, 2021 License Sapsan has a BSD-style license, as found in the LICENSE file. \u00a9 (or copyright) 2019. Triad National Security, LLC. All rights reserved. This program was produced under U.S. Government contract 89233218CNA000001 for Los Alamos National Laboratory (LANL), which is operated by Triad National Security, LLC for the U.S. Department of Energy/National Nuclear Security Administration. All rights in the program are reserved by Triad National Security, LLC, and the U.S. Department of Energy/National Nuclear Security Administration. The Government is granted for itself and others acting on its behalf a nonexclusive, paid-up, irrevocable worldwide license in this material to reproduce, prepare derivative works, distribute copies to the public, perform publicly and display publicly, and to permit others to do so.","title":"News and Publications"},{"location":"api/","text":"API Reference Glossary Variable Definition N # of Batches C in # of input channels (i.e. features) D or D b Data or Batch depth (z) H or H b Data or Batch height (y) W or W b Data or Batch width (x) Train/Evaluate Train CLASS sapsan.lib.experiments.train.Train (model: Estimator, data_parameters: dict, backend = FakeBackend(), show_log = True, run_name = 'train') Call Train to set up your run Parameters Name Type Discription Default model object model to use for training data_parameters dict data parameters from the data loader, necessary for tracking backend object backend to track the experiment FakeBackend() show_log bool show the loss vs. epoch progress plot (it will be save in mlflow in either case) True run_name str 'run name' tag as recorded under MLflow train sapsan.lib.experiments.train.Train.run() Run the model Return Type Description pytorch or sklearn or custom type trained model Evaluate CLASS sapsan.lib.experiments.evaluate.Evaluate (model: Estimator, data_parameters: dict, backend = FakeBackend(), cmap: str = 'plasma', run_name: str = 'evaluate', **kwargs) Call Evaluate to set up the testing of the trained model. Don't forget to update estimator.loaders with the new data for testing. Parameters Name Type Discription Default model object model to use for testing data_parameters dict data parameters from the data loader, necessary for tracking backend obejct backend to track the experiment FakeBackend() cmap str matplotlib colormap to use for slice plots plasma run_name str 'run name' tag as recorded under MLflow evaluate pdf_xlim tuple x-axis limits for the PDF plot pdf_ylim tuple y-axis limits for the PDF plot sapsan.lib.experiments.evaluate.Evaluate.run() Run the evaluation of the trained model Return Type Description dict{'target' : np.ndarray, 'predict' : np.ndarray} target and predicted data Estimators CNN3d CLASS sapsan.lib.estimator.CNN3d (loaders, config, model) A model based on Pytorch's 3D Convolutional Neural Network Parameters Name Type Discription Default loaders dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) CNN3dConfig() config class configuration to use for the model CNN3dConfig() model class the model itself - should not be adjusted CNN3dModel() sapsan.lib.estimator.CNN3d.save (path: str) Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.CNN3d.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config bool updates config parameters from {path}/params.json . False Return Type Description pytorch model loaded model CLASS sapsan.lib.estimator.CNN3dConfig (n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs) Configuration for the CNN3d - based on pytorch and catalyst libraries Parameters Name Type Discription Default n_epochs int number of epochs 1 patience int number of epochs with no improvement after which training will be stopped. Default 10 min_delta float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement 1e-5 log_dir int path to store the logs ./logs/ lr float learning rate 1e-3 min_lr float a lower bound of the learning rate for ReduceLROnPlateau lr*1e-2 device str specify the device to run the model on cuda (or switch to cpu) loader_key str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' metric_key str the metric to use for early stop 'loss' ddp bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs. This is passed to Catalyst to activate the ddp flag in runner (see more Distributed Training Tutorial ; the runner is set up in pytorch_estimator.py ). Note: doesn't support jupyter notebooks - prepare a script! False PIMLTurb CLASS sapsan.lib.estimator.PIMLTurb (activ, loss, loaders, ks_stop, ks_frac, ks_scale, l1_scale, l1_beta, sigma, config, model) Physics-informed machine learning model to predict Reynolds-like stress tensor, \\(Re\\) , for turbulence modeling. Learn more on the wiki: PIMLTurb A custom loss function was developed for this model combining spatial (SmoothL1) and statistical (Kolmogorov-Smirnov) losses. Parameters Name Type Discription Default activ str activation function to use from PyTorch Tanhshrink loss str loss function to use; accepts only custom SmoothL1_KSLoss loaders dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) ks_stop float early-stopping condition based on the KS loss value alone 0.1 ks_frac float fraction the KS loss contributes to the total loss 0.5 ks_scale float scale factor to prioritize KS loss over SmoothL1 (should not be altered) 1 l1_scale float scale factor to prioritize SmoothL1 loss over KS 1 l1_beta float \\(beta\\) threshold for smoothing the L1 loss 1 sigma float \\(sigma\\) for the last layer of the network that performs a filtering operation using a Gaussian kernel 1 config class configuration to use for the model PIMLTurbConfig() model class the model itself - should not be adjusted PIMLTurbModel() sapsan.lib.estimator.PIMLTurb.save (path: str) Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.PIMLTurb.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config bool updates config parameters from {path}/params.json . False Return Type Description pytorch model loaded model CLASS sapsan.lib.estimator.PIMLTurbConfig (n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs) Configuration for the PIMLTurb - based on pytorch (catalyst is not used) Parameters Name Type Discription Default n_epochs int number of epochs 1 patience int number of epochs with no improvement after which training will be stopped (not used) 10 min_delta float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement (not used) 1e-5 log_dir int path to store the logs ./logs/ lr float learning rate 1e-3 min_lr float a lower bound of the learning rate for ReduceLROnPlateau lr*1e-2 device str specify the device to run the model on cuda (or switch to cpu) loader_key str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' metric_key str the metric to use for early stop 'loss' ddp bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs. This is passed to Catalyst to activate the ddp flag in runner (see more Distributed Training Tutorial ; the runner is set up in pytorch_estimator.py ). Note: doesn't support jupyter notebooks - prepare a script! False PICAE CLASS sapsan.lib.estimator.PICAE (loaders, config, model) Convolutional Auto Encoder with Divergence-Free Kernel and with periodic padding. Further details can be found on the PICAE page Parameters Name Type Discription Default loaders dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) config class configuration to use for the model PICAEConfig() model class the model itself - should not be adjusted PICAEModel() sapsan.lib.estimator.PICAE.save (path: str) Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.PICAE.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config > bool updates config parameters from {path}/params.json False Return Type Description pytorch model loaded model CLASS sapsan.lib.estimator.PICAEConfig (n_epochs, patience, min_delta, logdir, lr, min_lr, weight_decay, nfilters, kernel_size, enc_nlayers, dec_nlayers, *args, **kwargs) Configuration for the CNN3d - based on pytorch and catalyst libraries Parameters Name Type Discription Default n_epochs int number of epochs 1 batch_dim int dimension of a batch in each axis 64 patience int number of epochs with no improvement after which training will be stopped 10 min_delta float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement 1e-5 log_dir str path to store the logs ./logs/ lr float learning rate 1e-3 min_lr float a lower bound of the learning rate for ReduceLROnPlateau lr*1e-2 weight_decay float weight decay (L2 penalty) 1e-5 nfilters int the output dim for each convolutional layer, which is the number of \"filters\" learned by that layer 6 kernel_size tuple size of the convolutional kernel (3,3,3) enc_layers int number of encoding layers 3 dec_layers int number of decoding layers 3 device str specify the device to run the model on cuda (or switch to cpu) loader_key str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' metric_key str the metric to use for early stop 'loss' ddp bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs. This is passed to Catalyst to activate the ddp flag in runner (see more Distributed Training Tutorial ; the runner is set up in pytorch_estimator.py ). Note: doesn't support jupyter notebooks - prepare a script! False KRR CLASS sapsan.lib.estimator.KRR (loaders, config, model) A model based on sk-learn Kernel Ridge Regression Parameters Name Type Discription Default loaders list contains input and target data config class configuration to use for the model KRRConfig() model class the model itself - should not be adjusted KRRModel() sapsan.lib.estimator.KRR.save (path: str) Saves the model Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.KRR.load (path: str, estimator, load_saved_config = False) Loads the model Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config bool updates config parameters from {path}/params.json False Return Type Description sklearn model loaded model CLASS sapsan.lib.estimator.KRRConfig (alpha, gamma) Configuration for the KRR model Parameters Name Type Discription Default alpha float regularization term, hyperparameter None gamma float full-width at half-max for the RBF kernel, hyperparameter None load_estimator CLASS sapsan.lib.estimator.load_estimator () Dummy estimator to call load() to load the saved pytorch models sapsan.lib.estimator.load_estimator.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further load_saved_config bool updates config parameters from {path}/params.json False Return Type Description pytorch model loaded model load_sklearn_estimator CLASS sapsan.lib.estimator.load_sklearn_estimator () Dummy estimator to call load() to load the saved sklearn models sapsan.lib.estimator.load_sklearn_estimator.load (path: str, estimator, load_saved_config = False) Loads model Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup to keep training the model further load_saved_config bool updates config parameters from {path}/params.json False Return Type Description sklearn model loaded model Torch Modules Gaussian CLASS sapsan.lib.estimator.torch_modules.Gaussian (sigma: int) [3D] Applies a Guassian filter as a torch layer through a series of 3 separable 1D convolutions, utilizing torch.nn.funcitonal.conv3d . CUDA is supported. Parameters Name Type Discription Default sigma int standard deviation \\(\\sigma\\) for a Gaussian kernel 2 sapsan.lib.estimator.torch_modules.Gaussian.forward (tensor: torch.tensor) Parameters Name Type Discription Default tensor torch.tensor input torch tensor of shape [N, C in , D in , H in , W in ] Return Type Description torch.tensor filtered 3D torch data Interp1d CLASS sapsan.lib.estimator.torch_modules.Interp1d () Linear 1D interpolation done in native PyTorch. CUDA is supported. Forked from @aliutkus sapsan.lib.estimator.torch_modules.Interp1d.forward (x: torch.tensor, y: torch.tensor, xnew: torch.tensor, out: torch.tensor) Parameters Name Type Discription Default x torch.tensor 1D or 2D tensor y torch.tensor 1D or 2D tensor; the length of y along its last dimension must be the same as that of x xnew torch.tensor 1D or 2D tensor of real values. xnew can only be 1D if both x and y are 1D. Otherwise, its length along the first dimension must be the same as that of whichever x and y is 2D. out torch.tensor Tensor for the output If None , allocated automatically Return Type Description torch.tensor interpolated tensor Data Loaders HDF5Dataset CLASS sapsan.lib.data.hdf5_dataset.HDF5Dataset ( path: str, features: List[str], target: List[str], checkpoints: List[int], batch_size: int = None, input_size: int = None, sampler: Optional[Sampling] = None, time_granularity: float = 1, features_label: Optional[List[str]] = None, target_label: Optional[List[str]] = None, flat: bool = False, shuffle: bool=False, train_fraction = None) HDF5 data loader class Parameters Name Type Discription Default path str path to the data in the following format: \"data/t_{checkpoint:1.0f}/{feature}_data.h5\" features List[str] list of train features to load ['not_specified_data'] target List[str] list of target features to load None checkpoints List[int] list of checkpoints to load (they will be appended as batches) input_size int dimension of the loaded data in each axis batch_size int dimension of a batch in each axis. If batch_size != input_size, the datacube will be evenly splitted input_size (doesn't work with sampler ) batch_num int the number of batches to be loaded at a time 1 sampler object data sampler to use (ex: EquidistantSampling()) None time_granularity float what is the time separation (dt) between checkpoints 1 features_label List[str] hdf5 data label for the train features list(file.keys())[-1], i.e. last one in hdf5 file target_label List[str] hdf5 data label for the target features list(file.keys())[-1], i.e. last one in hdf5 file flat bool flatten the data into [C in , D*H*W]. Required for sk-learn models False shuffle bool shuffle the dataset False train_fraction float or int a fraction of the dataset to be used for training (accessed through loaders['train']). The rest will be used for validation (accessed through loaders['valid']). If int is provided, then that number of batches will be used for training. If float is provided, then it will try to split the data either by batch or by actually slicing the data cube into smaller chunks None - training data will be used for validation, effectively skipping the latter sapsan.lib.data.hdf5_dataset.HDF5Dataset.load_numpy() HDF5 data loader method - call it to load the data as a numpy array. If targets are not specified, than only features will be loaded (hence you can just load 1 dataset at a time). Return Type Description np.ndarray, np.ndarray loaded a dataset as a numpy array sapsan.lib.data.hdf5_dataset.HDF5Dataset.convert_to_torch([x, y]) Splits numpy arrays into batches and converts to torch dataloader Parameters Name Type Discription Default [x, y] list or np.ndarray a list of input datasets to batch and convert to torch loaders Return Type Description OrderedDict{'train' : DataLoader, 'valid' : DataLoader } Data in Torch Dataloader format ready for training sapsan.lib.data.hdf5_dataset.HDF5Dataset.load() Loads, splits into batches, and converts into torch dataloader. Effectively combines .load_numpy and .convert_to_torch Return Type Description np.ndarray, np.ndarray loaded train and target features: x, y get_loader_shape sapsan.lib.data.data_functions.get_loader_shape() Returns the shape of the loaded tensors - the loaded data that has been split into train and valid datasets. Parameters Name Type Discription Default loaders torch DataLoader the loader of tensors passed for training name str name of the dataset in the loaders; usually either train or valid None - chooses the first entry in loaders Return Type Description np.ndarray shape of the tensor Data Manipulation EquidistantSampling CLASS sapsan.lib.data.sampling.EquidistantSampling (target_dim) Samples the data to a lower dimension, keeping separation between the data points equally distant Parameters Name Type Discription Default target_dim np.ndarray new shape of the input in the form [D, H, W] sapsan.lib.data.sampling.EquidistantSampling.sample (data) Performs sampling of the data Parameters Name Type Discription Default data np.ndarray input data to be sampled - has the shape of [axis, D, H, W] Return Type Description np.ndarray Sampled data with the shape [axis, D, H, W] split_data_by_batch sapsan.utils.shapes.split_data_by_batch (data: np.ndarray, size: int, batch_size: int, n_features: int, axis: int) [2D, 3D]: splits data into smaller cubes or squares of batches Parameters Name Type Discription Default data np.ndarray input 2D or 3D data, [C in , D, H, W] size int dimensionality of the data in each axis batch_size int dimensionality of the batch in each axis n_features int number of channels of the input data axis int number of axes, 2 or 3 Return Type Description np.ndarray batched data: [N, C in , D b , H b , W b ] combine_data sapsan.utils.shapes.combine_data (data: np.ndarray, input_size: tuple, batch_size: tuple, axis: int) [2D, 3D] - reverse of split_data_by_batch function Parameters Name Type Discription Default data np.ndarray input 2D or 3D data, [N, C in , D b , H b , W b ] input_size tuple dimensionality of the original data in each axis batch_size tuple dimensionality of the batch in each axis axis int number of axes, 2 or 3 Return Type Description np.ndarray reassembled data: [C in , D, H, W] slice_of_cube sapsan.utils.shapes.slice_of_cube (data: np.ndarray, feature: Optional[int] = None, n_slice: Optional[int] = None) Select a slice of a cube (to plot later) Parameters Name Type Discription Default data np.ndarray input 3D data, [C in , D, H, W] feature int feature to take the slice of, i.e. the value of C in 1 n_slice int what slice to select, i.e. the value of D 1 Return Type Description np.ndarray data slice: [H, W] Filter spectral sapsan.utils.filter.spectral (im: np.ndarray, fm: int) [2D, 3D] apply a spectral filter Parameters Name Type Discription Default im np.ndarray input dataset (ex: [C in , D, H, W]) fm int number of Fourier modes to filter down to Return Type Description np.ndarray filtered dataset box sapsan.utils.filter.box (im: np.ndarray, ksize) [2D] apply a box filter Parameters Name Type Discription Default im np.ndarray input dataset (ex: [C in , H, W]) ksize tupple kernel size (ex: ksize = (2,2)) Return Type Description np.ndarray filtered dataset gaussian sapsan.utils.filter.gaussian (im: np.ndarray, sigma) [2D, 3D] apply a gaussian filter Note: Guassian filter assumes dx=1 between the points. Adjust sigma accordingly. Parameters Name Type Discription Default im np.ndarray input dataset (ex: [H, W] or [D, H, W]) sigma float or tuple of floats standard deviation for Gaussian kernel. Sigma can be defined for each axis individually. Return Type Description np.ndarray filtered dataset Backend (Tracking) MLflowBackend CLASS sapsan.lib.backends.mlflow.MLflowBackend (name, host, port) Initilizes mlflow and starts up mlflow ui at a given host:port Parameters Name Type Discription Default name str name under which to record the experiment \"experiment\" host str host of mlflow ui \"localhost\" port int port of mlflow ui 9000 sapsan.lib.backends.mlflow.MLflowBackend.start_ui () starts MLflow ui at a specified host and port sapsan.lib.backends.mlflow.MLflowBackend.start (run_name: str, nested = False, run_id = None) Starts a tracking run Parameters Name Type Discription Default run_name str name of the run \"train\" for Train() , \"evaluate\" for Evaluate() nested bool whether or not to nest the recorded run False for Train() , True for Evaluate() run_id str run id None - a new will be generated Return Type Description str run_id sapsan.lib.backends.mlflow.MLflowBackend.resume (run_id, nested = True) Resumes a previous run, so you can record extra parameters Parameters Name Type Discription Default run_id str id of the run to resume nested bool whether or not to nest the recorded run True, since it will usually be an Evaluate() run sapsan.lib.backends.mlflow.MLflowBackend.log_metric () Logs a metric sapsan.lib.backends.mlflow.MLflowBackend.log_parameter () Logs a parameter sapsan.lib.backends.mlflow.MLflowBackend.log_artifact () Logs an artifact (any saved file such, e.g. .png, .txt) sapsan.lib.backends.mlflow.MLflowBackend.close_active_run () Closes all active MLflow runs sapsan.lib.backends.mlflow.MLflowBackend.end () Ends the most recent MLflow run FakeBackend CLASS sapsan.lib.backends.fake.FakeBackend() Pass to train in order to disable backend (tracking) Plotting plot_params sapsan.utils.plot.plot_params() Contains the matplotlib parameters that format all of the plots ( font.size , axes.labelsize , etc.) Return Type Description dict matplotlib parameters Default Parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def plot_params (): params = { 'font.size' : 14 , 'legend.fontsize' : 14 , 'axes.labelsize' : 20 , 'axes.titlesize' : 24 , 'xtick.labelsize' : 17 , 'ytick.labelsize' : 17 , 'axes.linewidth' : 1 , 'patch.linewidth' : 3 , 'lines.linewidth' : 3 , 'xtick.major.width' : 1.5 , 'ytick.major.width' : 1.5 , 'xtick.minor.width' : 1.25 , 'ytick.minor.width' : 1.25 , 'xtick.major.size' : 7 , 'ytick.major.size' : 7 , 'xtick.minor.size' : 4 , 'ytick.minor.size' : 4 , 'xtick.direction' : 'in' , 'ytick.direction' : 'in' , 'axes.formatter.limits' : [ - 7 , 7 ], 'axes.grid' : True , 'grid.linestyle' : ':' , 'grid.color' : '#999999' , 'text.usetex' : False ,} return params pdf_plot sapsan.utils.plot.pdf_plot (series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, figsize: tuple, dpi: int, ax: matplotlib.axes, style: str) Plot a probability density function (PDF) of a single or multiple datasets Parameters Name Type Discription Default series List[np.ndarray] input datasets bins int number of bins to use for the dataset to generate the pdf 100 label List[str] list of names to use as labels in the legend None figsize tuple figure size as passed to matplotlib figure (6,6) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure style str accepts matplotlib styles 'tableau-colorblind10' Return Type Description matplotlib.axes object ax cdf_plot sapsan.utils.plot.cdf_plot (series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, figsize: tuple, dpi: int, ax: matplotlib.axes, ks: bool, style: str) Plot a cumulative distribution function (CDF) of a single or multiple datasets Parameters Name Type Discription Default series List[np.ndarray] input datasets bins int number of bins to use for the dataset to generate the pdf 100 label List[str] list of names to use as labels in the legend None figsize tuple figure size as passed to matplotlib figure (6,6) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure ks bool if True prints out on the plot itself the Kolomogorov-Smirnov Statistic . It will also be returned along with the ax object False style str accepts matplotlib styles 'tableau-colorblind10' Return Type Description matplotlib.axes object, float (if ks==True) ax, ks (if ks==True) line_plot sapsan.utils.plot.line_plot (series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, plot_type: str, figsize: tuple, dpi: int, ax: matplotlib.axes, style: str) Plot linear data of x vs y - same matplotlib formatting will be used as the other plots Parameters Name Type Discription Default series List[np.ndarray] input datasets bins int number of bins to use for the dataset to generate the pdf 100 label List[str] list of names to use as labels in the legend None plot_type str axis type of the matplotlib plot; options = ['plot', 'semilogx', 'semilogy', 'loglog'] 'plot' figsize tuple figure size as passed to matplotlib figure (6,6) linestyle List[str] list of linestyles to use for each profile for the matplotlib figure ['-'] (solid line) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure style str accepts matplotlib styles 'tableau-colorblind10' Return Type Description matplotlib.axes object ax slice_plot sapsan.utils.plot.slice_plot (series: List[np.ndarray], label: Optional[List[str]] = None, cmap = 'plasma', figsize: tuple, dpi: int, ax: matplotlib.axes) Plot 2D spatial distributions (slices) of your target and prediction datasets Parameters Name Type Discription Default series List[np.ndarray] input datasets label List[str] list of names to use as labels in the legend None cmap str matplotlib colormap to use 'plasma' figsize tuple figure size as passed to matplotlib figure (6,6) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) WARNING: only works if a single image is supplied to slice_plot() , otherwise will be ignored None - creates a separate figure Return Type Description matplotlib.axes object ax log_plot sapsan.utils.plot.log_plot (show_log = True, log_path = 'logs/logs/train.csv', valid_log_path = 'logs/logs/valid.csv', delimiter=',', train_name = 'train_loss', valid_name = 'valid_loss', train_column = 1, valid_column = 1, epoch_column = 0) Plots an interactive training log of train_loss vs. epoch with plotly Parameters Name Type Discription Default show_log bool show the loss vs. epoch progress plot (it will be save in mlflow in either case) True log_path str path to training log produced by the catalyst wrapper 'logs/logs/train.csv' valid_log_path str path to validation log produced by the catalyst wrapper 'logs/logs/valid.csv' delimiter str delimiter to use for numpy.genfromtxt data loading ',' train_name str name for the training label 'train_loss' valid_name str name for the validation label 'valid_loss' train_column int column to load for training data from log_path 1 valid_column int column to load for validation data from valid_log_path 1 epoch_column int column to load the epoch index from log_path . If None , then epoch will be generated fro the number of entries 0 Return Type Description plotly.express object plot figure model_graph sapsan.utils.plot.model_graph (model, shape: np.array, transforms) Creates a graph of the ML model (needs graphviz to be installed). A tutorial is available on the wiki: Model Graph The method is based on hiddenlayer originally written by Waleed Abdulla. Parameters Name Type Discription Default model object initialized pytorch or tensorflow model shape np.array shape of the input array in the form [N, C in , D b , H b , W b ], where C in =1 transforms list[methods] a list of hiddenlayer transforms to be applied ( Fold, FoldId, Prune, PruneBranch, FoldDuplicates, Rename ) See below Default Parameters 1 2 3 4 5 6 7 8 9 10 11 12 import sapsan.utils.hiddenlayer as hl transforms = [ hl . transforms . Fold ( \"Conv > MaxPool > Relu\" , \"ConvPoolRelu\" ), hl . transforms . Fold ( \"Conv > MaxPool\" , \"ConvPool\" ), hl . transforms . Prune ( \"Shape\" ), hl . transforms . Prune ( \"Constant\" ), hl . transforms . Prune ( \"Gather\" ), hl . transforms . Prune ( \"Unsqueeze\" ), hl . transforms . Prune ( \"Concat\" ), hl . transforms . Rename ( \"Cast\" , to = \"Input\" ), hl . transforms . FoldDuplicates () ] Return Type Description graphviz.Digraph object graph of a model Physics ReynoldsStress sapsan.utils.physics.ReynoldsStress (u, filt, filt_size, only_x_components=False) Calculates a stress tensor of the form \\[ \\tau_{ij} = \\widetilde{u_i u_j} - \\tilde{u}_i\\tilde{u}_j \\] where \\(\\tilde{u}\\) is the filtered \\(u\\) Parameters Name Type Discription Default u np.ndarray input velocity in 3D - [axis, D, H, W] filt sapsan.utils.filters the type of filter to use (spectral, box, gaussian). Pass the filter itself by loading the appropriate one from sapsan.utils.filters gaussian filt_size int or float size of the filter to apply. For different filter types, the size is defined differently. Spectral - fourier mode to filter to, Box - k_size (box size), Gaussian - sigma 2 (sigma=2 for gaussian) only_x_component bool calculates and outputs only x components of the tensor in shape [row, D, H, W] - calculating all 9 can be taxing on memory False Return Type Description np.ndarray stress tensor of shape [column, row, D, H, W] PowerSpectrum CLASS sapsan.utils.physics.PowerSpectrum (u: np.ndarray) Sets up to produce a power spectrum Parameters Name Type Discription Default u np.ndarray input velocity in 3D - [axis, D, H, W] sapsan.utils.physics.PowerSpectrum.calculate() Calculates the power spectrum Return Type Description np.ndarray, np.ndarray k_bins (fourier modes), Ek_bins (E(k)) sapsan.utils.physics.PowerSpectrum.spectrum_plot (k_bins, Ek_bins, kolmogorov=True, kl_a) Plots the calculated power spectrum Parameters Name Type Discription Default k_bins np.ndarray fourier mode values along x-axis Ek_bins np.ndarray energy as a function of k: E(k) kolmogorov bool plots scaled Kolmogorov's -5/3 spectrum alongside the calculated one True kl_A float scaling factor of Kolmogorov's law np.amax(self.Ek_bins)*1e1 Return Type Description matplotlib.axes object spectrum plot GradientModel CLASS sapsan.utils.physics.GradientModel (u: np.ndarray, filter_width, delta_u = 1) sets up to apply a gradient turbulence subgrid model: \\[ \\tau_{ij} = \\frac{1}{12} \\Delta^2 \\,\\delta_k u^*_i \\,\\delta_k u^*_j \\] where \\(\\Delta\\) is the filter width and \\(u^*\\) is the filtered \\(u\\) Parameters Name Type Discription Default u np.ndarray input filtered quantity in 3D - [axis, D, H, W] filter_width float width of the filter which was applied onto u delta_u distance between the points on the grid to use for scaling 1 sapsan.utils.physics.GradientModel.gradient() calculated the gradient of the given input data from GradientModel Return Type Description np.ndarray gradient with shape [column, row, D, H, W] sapsan.utils.physics.GradientModel.model() calculates the gradient model tensor with shape [column, row, D, H, W] Return Type Description np.ndarray gradient model tensor DynamicSmagorinskyModel CLASS sapsan.utils.physics.DynamicSmagorinskyModel (u: np.ndarray, filt, original_filt_size, filt_ratio, du, delta_u) sets up to apply a Dynamic Smagorinsky (DS) turbulence subgrid model: \\[ \\tau_{ij} = -2(C_s\\Delta^*)^2|S^*|S^*_{ij} \\] where \\(\\Delta\\) is the filter width and \\(S^*\\) is the filtered \\(u\\) Parameters Name Type Discription Default u np.ndarray input filtered quantity either in 3D [axis, D, H, W] or 2D [axis, D, H] du np.ndarray gradient of u None*: if du is not provided, then it will be calculated with np.gradient() filt sapsan.utils.filters the type of filter to use (spectral, box, gaussian). Pass the filter itself by loading the appropriate one from sapsan.utils.filters spectral original_fil_size int width of the filter which was applied onto u 15 (spectral, fourier modes = 15) delta_u float distance between the points on the grid to use for scaling 1 filt_ratio float the ratio of additional filter that will be applied on the data to find the slope for Dynamic Smagorinsky extrapolation over original_filt_size 0.5 sapsan.utils.physics.DynamicSmagorinskyModel.model() calculates the DS model tensor with shape [column, row, D, H, W] Return Type Description np.ndarray DS model tensor","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#glossary","text":"Variable Definition N # of Batches C in # of input channels (i.e. features) D or D b Data or Batch depth (z) H or H b Data or Batch height (y) W or W b Data or Batch width (x)","title":"Glossary"},{"location":"api/#trainevaluate","text":"","title":"Train/Evaluate"},{"location":"api/#train","text":"CLASS sapsan.lib.experiments.train.Train (model: Estimator, data_parameters: dict, backend = FakeBackend(), show_log = True, run_name = 'train') Call Train to set up your run Parameters Name Type Discription Default model object model to use for training data_parameters dict data parameters from the data loader, necessary for tracking backend object backend to track the experiment FakeBackend() show_log bool show the loss vs. epoch progress plot (it will be save in mlflow in either case) True run_name str 'run name' tag as recorded under MLflow train sapsan.lib.experiments.train.Train.run() Run the model Return Type Description pytorch or sklearn or custom type trained model","title":"Train"},{"location":"api/#evaluate","text":"CLASS sapsan.lib.experiments.evaluate.Evaluate (model: Estimator, data_parameters: dict, backend = FakeBackend(), cmap: str = 'plasma', run_name: str = 'evaluate', **kwargs) Call Evaluate to set up the testing of the trained model. Don't forget to update estimator.loaders with the new data for testing. Parameters Name Type Discription Default model object model to use for testing data_parameters dict data parameters from the data loader, necessary for tracking backend obejct backend to track the experiment FakeBackend() cmap str matplotlib colormap to use for slice plots plasma run_name str 'run name' tag as recorded under MLflow evaluate pdf_xlim tuple x-axis limits for the PDF plot pdf_ylim tuple y-axis limits for the PDF plot sapsan.lib.experiments.evaluate.Evaluate.run() Run the evaluation of the trained model Return Type Description dict{'target' : np.ndarray, 'predict' : np.ndarray} target and predicted data","title":"Evaluate"},{"location":"api/#estimators","text":"","title":"Estimators"},{"location":"api/#cnn3d","text":"CLASS sapsan.lib.estimator.CNN3d (loaders, config, model) A model based on Pytorch's 3D Convolutional Neural Network Parameters Name Type Discription Default loaders dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) CNN3dConfig() config class configuration to use for the model CNN3dConfig() model class the model itself - should not be adjusted CNN3dModel() sapsan.lib.estimator.CNN3d.save (path: str) Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.CNN3d.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config bool updates config parameters from {path}/params.json . False Return Type Description pytorch model loaded model CLASS sapsan.lib.estimator.CNN3dConfig (n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs) Configuration for the CNN3d - based on pytorch and catalyst libraries Parameters Name Type Discription Default n_epochs int number of epochs 1 patience int number of epochs with no improvement after which training will be stopped. Default 10 min_delta float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement 1e-5 log_dir int path to store the logs ./logs/ lr float learning rate 1e-3 min_lr float a lower bound of the learning rate for ReduceLROnPlateau lr*1e-2 device str specify the device to run the model on cuda (or switch to cpu) loader_key str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' metric_key str the metric to use for early stop 'loss' ddp bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs. This is passed to Catalyst to activate the ddp flag in runner (see more Distributed Training Tutorial ; the runner is set up in pytorch_estimator.py ). Note: doesn't support jupyter notebooks - prepare a script! False","title":"CNN3d"},{"location":"api/#pimlturb","text":"CLASS sapsan.lib.estimator.PIMLTurb (activ, loss, loaders, ks_stop, ks_frac, ks_scale, l1_scale, l1_beta, sigma, config, model) Physics-informed machine learning model to predict Reynolds-like stress tensor, \\(Re\\) , for turbulence modeling. Learn more on the wiki: PIMLTurb A custom loss function was developed for this model combining spatial (SmoothL1) and statistical (Kolmogorov-Smirnov) losses. Parameters Name Type Discription Default activ str activation function to use from PyTorch Tanhshrink loss str loss function to use; accepts only custom SmoothL1_KSLoss loaders dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) ks_stop float early-stopping condition based on the KS loss value alone 0.1 ks_frac float fraction the KS loss contributes to the total loss 0.5 ks_scale float scale factor to prioritize KS loss over SmoothL1 (should not be altered) 1 l1_scale float scale factor to prioritize SmoothL1 loss over KS 1 l1_beta float \\(beta\\) threshold for smoothing the L1 loss 1 sigma float \\(sigma\\) for the last layer of the network that performs a filtering operation using a Gaussian kernel 1 config class configuration to use for the model PIMLTurbConfig() model class the model itself - should not be adjusted PIMLTurbModel() sapsan.lib.estimator.PIMLTurb.save (path: str) Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.PIMLTurb.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config bool updates config parameters from {path}/params.json . False Return Type Description pytorch model loaded model CLASS sapsan.lib.estimator.PIMLTurbConfig (n_epochs, patience, min_delta, logdir, lr, min_lr, *args, **kwargs) Configuration for the PIMLTurb - based on pytorch (catalyst is not used) Parameters Name Type Discription Default n_epochs int number of epochs 1 patience int number of epochs with no improvement after which training will be stopped (not used) 10 min_delta float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement (not used) 1e-5 log_dir int path to store the logs ./logs/ lr float learning rate 1e-3 min_lr float a lower bound of the learning rate for ReduceLROnPlateau lr*1e-2 device str specify the device to run the model on cuda (or switch to cpu) loader_key str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' metric_key str the metric to use for early stop 'loss' ddp bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs. This is passed to Catalyst to activate the ddp flag in runner (see more Distributed Training Tutorial ; the runner is set up in pytorch_estimator.py ). Note: doesn't support jupyter notebooks - prepare a script! False","title":"PIMLTurb"},{"location":"api/#picae","text":"CLASS sapsan.lib.estimator.PICAE (loaders, config, model) Convolutional Auto Encoder with Divergence-Free Kernel and with periodic padding. Further details can be found on the PICAE page Parameters Name Type Discription Default loaders dict contains input and target data (loaders['train'], loaders['valid']). Datasets themselves have to be torch.tensor(s) config class configuration to use for the model PICAEConfig() model class the model itself - should not be adjusted PICAEModel() sapsan.lib.estimator.PICAE.save (path: str) Saves model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.PICAE.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config > bool updates config parameters from {path}/params.json False Return Type Description pytorch model loaded model CLASS sapsan.lib.estimator.PICAEConfig (n_epochs, patience, min_delta, logdir, lr, min_lr, weight_decay, nfilters, kernel_size, enc_nlayers, dec_nlayers, *args, **kwargs) Configuration for the CNN3d - based on pytorch and catalyst libraries Parameters Name Type Discription Default n_epochs int number of epochs 1 batch_dim int dimension of a batch in each axis 64 patience int number of epochs with no improvement after which training will be stopped 10 min_delta float minimum change in the monitored metric to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement 1e-5 log_dir str path to store the logs ./logs/ lr float learning rate 1e-3 min_lr float a lower bound of the learning rate for ReduceLROnPlateau lr*1e-2 weight_decay float weight decay (L2 penalty) 1e-5 nfilters int the output dim for each convolutional layer, which is the number of \"filters\" learned by that layer 6 kernel_size tuple size of the convolutional kernel (3,3,3) enc_layers int number of encoding layers 3 dec_layers int number of decoding layers 3 device str specify the device to run the model on cuda (or switch to cpu) loader_key str the loader to use for early stop: train or valid first loader provided*, which is usually 'train' metric_key str the metric to use for early stop 'loss' ddp bool turn on Distributed Data Parallel (DDP) in order to distribute the data and train the model across multiple GPUs. This is passed to Catalyst to activate the ddp flag in runner (see more Distributed Training Tutorial ; the runner is set up in pytorch_estimator.py ). Note: doesn't support jupyter notebooks - prepare a script! False","title":"PICAE"},{"location":"api/#krr","text":"CLASS sapsan.lib.estimator.KRR (loaders, config, model) A model based on sk-learn Kernel Ridge Regression Parameters Name Type Discription Default loaders list contains input and target data config class configuration to use for the model KRRConfig() model class the model itself - should not be adjusted KRRModel() sapsan.lib.estimator.KRR.save (path: str) Saves the model Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively sapsan.lib.estimator.KRR.load (path: str, estimator, load_saved_config = False) Loads the model Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further. load_saved_config bool updates config parameters from {path}/params.json False Return Type Description sklearn model loaded model CLASS sapsan.lib.estimator.KRRConfig (alpha, gamma) Configuration for the KRR model Parameters Name Type Discription Default alpha float regularization term, hyperparameter None gamma float full-width at half-max for the RBF kernel, hyperparameter None","title":"KRR"},{"location":"api/#load_estimator","text":"CLASS sapsan.lib.estimator.load_estimator () Dummy estimator to call load() to load the saved pytorch models sapsan.lib.estimator.load_estimator.load (path: str, estimator, load_saved_config = False) Loads model and optimizer states, as well as final epoch and loss Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup, changing n_epochs to keep training the model further load_saved_config bool updates config parameters from {path}/params.json False Return Type Description pytorch model loaded model","title":"load_estimator"},{"location":"api/#load_sklearn_estimator","text":"CLASS sapsan.lib.estimator.load_sklearn_estimator () Dummy estimator to call load() to load the saved sklearn models sapsan.lib.estimator.load_sklearn_estimator.load (path: str, estimator, load_saved_config = False) Loads model Parameters Name Type Discription Default path str save path of the model and its config parameters, {path}/model.pt and {path}/params.json respectively estimator estimator need to provide an initialized model for which to load the weights. The estimator can include a new config setup to keep training the model further load_saved_config bool updates config parameters from {path}/params.json False Return Type Description sklearn model loaded model","title":"load_sklearn_estimator"},{"location":"api/#torch-modules","text":"","title":"Torch Modules"},{"location":"api/#gaussian","text":"CLASS sapsan.lib.estimator.torch_modules.Gaussian (sigma: int) [3D] Applies a Guassian filter as a torch layer through a series of 3 separable 1D convolutions, utilizing torch.nn.funcitonal.conv3d . CUDA is supported. Parameters Name Type Discription Default sigma int standard deviation \\(\\sigma\\) for a Gaussian kernel 2 sapsan.lib.estimator.torch_modules.Gaussian.forward (tensor: torch.tensor) Parameters Name Type Discription Default tensor torch.tensor input torch tensor of shape [N, C in , D in , H in , W in ] Return Type Description torch.tensor filtered 3D torch data","title":"Gaussian"},{"location":"api/#interp1d","text":"CLASS sapsan.lib.estimator.torch_modules.Interp1d () Linear 1D interpolation done in native PyTorch. CUDA is supported. Forked from @aliutkus sapsan.lib.estimator.torch_modules.Interp1d.forward (x: torch.tensor, y: torch.tensor, xnew: torch.tensor, out: torch.tensor) Parameters Name Type Discription Default x torch.tensor 1D or 2D tensor y torch.tensor 1D or 2D tensor; the length of y along its last dimension must be the same as that of x xnew torch.tensor 1D or 2D tensor of real values. xnew can only be 1D if both x and y are 1D. Otherwise, its length along the first dimension must be the same as that of whichever x and y is 2D. out torch.tensor Tensor for the output If None , allocated automatically Return Type Description torch.tensor interpolated tensor","title":"Interp1d"},{"location":"api/#data-loaders","text":"","title":"Data Loaders"},{"location":"api/#hdf5dataset","text":"CLASS sapsan.lib.data.hdf5_dataset.HDF5Dataset ( path: str, features: List[str], target: List[str], checkpoints: List[int], batch_size: int = None, input_size: int = None, sampler: Optional[Sampling] = None, time_granularity: float = 1, features_label: Optional[List[str]] = None, target_label: Optional[List[str]] = None, flat: bool = False, shuffle: bool=False, train_fraction = None) HDF5 data loader class Parameters Name Type Discription Default path str path to the data in the following format: \"data/t_{checkpoint:1.0f}/{feature}_data.h5\" features List[str] list of train features to load ['not_specified_data'] target List[str] list of target features to load None checkpoints List[int] list of checkpoints to load (they will be appended as batches) input_size int dimension of the loaded data in each axis batch_size int dimension of a batch in each axis. If batch_size != input_size, the datacube will be evenly splitted input_size (doesn't work with sampler ) batch_num int the number of batches to be loaded at a time 1 sampler object data sampler to use (ex: EquidistantSampling()) None time_granularity float what is the time separation (dt) between checkpoints 1 features_label List[str] hdf5 data label for the train features list(file.keys())[-1], i.e. last one in hdf5 file target_label List[str] hdf5 data label for the target features list(file.keys())[-1], i.e. last one in hdf5 file flat bool flatten the data into [C in , D*H*W]. Required for sk-learn models False shuffle bool shuffle the dataset False train_fraction float or int a fraction of the dataset to be used for training (accessed through loaders['train']). The rest will be used for validation (accessed through loaders['valid']). If int is provided, then that number of batches will be used for training. If float is provided, then it will try to split the data either by batch or by actually slicing the data cube into smaller chunks None - training data will be used for validation, effectively skipping the latter sapsan.lib.data.hdf5_dataset.HDF5Dataset.load_numpy() HDF5 data loader method - call it to load the data as a numpy array. If targets are not specified, than only features will be loaded (hence you can just load 1 dataset at a time). Return Type Description np.ndarray, np.ndarray loaded a dataset as a numpy array sapsan.lib.data.hdf5_dataset.HDF5Dataset.convert_to_torch([x, y]) Splits numpy arrays into batches and converts to torch dataloader Parameters Name Type Discription Default [x, y] list or np.ndarray a list of input datasets to batch and convert to torch loaders Return Type Description OrderedDict{'train' : DataLoader, 'valid' : DataLoader } Data in Torch Dataloader format ready for training sapsan.lib.data.hdf5_dataset.HDF5Dataset.load() Loads, splits into batches, and converts into torch dataloader. Effectively combines .load_numpy and .convert_to_torch Return Type Description np.ndarray, np.ndarray loaded train and target features: x, y","title":"HDF5Dataset"},{"location":"api/#get_loader_shape","text":"sapsan.lib.data.data_functions.get_loader_shape() Returns the shape of the loaded tensors - the loaded data that has been split into train and valid datasets. Parameters Name Type Discription Default loaders torch DataLoader the loader of tensors passed for training name str name of the dataset in the loaders; usually either train or valid None - chooses the first entry in loaders Return Type Description np.ndarray shape of the tensor","title":"get_loader_shape"},{"location":"api/#data-manipulation","text":"","title":"Data Manipulation"},{"location":"api/#equidistantsampling","text":"CLASS sapsan.lib.data.sampling.EquidistantSampling (target_dim) Samples the data to a lower dimension, keeping separation between the data points equally distant Parameters Name Type Discription Default target_dim np.ndarray new shape of the input in the form [D, H, W] sapsan.lib.data.sampling.EquidistantSampling.sample (data) Performs sampling of the data Parameters Name Type Discription Default data np.ndarray input data to be sampled - has the shape of [axis, D, H, W] Return Type Description np.ndarray Sampled data with the shape [axis, D, H, W]","title":"EquidistantSampling"},{"location":"api/#split_data_by_batch","text":"sapsan.utils.shapes.split_data_by_batch (data: np.ndarray, size: int, batch_size: int, n_features: int, axis: int) [2D, 3D]: splits data into smaller cubes or squares of batches Parameters Name Type Discription Default data np.ndarray input 2D or 3D data, [C in , D, H, W] size int dimensionality of the data in each axis batch_size int dimensionality of the batch in each axis n_features int number of channels of the input data axis int number of axes, 2 or 3 Return Type Description np.ndarray batched data: [N, C in , D b , H b , W b ]","title":"split_data_by_batch"},{"location":"api/#combine_data","text":"sapsan.utils.shapes.combine_data (data: np.ndarray, input_size: tuple, batch_size: tuple, axis: int) [2D, 3D] - reverse of split_data_by_batch function Parameters Name Type Discription Default data np.ndarray input 2D or 3D data, [N, C in , D b , H b , W b ] input_size tuple dimensionality of the original data in each axis batch_size tuple dimensionality of the batch in each axis axis int number of axes, 2 or 3 Return Type Description np.ndarray reassembled data: [C in , D, H, W]","title":"combine_data"},{"location":"api/#slice_of_cube","text":"sapsan.utils.shapes.slice_of_cube (data: np.ndarray, feature: Optional[int] = None, n_slice: Optional[int] = None) Select a slice of a cube (to plot later) Parameters Name Type Discription Default data np.ndarray input 3D data, [C in , D, H, W] feature int feature to take the slice of, i.e. the value of C in 1 n_slice int what slice to select, i.e. the value of D 1 Return Type Description np.ndarray data slice: [H, W]","title":"slice_of_cube"},{"location":"api/#filter","text":"","title":"Filter"},{"location":"api/#spectral","text":"sapsan.utils.filter.spectral (im: np.ndarray, fm: int) [2D, 3D] apply a spectral filter Parameters Name Type Discription Default im np.ndarray input dataset (ex: [C in , D, H, W]) fm int number of Fourier modes to filter down to Return Type Description np.ndarray filtered dataset","title":"spectral"},{"location":"api/#box","text":"sapsan.utils.filter.box (im: np.ndarray, ksize) [2D] apply a box filter Parameters Name Type Discription Default im np.ndarray input dataset (ex: [C in , H, W]) ksize tupple kernel size (ex: ksize = (2,2)) Return Type Description np.ndarray filtered dataset","title":"box"},{"location":"api/#gaussian_1","text":"sapsan.utils.filter.gaussian (im: np.ndarray, sigma) [2D, 3D] apply a gaussian filter Note: Guassian filter assumes dx=1 between the points. Adjust sigma accordingly. Parameters Name Type Discription Default im np.ndarray input dataset (ex: [H, W] or [D, H, W]) sigma float or tuple of floats standard deviation for Gaussian kernel. Sigma can be defined for each axis individually. Return Type Description np.ndarray filtered dataset","title":"gaussian"},{"location":"api/#backend-tracking","text":"","title":"Backend (Tracking)"},{"location":"api/#mlflowbackend","text":"CLASS sapsan.lib.backends.mlflow.MLflowBackend (name, host, port) Initilizes mlflow and starts up mlflow ui at a given host:port Parameters Name Type Discription Default name str name under which to record the experiment \"experiment\" host str host of mlflow ui \"localhost\" port int port of mlflow ui 9000 sapsan.lib.backends.mlflow.MLflowBackend.start_ui () starts MLflow ui at a specified host and port sapsan.lib.backends.mlflow.MLflowBackend.start (run_name: str, nested = False, run_id = None) Starts a tracking run Parameters Name Type Discription Default run_name str name of the run \"train\" for Train() , \"evaluate\" for Evaluate() nested bool whether or not to nest the recorded run False for Train() , True for Evaluate() run_id str run id None - a new will be generated Return Type Description str run_id sapsan.lib.backends.mlflow.MLflowBackend.resume (run_id, nested = True) Resumes a previous run, so you can record extra parameters Parameters Name Type Discription Default run_id str id of the run to resume nested bool whether or not to nest the recorded run True, since it will usually be an Evaluate() run sapsan.lib.backends.mlflow.MLflowBackend.log_metric () Logs a metric sapsan.lib.backends.mlflow.MLflowBackend.log_parameter () Logs a parameter sapsan.lib.backends.mlflow.MLflowBackend.log_artifact () Logs an artifact (any saved file such, e.g. .png, .txt) sapsan.lib.backends.mlflow.MLflowBackend.close_active_run () Closes all active MLflow runs sapsan.lib.backends.mlflow.MLflowBackend.end () Ends the most recent MLflow run","title":"MLflowBackend"},{"location":"api/#fakebackend","text":"CLASS sapsan.lib.backends.fake.FakeBackend() Pass to train in order to disable backend (tracking)","title":"FakeBackend"},{"location":"api/#plotting","text":"","title":"Plotting"},{"location":"api/#plot_params","text":"sapsan.utils.plot.plot_params() Contains the matplotlib parameters that format all of the plots ( font.size , axes.labelsize , etc.) Return Type Description dict matplotlib parameters Default Parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def plot_params (): params = { 'font.size' : 14 , 'legend.fontsize' : 14 , 'axes.labelsize' : 20 , 'axes.titlesize' : 24 , 'xtick.labelsize' : 17 , 'ytick.labelsize' : 17 , 'axes.linewidth' : 1 , 'patch.linewidth' : 3 , 'lines.linewidth' : 3 , 'xtick.major.width' : 1.5 , 'ytick.major.width' : 1.5 , 'xtick.minor.width' : 1.25 , 'ytick.minor.width' : 1.25 , 'xtick.major.size' : 7 , 'ytick.major.size' : 7 , 'xtick.minor.size' : 4 , 'ytick.minor.size' : 4 , 'xtick.direction' : 'in' , 'ytick.direction' : 'in' , 'axes.formatter.limits' : [ - 7 , 7 ], 'axes.grid' : True , 'grid.linestyle' : ':' , 'grid.color' : '#999999' , 'text.usetex' : False ,} return params","title":"plot_params"},{"location":"api/#pdf_plot","text":"sapsan.utils.plot.pdf_plot (series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, figsize: tuple, dpi: int, ax: matplotlib.axes, style: str) Plot a probability density function (PDF) of a single or multiple datasets Parameters Name Type Discription Default series List[np.ndarray] input datasets bins int number of bins to use for the dataset to generate the pdf 100 label List[str] list of names to use as labels in the legend None figsize tuple figure size as passed to matplotlib figure (6,6) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure style str accepts matplotlib styles 'tableau-colorblind10' Return Type Description matplotlib.axes object ax","title":"pdf_plot"},{"location":"api/#cdf_plot","text":"sapsan.utils.plot.cdf_plot (series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, figsize: tuple, dpi: int, ax: matplotlib.axes, ks: bool, style: str) Plot a cumulative distribution function (CDF) of a single or multiple datasets Parameters Name Type Discription Default series List[np.ndarray] input datasets bins int number of bins to use for the dataset to generate the pdf 100 label List[str] list of names to use as labels in the legend None figsize tuple figure size as passed to matplotlib figure (6,6) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure ks bool if True prints out on the plot itself the Kolomogorov-Smirnov Statistic . It will also be returned along with the ax object False style str accepts matplotlib styles 'tableau-colorblind10' Return Type Description matplotlib.axes object, float (if ks==True) ax, ks (if ks==True)","title":"cdf_plot"},{"location":"api/#line_plot","text":"sapsan.utils.plot.line_plot (series: List[np.ndarray], bins: int = 100, label: Optional[List[str]] = None, plot_type: str, figsize: tuple, dpi: int, ax: matplotlib.axes, style: str) Plot linear data of x vs y - same matplotlib formatting will be used as the other plots Parameters Name Type Discription Default series List[np.ndarray] input datasets bins int number of bins to use for the dataset to generate the pdf 100 label List[str] list of names to use as labels in the legend None plot_type str axis type of the matplotlib plot; options = ['plot', 'semilogx', 'semilogy', 'loglog'] 'plot' figsize tuple figure size as passed to matplotlib figure (6,6) linestyle List[str] list of linestyles to use for each profile for the matplotlib figure ['-'] (solid line) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) None - creates a separate figure style str accepts matplotlib styles 'tableau-colorblind10' Return Type Description matplotlib.axes object ax","title":"line_plot"},{"location":"api/#slice_plot","text":"sapsan.utils.plot.slice_plot (series: List[np.ndarray], label: Optional[List[str]] = None, cmap = 'plasma', figsize: tuple, dpi: int, ax: matplotlib.axes) Plot 2D spatial distributions (slices) of your target and prediction datasets Parameters Name Type Discription Default series List[np.ndarray] input datasets label List[str] list of names to use as labels in the legend None cmap str matplotlib colormap to use 'plasma' figsize tuple figure size as passed to matplotlib figure (6,6) dpi int resolution of the figure 60 ax matplotlib.axes axes object to use for plotting (if you want to define your own figure and subplots) WARNING: only works if a single image is supplied to slice_plot() , otherwise will be ignored None - creates a separate figure Return Type Description matplotlib.axes object ax","title":"slice_plot"},{"location":"api/#log_plot","text":"sapsan.utils.plot.log_plot (show_log = True, log_path = 'logs/logs/train.csv', valid_log_path = 'logs/logs/valid.csv', delimiter=',', train_name = 'train_loss', valid_name = 'valid_loss', train_column = 1, valid_column = 1, epoch_column = 0) Plots an interactive training log of train_loss vs. epoch with plotly Parameters Name Type Discription Default show_log bool show the loss vs. epoch progress plot (it will be save in mlflow in either case) True log_path str path to training log produced by the catalyst wrapper 'logs/logs/train.csv' valid_log_path str path to validation log produced by the catalyst wrapper 'logs/logs/valid.csv' delimiter str delimiter to use for numpy.genfromtxt data loading ',' train_name str name for the training label 'train_loss' valid_name str name for the validation label 'valid_loss' train_column int column to load for training data from log_path 1 valid_column int column to load for validation data from valid_log_path 1 epoch_column int column to load the epoch index from log_path . If None , then epoch will be generated fro the number of entries 0 Return Type Description plotly.express object plot figure","title":"log_plot"},{"location":"api/#model_graph","text":"sapsan.utils.plot.model_graph (model, shape: np.array, transforms) Creates a graph of the ML model (needs graphviz to be installed). A tutorial is available on the wiki: Model Graph The method is based on hiddenlayer originally written by Waleed Abdulla. Parameters Name Type Discription Default model object initialized pytorch or tensorflow model shape np.array shape of the input array in the form [N, C in , D b , H b , W b ], where C in =1 transforms list[methods] a list of hiddenlayer transforms to be applied ( Fold, FoldId, Prune, PruneBranch, FoldDuplicates, Rename ) See below Default Parameters 1 2 3 4 5 6 7 8 9 10 11 12 import sapsan.utils.hiddenlayer as hl transforms = [ hl . transforms . Fold ( \"Conv > MaxPool > Relu\" , \"ConvPoolRelu\" ), hl . transforms . Fold ( \"Conv > MaxPool\" , \"ConvPool\" ), hl . transforms . Prune ( \"Shape\" ), hl . transforms . Prune ( \"Constant\" ), hl . transforms . Prune ( \"Gather\" ), hl . transforms . Prune ( \"Unsqueeze\" ), hl . transforms . Prune ( \"Concat\" ), hl . transforms . Rename ( \"Cast\" , to = \"Input\" ), hl . transforms . FoldDuplicates () ] Return Type Description graphviz.Digraph object graph of a model","title":"model_graph"},{"location":"api/#physics","text":"","title":"Physics"},{"location":"api/#reynoldsstress","text":"sapsan.utils.physics.ReynoldsStress (u, filt, filt_size, only_x_components=False) Calculates a stress tensor of the form \\[ \\tau_{ij} = \\widetilde{u_i u_j} - \\tilde{u}_i\\tilde{u}_j \\] where \\(\\tilde{u}\\) is the filtered \\(u\\) Parameters Name Type Discription Default u np.ndarray input velocity in 3D - [axis, D, H, W] filt sapsan.utils.filters the type of filter to use (spectral, box, gaussian). Pass the filter itself by loading the appropriate one from sapsan.utils.filters gaussian filt_size int or float size of the filter to apply. For different filter types, the size is defined differently. Spectral - fourier mode to filter to, Box - k_size (box size), Gaussian - sigma 2 (sigma=2 for gaussian) only_x_component bool calculates and outputs only x components of the tensor in shape [row, D, H, W] - calculating all 9 can be taxing on memory False Return Type Description np.ndarray stress tensor of shape [column, row, D, H, W]","title":"ReynoldsStress"},{"location":"api/#powerspectrum","text":"CLASS sapsan.utils.physics.PowerSpectrum (u: np.ndarray) Sets up to produce a power spectrum Parameters Name Type Discription Default u np.ndarray input velocity in 3D - [axis, D, H, W] sapsan.utils.physics.PowerSpectrum.calculate() Calculates the power spectrum Return Type Description np.ndarray, np.ndarray k_bins (fourier modes), Ek_bins (E(k)) sapsan.utils.physics.PowerSpectrum.spectrum_plot (k_bins, Ek_bins, kolmogorov=True, kl_a) Plots the calculated power spectrum Parameters Name Type Discription Default k_bins np.ndarray fourier mode values along x-axis Ek_bins np.ndarray energy as a function of k: E(k) kolmogorov bool plots scaled Kolmogorov's -5/3 spectrum alongside the calculated one True kl_A float scaling factor of Kolmogorov's law np.amax(self.Ek_bins)*1e1 Return Type Description matplotlib.axes object spectrum plot","title":"PowerSpectrum"},{"location":"api/#gradientmodel","text":"CLASS sapsan.utils.physics.GradientModel (u: np.ndarray, filter_width, delta_u = 1) sets up to apply a gradient turbulence subgrid model: \\[ \\tau_{ij} = \\frac{1}{12} \\Delta^2 \\,\\delta_k u^*_i \\,\\delta_k u^*_j \\] where \\(\\Delta\\) is the filter width and \\(u^*\\) is the filtered \\(u\\) Parameters Name Type Discription Default u np.ndarray input filtered quantity in 3D - [axis, D, H, W] filter_width float width of the filter which was applied onto u delta_u distance between the points on the grid to use for scaling 1 sapsan.utils.physics.GradientModel.gradient() calculated the gradient of the given input data from GradientModel Return Type Description np.ndarray gradient with shape [column, row, D, H, W] sapsan.utils.physics.GradientModel.model() calculates the gradient model tensor with shape [column, row, D, H, W] Return Type Description np.ndarray gradient model tensor","title":"GradientModel"},{"location":"api/#dynamicsmagorinskymodel","text":"CLASS sapsan.utils.physics.DynamicSmagorinskyModel (u: np.ndarray, filt, original_filt_size, filt_ratio, du, delta_u) sets up to apply a Dynamic Smagorinsky (DS) turbulence subgrid model: \\[ \\tau_{ij} = -2(C_s\\Delta^*)^2|S^*|S^*_{ij} \\] where \\(\\Delta\\) is the filter width and \\(S^*\\) is the filtered \\(u\\) Parameters Name Type Discription Default u np.ndarray input filtered quantity either in 3D [axis, D, H, W] or 2D [axis, D, H] du np.ndarray gradient of u None*: if du is not provided, then it will be calculated with np.gradient() filt sapsan.utils.filters the type of filter to use (spectral, box, gaussian). Pass the filter itself by loading the appropriate one from sapsan.utils.filters spectral original_fil_size int width of the filter which was applied onto u 15 (spectral, fourier modes = 15) delta_u float distance between the points on the grid to use for scaling 1 filt_ratio float the ratio of additional filter that will be applied on the data to find the slope for Dynamic Smagorinsky extrapolation over original_filt_size 0.5 sapsan.utils.physics.DynamicSmagorinskyModel.model() calculates the DS model tensor with shape [column, row, D, H, W] Return Type Description np.ndarray DS model tensor","title":"DynamicSmagorinskyModel"},{"location":"cheat_sheet/","tags":["HTML5","JavaScript","CSS"],"text":"Cheat-Sheet mike set-default --push latest mike deploy --push --update-aliases 0.1 latest publishes a new wiki version Hover me GLSR Admonition smth here admonition without a title Collapsible Oh what! You clicked?! Collapsible Already expanded Info inline or inline end to inline admonition with text Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem footnote 1 dolor sit amet, consectetur adipiscing another one 2 Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet Mauris dictum mi lacus Ut sit amet placerat ante Checked box unchecked box In hac habitasse platea dictumst Method Description GET Fetch resource PUT Update resource DELETE Delete resource \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] Also works as in latex inline \\(19 M_{\\odot}\\) 1 2 3 theme : features : - content.code.annotate # (1)! I'm a code annotation! I can contain code , formatted text , images, ... basically anything that can be written in Markdown. bubble_sort.py 1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] The range () function is used to generate a sequence of numbers. This is my text number 1 This is my text number 2 This is my text number 3 This is my text number 4 This is my text number 5 This is my text number 6 some blue text . some CLASS text . var hcolor = 'hi' document.getElementById('output').innerHTML = hcolor; Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Cheat-Sheet"},{"location":"cheat_sheet/#cheat-sheet","text":"mike set-default --push latest mike deploy --push --update-aliases 0.1 latest publishes a new wiki version Hover me GLSR Admonition smth here admonition without a title Collapsible Oh what! You clicked?! Collapsible Already expanded Info inline or inline end to inline admonition with text Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem footnote 1 dolor sit amet, consectetur adipiscing another one 2 Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet Mauris dictum mi lacus Ut sit amet placerat ante Checked box unchecked box In hac habitasse platea dictumst Method Description GET Fetch resource PUT Update resource DELETE Delete resource \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] Also works as in latex inline \\(19 M_{\\odot}\\) 1 2 3 theme : features : - content.code.annotate # (1)! I'm a code annotation! I can contain code , formatted text , images, ... basically anything that can be written in Markdown. bubble_sort.py 1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] The range () function is used to generate a sequence of numbers. This is my text number 1 This is my text number 2 This is my text number 3 This is my text number 4 This is my text number 5 This is my text number 6 some blue text . some CLASS text . var hcolor = 'hi' document.getElementById('output').innerHTML = hcolor; Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Cheat-Sheet"},{"boost":15,"location":"details/estimators/","text":"Estimators Sapsan has several models in its arsenal to get started. Convolution Neural Network ( CNN ) Example: cnn_example.ipynb Estimator: cnn3d_estimator.py The network is based around Conv3d and MaxPool3d layers, reducing the spatial dimensions down to 1 by increasing the number of features . In order to do that, the network iterates over the following NN block: 1 2 3 def nn_block (): torch . nn . Conv3d ( D_in , D_in * 2 , kernel_size = 2 , stride = 2 , padding = 1 ) torch . nn . MaxPool3d ( kernel_size = 2 , padding = 1 ) where D_in is the input dimension. As final layers, ReLU activation function is used and the data is linearized . An example model graph for the input data with the spatial dimensions of [16, 16, 16] split into 8 batches is provided below. Physics-Informed CNN for Turbulence Modeling ( PIMLTurb ) Accepted 2022, Astrophysical Journal (ApJ) Example: pimlturb_diagonal_example.ipynb Estimator: pimlturb_diagonal_estimator.py The estimator is based on Physics-Informed Machine Learning for Modeling Turbulence in Supernovae by P.I.Karpov et al. The model is based on a 3D convolutional network with some additions to enforce a realizability constraint ( \\(Re_{ii} > 0\\) , where \\(Re\\) is the Reynolds stress tensor and \\(i\\) is the component index). Its overall schematic and graph are shown below. The method also utilizes a custom loss that combines statistical (Kolmogorov-Smirnov Statistic) and spatial (Smooth L1) losses. The full description can be found in the paper linked above. For the example included in Sapsan, the data included is from the same dataset as the publication, but it has been heavily sampled (down to \\(17^3\\) ). To achieve comparable published results, the model will need to be trained for 3000-4000 epochs. Physics-Informed Convolutional Autoencoder ( PICAE ) Example: picae_example.ipynb Estimator: picae_estimator.py Note: The estimator is based on Embedding Hard Physical Constraints in Neural Network Coarse-Graining of 3D Turbulence by M.T.Arvind et al. The model consists of 2 main parts: 1. Convolutional Auto-Encoder (trainable) 2. Static layers enforcing divergence-free condition (constant) Thus, the latter force the CAE portion of the model to adjust to the curl of \\(A\\) to be 0. Through this, we are effectively enforcing the conservation of mass. A schematic of the model is shown below. Kernel Ridge Regression ( KRR ) Example: krr_example.ipynb Estimator: krr_estimator.py We have included one of the classic regression-based methods used in machine learning - Kernel Ridge Regression . The model has two hyperparameters to be tuned: regularization term \\(\\alpha\\) and full-width at half-max \\(\\sigma\\) . KRR has the following form: \\[ y^\u2032\u2004=\u2004y(K\u2005+\u2005\\alpha I)^{\u2212\u20051}k \\] where \\(K\\) is the kernel, chosen to be the radial basis function (gaussian): \\[ K(x,\u2006x^\u2032)\u2004=\u2004exp\\left(\u2005-\\frac{||x\u2005\u2212\u2005x^\u2032||^2}{2\\sigma^2}\\right) \\]","title":"Estimators"},{"location":"details/estimators/#estimators","text":"Sapsan has several models in its arsenal to get started.","title":"Estimators"},{"location":"details/estimators/#convolution-neural-network-cnn","text":"Example: cnn_example.ipynb Estimator: cnn3d_estimator.py The network is based around Conv3d and MaxPool3d layers, reducing the spatial dimensions down to 1 by increasing the number of features . In order to do that, the network iterates over the following NN block: 1 2 3 def nn_block (): torch . nn . Conv3d ( D_in , D_in * 2 , kernel_size = 2 , stride = 2 , padding = 1 ) torch . nn . MaxPool3d ( kernel_size = 2 , padding = 1 ) where D_in is the input dimension. As final layers, ReLU activation function is used and the data is linearized . An example model graph for the input data with the spatial dimensions of [16, 16, 16] split into 8 batches is provided below.","title":"Convolution Neural Network (CNN)"},{"location":"details/estimators/#physics-informed-cnn-for-turbulence-modeling-pimlturb","text":"Accepted 2022, Astrophysical Journal (ApJ) Example: pimlturb_diagonal_example.ipynb Estimator: pimlturb_diagonal_estimator.py The estimator is based on Physics-Informed Machine Learning for Modeling Turbulence in Supernovae by P.I.Karpov et al. The model is based on a 3D convolutional network with some additions to enforce a realizability constraint ( \\(Re_{ii} > 0\\) , where \\(Re\\) is the Reynolds stress tensor and \\(i\\) is the component index). Its overall schematic and graph are shown below. The method also utilizes a custom loss that combines statistical (Kolmogorov-Smirnov Statistic) and spatial (Smooth L1) losses. The full description can be found in the paper linked above. For the example included in Sapsan, the data included is from the same dataset as the publication, but it has been heavily sampled (down to \\(17^3\\) ). To achieve comparable published results, the model will need to be trained for 3000-4000 epochs.","title":"Physics-Informed CNN for Turbulence Modeling (PIMLTurb)"},{"location":"details/estimators/#physics-informed-convolutional-autoencoder-picae","text":"Example: picae_example.ipynb Estimator: picae_estimator.py Note: The estimator is based on Embedding Hard Physical Constraints in Neural Network Coarse-Graining of 3D Turbulence by M.T.Arvind et al. The model consists of 2 main parts: 1. Convolutional Auto-Encoder (trainable) 2. Static layers enforcing divergence-free condition (constant) Thus, the latter force the CAE portion of the model to adjust to the curl of \\(A\\) to be 0. Through this, we are effectively enforcing the conservation of mass. A schematic of the model is shown below.","title":"Physics-Informed Convolutional Autoencoder (PICAE)"},{"location":"details/estimators/#kernel-ridge-regression-krr","text":"Example: krr_example.ipynb Estimator: krr_estimator.py We have included one of the classic regression-based methods used in machine learning - Kernel Ridge Regression . The model has two hyperparameters to be tuned: regularization term \\(\\alpha\\) and full-width at half-max \\(\\sigma\\) . KRR has the following form: \\[ y^\u2032\u2004=\u2004y(K\u2005+\u2005\\alpha I)^{\u2212\u20051}k \\] where \\(K\\) is the kernel, chosen to be the radial basis function (gaussian): \\[ K(x,\u2006x^\u2032)\u2004=\u2004exp\\left(\u2005-\\frac{||x\u2005\u2212\u2005x^\u2032||^2}{2\\sigma^2}\\right) \\]","title":"Kernel Ridge Regression (KRR)"},{"location":"details/structure/","text":"Structure Dependencies Sapsan is a python-based framework. Dependencies can be associated with logical modules of the project. The core module does not have any particular dependencies as all classes are implemented using native Python. Lib modules relying heavily on PyTorch with a Catalyst wrapper, as well as scikit-learn for regression-based ML models. CLI module depends on the click library for implementing command line interfaces. Sapsan is integrated with MLflow to provide for easy and automatic tracking during the experiment stage, as well as saving the trained model. This gives direct access to run history and performance, which in turn gives the user ability to analyze and further tweak their model. Structure & flexibility To provide flexibility and scalability of the project a number of abstraction classes were introduced. Core abstractions include: Core Abstraction Description Experiment main abstraction which encapsulates execution of algorithms, experiments, tests, etc. Algorithm a base class which all models are extended from BaseAlgorithm base class for all algorithms that do not need to be trained and has only run method Estimator an algorithm that has train and predict methods, like regression model or classifier Dataset high level wrapped over dataset loaders Next Sapsan has utility abstractions responsible for all-things tracking: Utility Abstractions Description Metric a single instance of metric emitted during the experiment run Parameter a parameter used in the experiment Artifact artifacts for an algorithm (model weights, images, etc.) TrackingBackend adapter for tracking systems to save metrics, parameters, and artifact The project is built around those abstractions to make it easier to reason about. In order to extend the project with new models/algorithms, the user will inherit from Estimator(or BaseAlgorithm) and implement required methods.","title":"Structure"},{"location":"details/structure/#structure","text":"","title":"Structure"},{"location":"details/structure/#dependencies","text":"Sapsan is a python-based framework. Dependencies can be associated with logical modules of the project. The core module does not have any particular dependencies as all classes are implemented using native Python. Lib modules relying heavily on PyTorch with a Catalyst wrapper, as well as scikit-learn for regression-based ML models. CLI module depends on the click library for implementing command line interfaces. Sapsan is integrated with MLflow to provide for easy and automatic tracking during the experiment stage, as well as saving the trained model. This gives direct access to run history and performance, which in turn gives the user ability to analyze and further tweak their model.","title":"Dependencies"},{"location":"details/structure/#structure-flexibility","text":"To provide flexibility and scalability of the project a number of abstraction classes were introduced. Core abstractions include: Core Abstraction Description Experiment main abstraction which encapsulates execution of algorithms, experiments, tests, etc. Algorithm a base class which all models are extended from BaseAlgorithm base class for all algorithms that do not need to be trained and has only run method Estimator an algorithm that has train and predict methods, like regression model or classifier Dataset high level wrapped over dataset loaders Next Sapsan has utility abstractions responsible for all-things tracking: Utility Abstractions Description Metric a single instance of metric emitted during the experiment run Parameter a parameter used in the experiment Artifact artifacts for an algorithm (model weights, images, etc.) TrackingBackend adapter for tracking systems to save metrics, parameters, and artifact The project is built around those abstractions to make it easier to reason about. In order to extend the project with new models/algorithms, the user will inherit from Estimator(or BaseAlgorithm) and implement required methods.","title":"Structure &amp; flexibility"},{"location":"other/community/","text":"Community Guidelines Sapsan welcomes contributions from the community, looking to improve the pipeline and to grow its library of models. Let's make ML models more accessible together! General Suggestions Please feel free to post any bugs you run into or make suggestions through the Issues . I will do my best to address them as soon as possible. If you would like to contribute directly, then a Pull Request would be the most straightforward way to do so. Once approved, you will be added as a contributor to Sapsan on GitHub. Adding a Model You would like to contribute to Sapsan's 'model zoo'? That's great! Here are the steps to do so 1. Create a new folder under sapsan/lib/estimator with the name to reflect your model ( custom_model for now). 2. Place your python script with the model into that folder, adhering to the format outlined in the template (see Custom Estimator for details) * make sure you initialize the model with sapsan/lib/estimator/custom_model/__init__.py * add to sapsan/lib/estimator/__init__.py a line to access your model, such as 1 from .custom_model.custom_model import Custom_Model , Custom_ModelConfig 3. Set up a Jupyter notebook example and include it under sapsan/examples . Make sure the example data is either randomly generated, provided in a small batch, or can be auto-downloaded. 4. Write a short description of your model for the Estimators' page on the Wiki. It is a good idea to provide a graph to show the structure of your model ( graph example ), along with the links to any publications of the model if such exist. 5. Pull Request it! Once approved, your model will be included in automatic testing on push for all future Sapsan releases. Analytical Tools We use a huge variety of tools to analyze our results depending on the problem at hand. Sapsan certainly won't be able to cover everything, but it tries to cover the most general ones (e.g. power spectrum). If there is something major missing, please write about it in the Issues or create a Pull Request . For the latter, the tools should be added into sapsan/utils . You can further add to either 1. plot.py as a separate function if it is a visual analysis (e.g. plotting probability density function) 2. physics.py as a separate Class for any type of physics-based calculations 3. Anything custom is fine too Analytical Turbulence Models I am looking to expand a library of analytical turbulence models (i.e. gradient model) to compare ones results with. There are lots of flavors of such, hence a pull request would be highly appreciated. Analytical models should be added as a separate Class in sapsan/utils/physics.py . In addition, please prepare a short description of it for the Wiki.","title":"Community Guidelines"},{"location":"other/community/#community-guidelines","text":"Sapsan welcomes contributions from the community, looking to improve the pipeline and to grow its library of models. Let's make ML models more accessible together!","title":"Community Guidelines"},{"location":"other/community/#general-suggestions","text":"Please feel free to post any bugs you run into or make suggestions through the Issues . I will do my best to address them as soon as possible. If you would like to contribute directly, then a Pull Request would be the most straightforward way to do so. Once approved, you will be added as a contributor to Sapsan on GitHub.","title":"General Suggestions"},{"location":"other/community/#adding-a-model","text":"You would like to contribute to Sapsan's 'model zoo'? That's great! Here are the steps to do so 1. Create a new folder under sapsan/lib/estimator with the name to reflect your model ( custom_model for now). 2. Place your python script with the model into that folder, adhering to the format outlined in the template (see Custom Estimator for details) * make sure you initialize the model with sapsan/lib/estimator/custom_model/__init__.py * add to sapsan/lib/estimator/__init__.py a line to access your model, such as 1 from .custom_model.custom_model import Custom_Model , Custom_ModelConfig 3. Set up a Jupyter notebook example and include it under sapsan/examples . Make sure the example data is either randomly generated, provided in a small batch, or can be auto-downloaded. 4. Write a short description of your model for the Estimators' page on the Wiki. It is a good idea to provide a graph to show the structure of your model ( graph example ), along with the links to any publications of the model if such exist. 5. Pull Request it! Once approved, your model will be included in automatic testing on push for all future Sapsan releases.","title":"Adding a Model"},{"location":"other/community/#analytical-tools","text":"We use a huge variety of tools to analyze our results depending on the problem at hand. Sapsan certainly won't be able to cover everything, but it tries to cover the most general ones (e.g. power spectrum). If there is something major missing, please write about it in the Issues or create a Pull Request . For the latter, the tools should be added into sapsan/utils . You can further add to either 1. plot.py as a separate function if it is a visual analysis (e.g. plotting probability density function) 2. physics.py as a separate Class for any type of physics-based calculations 3. Anything custom is fine too","title":"Analytical Tools"},{"location":"other/community/#analytical-turbulence-models","text":"I am looking to expand a library of analytical turbulence models (i.e. gradient model) to compare ones results with. There are lots of flavors of such, hence a pull request would be highly appreciated. Analytical models should be added as a separate Class in sapsan/utils/physics.py . In addition, please prepare a short description of it for the Wiki.","title":"Analytical Turbulence Models"},{"boost":10,"location":"overview/getting_started/","text":"Getting Started Command Line Interface ( CLI ) & Jupyter Notebooks CLI allows users to create new projects leveraging the structure and abstractions of Sapsan providing a unified interface of interaction with the experiments. In addition, you can test your installation and play around with a few included examples. Testing To make sure everything is working correctly and Sapsan was installed without issues, run: 1 sapsan test Running Examples To get started and familiarize yourself with the Jupyter Notebook interface, feel free to run the included examples ( CNN , PICAE , or PIMLTurb on 3D data, and KRR on 2D data). To copy the examples, type: 1 sapsan get_examples This will create a folder ./sapsan_examples with appropriate example jupyter notebooks. Custom Projects In order to get started on your own project, proceed as follows: 1 sapsan create --name {name} where {name} should be replaced with your custom project name. This will result in creation of the following file structure: 1 2 3 4 5 6 Project Folder: {name}/ Data Folder: {name}/data/ Estimator Template: {name}/{name}_estimator.py Jupyter Notebook Template: {name}/{name}.ipynb Docker Template: {name}/Dockerfile Docker Makefile: {name}/Makefile This structure allows you to focus on the designing your network structure itself in {name}_estimator.py . At the same time, you can quickly jump into Jupyter Notebook and start running your custom setup. Lastly, Dockerfile is already pre-filled to easily share your work with your collaborators or as part of a publication. Graphical User Interface (GUI) - beta In the aim to provide a user-friendly experience, best suited for demonstrations of your models at talks and conferences, while attempting to not sacrifice too much on customization we have designed a GUI for Sapsan. By utilizing Streamlit , a python library to build web applications, Sapsan can be fully interacted in the browser running locally. A user can tweak the parameters, edit the portion of the code responsible for the ML model, perform visual layer-by-layer analysis, train/validate, analyze the results, and more. Lastly, Sapsan can be tried out in the demo-mode directly on the website - sapsan.app . There, one has limited editing capabilities but can explore the hyper-parameters and get a general understanding of what the framework is capable of. Running GUI In order to run it type in the following and follow the instructions - the interface will be opened in your browser 1 2 sapsan get_examples streamlit run ./sapsan-examples/GUI/st_intro.py Learn more at GUI Examples . Troubleshooting If you encounter the following error when launching streamlit 1 upper limit on inotify watches reached! Then follow the following instruction by Shivani Bhardwaj to increase the watchdog limit (it won't hog your RAM)","title":"Getting Started"},{"location":"overview/getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"overview/getting_started/#command-line-interface-cli-jupyter-notebooks","text":"CLI allows users to create new projects leveraging the structure and abstractions of Sapsan providing a unified interface of interaction with the experiments. In addition, you can test your installation and play around with a few included examples.","title":"Command Line Interface (CLI) &amp; Jupyter Notebooks"},{"location":"overview/getting_started/#testing","text":"To make sure everything is working correctly and Sapsan was installed without issues, run: 1 sapsan test","title":"Testing"},{"location":"overview/getting_started/#running-examples","text":"To get started and familiarize yourself with the Jupyter Notebook interface, feel free to run the included examples ( CNN , PICAE , or PIMLTurb on 3D data, and KRR on 2D data). To copy the examples, type: 1 sapsan get_examples This will create a folder ./sapsan_examples with appropriate example jupyter notebooks.","title":"Running Examples"},{"location":"overview/getting_started/#custom-projects","text":"In order to get started on your own project, proceed as follows: 1 sapsan create --name {name} where {name} should be replaced with your custom project name. This will result in creation of the following file structure: 1 2 3 4 5 6 Project Folder: {name}/ Data Folder: {name}/data/ Estimator Template: {name}/{name}_estimator.py Jupyter Notebook Template: {name}/{name}.ipynb Docker Template: {name}/Dockerfile Docker Makefile: {name}/Makefile This structure allows you to focus on the designing your network structure itself in {name}_estimator.py . At the same time, you can quickly jump into Jupyter Notebook and start running your custom setup. Lastly, Dockerfile is already pre-filled to easily share your work with your collaborators or as part of a publication.","title":"Custom Projects"},{"location":"overview/getting_started/#graphical-user-interface-gui-beta","text":"In the aim to provide a user-friendly experience, best suited for demonstrations of your models at talks and conferences, while attempting to not sacrifice too much on customization we have designed a GUI for Sapsan. By utilizing Streamlit , a python library to build web applications, Sapsan can be fully interacted in the browser running locally. A user can tweak the parameters, edit the portion of the code responsible for the ML model, perform visual layer-by-layer analysis, train/validate, analyze the results, and more. Lastly, Sapsan can be tried out in the demo-mode directly on the website - sapsan.app . There, one has limited editing capabilities but can explore the hyper-parameters and get a general understanding of what the framework is capable of.","title":"Graphical User Interface (GUI) - beta"},{"location":"overview/getting_started/#running-gui","text":"In order to run it type in the following and follow the instructions - the interface will be opened in your browser 1 2 sapsan get_examples streamlit run ./sapsan-examples/GUI/st_intro.py Learn more at GUI Examples .","title":"Running GUI"},{"location":"overview/getting_started/#troubleshooting","text":"If you encounter the following error when launching streamlit 1 upper limit on inotify watches reached! Then follow the following instruction by Shivani Bhardwaj to increase the watchdog limit (it won't hog your RAM)","title":"Troubleshooting"},{"boost":5,"location":"overview/installation/","text":"Installation Install Sapsan 1. Install PyTorch (prerequisite) Sapsan can be run on both cpu and gpu. Below are the requirements for each version Device CPU torch>=1.9.0 torchvision>=0.10.0 GPU torch>=1.9.0+cu111 torchvision>=0.10.0+cu111 Please follow the instructions on PyTorch to install either version. CUDA>=11.1 can be installed directly with PyTorch as well. 2a. Install via pip (recommended) 1 pip install sapsan 2b. Clone from github (alternative) 1 2 3 git clone https://github.com/pikarpov-LANL/Sapsan.git cd Sapsan/ python setup.py install If you experience any issues, you can try installing packages individually with: 1 pip install -r requirements.txt Note: make sure you are using the latest release version Install Graphviz (optional) In order to create model graphs, Sapsan is using graphviz . If you would like to utilize this functionality, then please install graphviz via: 1 conda install graphviz or 1 sudo apt-get install graphviz Install Docker (optional) In order to run Sapsan through Docker or build your own container to share, you will need to install it 1 pip install docker Next, you can build a docker setup with the following: 1 make build-container this will create a container named sapsan-docker . If you want to run the container, type: 1 make run-container a Jupyter notebook will be launched at localhost:7654 Troubleshooting If you get the following error: 1 ImportError: libGL.so.1: cannot open shared object file: No such file or directory your opencv-python package has some dependency issues. To resolve, try the following: 1 2 apt-get update apt-get install ffmpeg libsm6 libxext6 -y","title":"Installation"},{"location":"overview/installation/#installation","text":"","title":"Installation"},{"location":"overview/installation/#install-sapsan","text":"","title":"Install Sapsan"},{"location":"overview/installation/#1-install-pytorch-prerequisite","text":"Sapsan can be run on both cpu and gpu. Below are the requirements for each version Device CPU torch>=1.9.0 torchvision>=0.10.0 GPU torch>=1.9.0+cu111 torchvision>=0.10.0+cu111 Please follow the instructions on PyTorch to install either version. CUDA>=11.1 can be installed directly with PyTorch as well.","title":"1. Install PyTorch (prerequisite)"},{"location":"overview/installation/#2a-install-via-pip-recommended","text":"1 pip install sapsan","title":"2a. Install via pip (recommended)"},{"location":"overview/installation/#2b-clone-from-github-alternative","text":"1 2 3 git clone https://github.com/pikarpov-LANL/Sapsan.git cd Sapsan/ python setup.py install If you experience any issues, you can try installing packages individually with: 1 pip install -r requirements.txt Note: make sure you are using the latest release version","title":"2b. Clone from github (alternative)"},{"location":"overview/installation/#install-graphviz-optional","text":"In order to create model graphs, Sapsan is using graphviz . If you would like to utilize this functionality, then please install graphviz via: 1 conda install graphviz or 1 sudo apt-get install graphviz","title":"Install Graphviz (optional)"},{"location":"overview/installation/#install-docker-optional","text":"In order to run Sapsan through Docker or build your own container to share, you will need to install it 1 pip install docker Next, you can build a docker setup with the following: 1 make build-container this will create a container named sapsan-docker . If you want to run the container, type: 1 make run-container a Jupyter notebook will be launched at localhost:7654","title":"Install Docker (optional)"},{"location":"overview/installation/#troubleshooting","text":"If you get the following error: 1 ImportError: libGL.so.1: cannot open shared object file: No such file or directory your opencv-python package has some dependency issues. To resolve, try the following: 1 2 apt-get update apt-get install ffmpeg libsm6 libxext6 -y","title":"Troubleshooting"},{"location":"overview/examples/local_examples/","text":"Built-in Examples Jupyter Notebook Examples You can run the included examples ( CNN , PIMLTurb , or PICAE on 3D data, and KRR on 2D data). To copy the examples, type: 1 sapsan get_examples This will create a folder ./sapsan_examples with appropriate example jupyter notebooks and GUI. For starters, to launch a CNN example: 1 jupyter notebook ./sapsan_examples/cnn_example.ipynb GUI Examples In order to try out Sapsan's GUI, start a streamlit instance to open in a browser. After the examples have been compied into your working directory as described above, you will be able to find the GUI example. The entry point: 1 streamlit run ./sapsan_examples/GUI/Welcome.py The scripts for the pages you see ( welcome and examples ) are located in the subsequent directory: ./sapsan_examples/GUI/pages/ . If you want to build your own demo, then look into Examples.py to get started. Ideally, you would only need to import your Estimator , EstimatorConfig , EstimatorModel and adjust the run_experiment() function, which has a nearly identical setup to a standard Sapsan's jupyter notebook interface. Sample Data The data for the CNN and KRR examples has been sourced from JHTDB . Specifically the Forced MHD Dataset (1024 3 ) has been used as a starting point. Data Description u_dim128_2d.h5 velocity field sampled down to [128,128,128] and using the 1 st slice u_dim32_fm15.h5 velocity field sampled down to [32,32,32], and spectrally filtered down to 15 modes","title":"Built-in Examples"},{"location":"overview/examples/local_examples/#built-in-examples","text":"","title":"Built-in Examples"},{"location":"overview/examples/local_examples/#jupyter-notebook-examples","text":"You can run the included examples ( CNN , PIMLTurb , or PICAE on 3D data, and KRR on 2D data). To copy the examples, type: 1 sapsan get_examples This will create a folder ./sapsan_examples with appropriate example jupyter notebooks and GUI. For starters, to launch a CNN example: 1 jupyter notebook ./sapsan_examples/cnn_example.ipynb","title":"Jupyter Notebook Examples"},{"location":"overview/examples/local_examples/#gui-examples","text":"In order to try out Sapsan's GUI, start a streamlit instance to open in a browser. After the examples have been compied into your working directory as described above, you will be able to find the GUI example. The entry point: 1 streamlit run ./sapsan_examples/GUI/Welcome.py The scripts for the pages you see ( welcome and examples ) are located in the subsequent directory: ./sapsan_examples/GUI/pages/ . If you want to build your own demo, then look into Examples.py to get started. Ideally, you would only need to import your Estimator , EstimatorConfig , EstimatorModel and adjust the run_experiment() function, which has a nearly identical setup to a standard Sapsan's jupyter notebook interface.","title":"GUI Examples"},{"location":"overview/examples/local_examples/#sample-data","text":"The data for the CNN and KRR examples has been sourced from JHTDB . Specifically the Forced MHD Dataset (1024 3 ) has been used as a starting point. Data Description u_dim128_2d.h5 velocity field sampled down to [128,128,128] and using the 1 st slice u_dim32_fm15.h5 velocity field sampled down to [32,32,32], and spectrally filtered down to 15 modes","title":"Sample Data"},{"location":"overview/examples/web_examples/","text":"Online Examples Jupyter Notebook on Google-Colab You can play around with the CNN example on the google-colab. Besides the initial setup in the first cell, the rest of the notebook is identical to the one on github. Proceed via the link cnn_example.ipynb on google-colab . GUI Web Demo Sapsan's GUI is powered by streamlit . A short demo with the CNN example has been setup on google cloud. Try it out on sapsan.app","title":"Online Examples"},{"location":"overview/examples/web_examples/#online-examples","text":"","title":"Online Examples"},{"location":"overview/examples/web_examples/#jupyter-notebook-on-google-colab","text":"You can play around with the CNN example on the google-colab. Besides the initial setup in the first cell, the rest of the notebook is identical to the one on github. Proceed via the link cnn_example.ipynb on google-colab .","title":"Jupyter Notebook on Google-Colab"},{"location":"overview/examples/web_examples/#gui-web-demo","text":"Sapsan's GUI is powered by streamlit . A short demo with the CNN example has been setup on google cloud. Try it out on sapsan.app","title":"GUI Web Demo"},{"boost":5,"location":"tutorials/custom_docker/","text":"Custom Docker The best way to release your code along with the publication is through Docker . This will ensure the reproducibility of your paper's results, making your methods easily accessible to the readers and general public. While you can write and handle Docker containers in any fashion you want, Sapsan includes a ready-to-go template to make this process easier. Here are the steps in the Docker template: Setup a virtual environment Install requirements (Sapsan) Launch a Jupyter notebook to reproduce the results That's it! Now there won't be any struggle or emails to you, the author, about the setup and configuration of your methods! In order to make this work, we will need to set up a Dockerfile, build a container, and run it. The latter steps are combined into a Makefile. When it comes to publishing your Docker, share the Docker setup files for the container to be built on-site. In this article, we will first discuss the Docker setup and then the release options. Note Make sure Docker is installed on your machine ( Installation ) Docker Setup Dockerfile The template below is will be created when starting a project via sapsan create -n {name} , where {name} is your custom project name. Feel free to edit it to your liking, such as adding further packages to install outside of Sapsan, name of working directories and etc. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FROM python:3.8.5-slim # remember to expose the port your app will run on EXPOSE 7654 ENV GIT_PYTHON_REFRESH = quiet RUN pip install -U pip RUN pip install sapsan =={ version } # copy the notebook and data into a directory of its own (so it isn't in the top-level dir) COPY { name } _estimator.py { name } _docker/ COPY { name } .ipynb { name } _docker/ COPY ./data/ { name } _docker/data/ WORKDIR / { name } _docker # run it! ENTRYPOINT [ \"jupyter\" , \"notebook\" , \"{name}.ipynb\" , \"--port=7654\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--NotebookApp.token=''\" , \"--NotebookApp.password=''\" , \"--no-browser\" ] Here is a working Dockerfile to dockerize the Sapsan's included CNN example. Makefile to build and run the container The Makefile is also created upon initializing a project. It makes it straightforward to build and run your Docker container, launching a Jupyter Notebook as a result. 1 2 3 4 5 6 7 8 # to build and start the container build-container: @docker build . -t { name } -docker # to run existing the container created above # (jupyter notebook will be started at --port==7654) run-container: @docker run -p 7654 :7654 { name } -docker:latest Thus, the user will need to type the following to build and run the Docker container: 1 2 make build-container make run-container Here is a working Makefile for Sapsan's included CNN example. Release Your Docker Provide the Setup Files In order for someone to reproduce your results, you will need to provide: Dockerfile Makefile Jupyter Notebook Training Data The virtual environment will be built from the ground up on the user's local machine. Besides the training data, the other files won't weigh anything. The only pre-requisite is to have the Docker installed, which can be done through pip .","title":"Custom Docker"},{"location":"tutorials/custom_docker/#custom-docker","text":"The best way to release your code along with the publication is through Docker . This will ensure the reproducibility of your paper's results, making your methods easily accessible to the readers and general public. While you can write and handle Docker containers in any fashion you want, Sapsan includes a ready-to-go template to make this process easier. Here are the steps in the Docker template: Setup a virtual environment Install requirements (Sapsan) Launch a Jupyter notebook to reproduce the results That's it! Now there won't be any struggle or emails to you, the author, about the setup and configuration of your methods! In order to make this work, we will need to set up a Dockerfile, build a container, and run it. The latter steps are combined into a Makefile. When it comes to publishing your Docker, share the Docker setup files for the container to be built on-site. In this article, we will first discuss the Docker setup and then the release options. Note Make sure Docker is installed on your machine ( Installation )","title":"Custom Docker"},{"location":"tutorials/custom_docker/#docker-setup","text":"","title":"Docker Setup"},{"location":"tutorials/custom_docker/#dockerfile","text":"The template below is will be created when starting a project via sapsan create -n {name} , where {name} is your custom project name. Feel free to edit it to your liking, such as adding further packages to install outside of Sapsan, name of working directories and etc. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FROM python:3.8.5-slim # remember to expose the port your app will run on EXPOSE 7654 ENV GIT_PYTHON_REFRESH = quiet RUN pip install -U pip RUN pip install sapsan =={ version } # copy the notebook and data into a directory of its own (so it isn't in the top-level dir) COPY { name } _estimator.py { name } _docker/ COPY { name } .ipynb { name } _docker/ COPY ./data/ { name } _docker/data/ WORKDIR / { name } _docker # run it! ENTRYPOINT [ \"jupyter\" , \"notebook\" , \"{name}.ipynb\" , \"--port=7654\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--NotebookApp.token=''\" , \"--NotebookApp.password=''\" , \"--no-browser\" ] Here is a working Dockerfile to dockerize the Sapsan's included CNN example.","title":"Dockerfile"},{"location":"tutorials/custom_docker/#makefile-to-build-and-run-the-container","text":"The Makefile is also created upon initializing a project. It makes it straightforward to build and run your Docker container, launching a Jupyter Notebook as a result. 1 2 3 4 5 6 7 8 # to build and start the container build-container: @docker build . -t { name } -docker # to run existing the container created above # (jupyter notebook will be started at --port==7654) run-container: @docker run -p 7654 :7654 { name } -docker:latest Thus, the user will need to type the following to build and run the Docker container: 1 2 make build-container make run-container Here is a working Makefile for Sapsan's included CNN example.","title":"Makefile to build and run the container"},{"location":"tutorials/custom_docker/#release-your-docker","text":"","title":"Release Your Docker"},{"location":"tutorials/custom_docker/#provide-the-setup-files","text":"In order for someone to reproduce your results, you will need to provide: Dockerfile Makefile Jupyter Notebook Training Data The virtual environment will be built from the ground up on the user's local machine. Besides the training data, the other files won't weigh anything. The only pre-requisite is to have the Docker installed, which can be done through pip .","title":"Provide the Setup Files"},{"boost":5,"location":"tutorials/custom_estimator/","text":"Custom Estimator Sapsan makes it easy to get started on designing your own ML model layer-by-layer. Command-line Interface ( CLI ) Here is the easiest way to get started, where you should replace {name} with your custom project name. 1 sapsan create -n { name } This will create the full structure for your project, but in a template form. You will primarily focus on the designing your ML model (estimator). You will find the template for it in 1 { name } / { name } _estimator.py The template is structured to utilize a custom backend sapsan.lib.estimator.torch_backend.py , hence it revolves around using PyTorch. In the template, you will define the layers your want to use, the order in which they should be executed, and a few custom model parameters (Optimizer, Loss Function, Scheduler). Since we are talking about PyTorch, refer to its API to define your layers . Estimator Template {name}Model define your ML layers forward function (layer order) {name}Config set parameters (e.g. number of epochs) - usually set through a high-level interface (e.g. a jupyter notebook ) add custom parameters to be tracked by MLflow {name} Set the Optimizer Set the Loss Functions Set the Scheduler Set the Model (based on {name}Model & {name}Config) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \"\"\" Estimator Template Please replace everything between triple quotes to create your custom estimator. \"\"\" import json import numpy as np import torch from sapsan.core.models import EstimatorConfig from sapsan.lib.estimator.torch_backend import TorchBackend from sapsan.lib.data import get_loader_shape class { name_upper } Model ( torch . nn . Module ): # input channels, output channels can be the input to define the layers def __init__ ( self ): super ({ name_upper } Model , self ) . __init__ () # define your layers \"\"\" self.layer_1 = torch.nn.Linear(4, 8) self.layer_2 = torch.nn.Linear(8, 16) \"\"\" def forward ( self , x ): # set the layer order here \"\"\" l1 = self.layer_1(x) output = self.layer_2(l1) \"\"\" return output class { name_upper } Config ( EstimatorConfig ): # set defaults to your liking, add more parameters def __init__ ( self , n_epochs : int = 1 , batch_dim : int = 64 , patience : int = 10 , min_delta : float = 1e-5 , logdir : str = \"./logs/\" , lr : float = 1e-3 , min_lr = None , * args , ** kwargs ): self . n_epochs = n_epochs self . batch_dim = batch_dim self . logdir = logdir self . patience = patience self . min_delta = min_delta self . lr = lr if min_lr == None : self . min_lr = lr * 1e-2 else : self . min_lr = min_lr self . kwargs = kwargs #everything in self.parameters will get recorded by MLflow #by default, all 'self' variables will get recorded self . parameters = {{ f 'model - {{ k }} ' : v for k , v in self . __dict__ . items () if k != 'kwargs' }} if bool ( self . kwargs ): self . parameters . update ({{ f 'model - {{ k }} ' : v for k , v in self . kwargs . items ()}}) class { name_upper }( TorchBackend ): # Set your optimizer, loss function, and scheduler here def __init__ ( self , loaders , config = { name_upper } Config (), model = { name_upper } Model ()): super () . __init__ ( config , model ) self . config = config self . loaders = loaders #uncomment if you need dataloader shapes for model input #x_shape, y_shape = get_shape(loaders) self . model = { name_upper } Model () self . optimizer = \"\"\" optimizer \"\"\" self . loss_func = \"\"\" loss function \"\"\" self . scheduler = \"\"\" scheduler \"\"\" def train ( self ): trained_model = self . torch_train ( self . loaders , self . model , self . optimizer , self . loss_func , self . scheduler , self . config ) return trained_model Editing Catalyst Runner For majority of applications, you won't need to touch Catalyst Runner settings, which located in torch_backend.py . However, in case you would like to dig further into more unique loss functions, optimizers, data distribution setups, then you can copy the torch_backend.py via --get_torch_backend or shorthand --gtb flag during the creation of the project: 1 sapsan create --gtb -n { name } or just copy it to your current directory by: 1 sapsan gtb For runner types and extensive options please refer to Catalyst Documentation . As for runner adjustments to parallele your training, Sapsan's Wiki includes a page on Parallel GPU Training . Loss Catalyst includes a more extensive list of losses , i.e. criterions , than the standard PyTorch. Their implementations might require to include some extra Callback s to be specified in the runner ( Criterion Documentation ). Please refer to Catalyst examples to create your own loss functions. Optimizer Similar deal is with the optimizer. While using standard (i.e. Adam) can be specified within Estimator Template, for a more complex or custom setup you will need to refer to the runner ( Optimizer Documentation ).","title":"Custom Estimator"},{"location":"tutorials/custom_estimator/#custom-estimator","text":"Sapsan makes it easy to get started on designing your own ML model layer-by-layer.","title":"Custom Estimator"},{"location":"tutorials/custom_estimator/#command-line-interface-cli","text":"Here is the easiest way to get started, where you should replace {name} with your custom project name. 1 sapsan create -n { name } This will create the full structure for your project, but in a template form. You will primarily focus on the designing your ML model (estimator). You will find the template for it in 1 { name } / { name } _estimator.py The template is structured to utilize a custom backend sapsan.lib.estimator.torch_backend.py , hence it revolves around using PyTorch. In the template, you will define the layers your want to use, the order in which they should be executed, and a few custom model parameters (Optimizer, Loss Function, Scheduler). Since we are talking about PyTorch, refer to its API to define your layers .","title":"Command-line Interface (CLI)"},{"location":"tutorials/custom_estimator/#estimator-template","text":"{name}Model define your ML layers forward function (layer order) {name}Config set parameters (e.g. number of epochs) - usually set through a high-level interface (e.g. a jupyter notebook ) add custom parameters to be tracked by MLflow {name} Set the Optimizer Set the Loss Functions Set the Scheduler Set the Model (based on {name}Model & {name}Config) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \"\"\" Estimator Template Please replace everything between triple quotes to create your custom estimator. \"\"\" import json import numpy as np import torch from sapsan.core.models import EstimatorConfig from sapsan.lib.estimator.torch_backend import TorchBackend from sapsan.lib.data import get_loader_shape class { name_upper } Model ( torch . nn . Module ): # input channels, output channels can be the input to define the layers def __init__ ( self ): super ({ name_upper } Model , self ) . __init__ () # define your layers \"\"\" self.layer_1 = torch.nn.Linear(4, 8) self.layer_2 = torch.nn.Linear(8, 16) \"\"\" def forward ( self , x ): # set the layer order here \"\"\" l1 = self.layer_1(x) output = self.layer_2(l1) \"\"\" return output class { name_upper } Config ( EstimatorConfig ): # set defaults to your liking, add more parameters def __init__ ( self , n_epochs : int = 1 , batch_dim : int = 64 , patience : int = 10 , min_delta : float = 1e-5 , logdir : str = \"./logs/\" , lr : float = 1e-3 , min_lr = None , * args , ** kwargs ): self . n_epochs = n_epochs self . batch_dim = batch_dim self . logdir = logdir self . patience = patience self . min_delta = min_delta self . lr = lr if min_lr == None : self . min_lr = lr * 1e-2 else : self . min_lr = min_lr self . kwargs = kwargs #everything in self.parameters will get recorded by MLflow #by default, all 'self' variables will get recorded self . parameters = {{ f 'model - {{ k }} ' : v for k , v in self . __dict__ . items () if k != 'kwargs' }} if bool ( self . kwargs ): self . parameters . update ({{ f 'model - {{ k }} ' : v for k , v in self . kwargs . items ()}}) class { name_upper }( TorchBackend ): # Set your optimizer, loss function, and scheduler here def __init__ ( self , loaders , config = { name_upper } Config (), model = { name_upper } Model ()): super () . __init__ ( config , model ) self . config = config self . loaders = loaders #uncomment if you need dataloader shapes for model input #x_shape, y_shape = get_shape(loaders) self . model = { name_upper } Model () self . optimizer = \"\"\" optimizer \"\"\" self . loss_func = \"\"\" loss function \"\"\" self . scheduler = \"\"\" scheduler \"\"\" def train ( self ): trained_model = self . torch_train ( self . loaders , self . model , self . optimizer , self . loss_func , self . scheduler , self . config ) return trained_model","title":"Estimator Template"},{"location":"tutorials/custom_estimator/#editing-catalyst-runner","text":"For majority of applications, you won't need to touch Catalyst Runner settings, which located in torch_backend.py . However, in case you would like to dig further into more unique loss functions, optimizers, data distribution setups, then you can copy the torch_backend.py via --get_torch_backend or shorthand --gtb flag during the creation of the project: 1 sapsan create --gtb -n { name } or just copy it to your current directory by: 1 sapsan gtb For runner types and extensive options please refer to Catalyst Documentation . As for runner adjustments to parallele your training, Sapsan's Wiki includes a page on Parallel GPU Training .","title":"Editing Catalyst Runner"},{"location":"tutorials/custom_estimator/#loss","text":"Catalyst includes a more extensive list of losses , i.e. criterions , than the standard PyTorch. Their implementations might require to include some extra Callback s to be specified in the runner ( Criterion Documentation ). Please refer to Catalyst examples to create your own loss functions.","title":"Loss"},{"location":"tutorials/custom_estimator/#optimizer","text":"Similar deal is with the optimizer. While using standard (i.e. Adam) can be specified within Estimator Template, for a more complex or custom setup you will need to refer to the runner ( Optimizer Documentation ).","title":"Optimizer"},{"boost":5,"location":"tutorials/mlflow/","text":"MLflow Tracking Default MLflow Tracking in Sapsan Starting MLflow Server mlflow ui server will automatically start locally if a designated port is open. If not, Sapsan assumes the mLflow ui server is already running on that local port and will direct mlflow to write to it. Also you can start mlflow ui manually via: 1 mlflow ui --host localhost --port 9000 Structure By default, Sapsan will keep the following structure in MLflow: Train 1 Evaluate 1 Evaluate 2 Train 2 Evaluate 1 Evaluate 2 where all evaluation runs are nested under the trained run entry. This way all evaluations are grouped together under the model that was just trained. Every Train checks for other active runs, terminates them, and starts a new run. At the end of the Train method, the run does not terminate, awaiting Evaluate runs to be nested under it. Thus, Evaluate runs start and end at the end of the method. However, one can still add extra metrics, artifacts and etc by resuming the previously closed run and writing to it, as discussed in the later section . Tracked Parameters Evaluation runs include the training model parameters and metrics to make it easier to parse through. Here is a complete list of what is tracked by default after running Train or Evaluate loop. Parameter Train Evaluate Everything passed to ModelConfig() (including new parameters passed to kwargs ) model - {parameter} - device, logdir, lr, min_delta, min_lr, n_epochs, patience data - {parameter} - features, features_label, target, target_label, axis, path, path, shuffle chkpnt - {parameter} - initial_size, sample to size, batch_size, batch_num, time, time_granularity Since Train metrics are recorded for Evaluate runs, they are prefixed as train - {metric} . Subsequently, all Evaluate metrics are written as eval - {metric} Metrics Train Evaluate eval - MSE Loss - Mean Squared Error (if the target is provided) eval - KS Stat - Kolmogorov-Smirnov Statistic (if the target is provided) train - final epoch - final training epoch All model metrics model.metrics() (provided by Catalyst and PyTorch) Runtime Artifacts Train Evaluate model_details.txt - model layer init & optimizer settings model_forward.txt - Model.forward() function runtime_log.html - loss vs. epoch training progress pdf_cdf.png - Probability Density Function (PDF) & Cummulative Distribution Function (CDF) plots slices_plot.png - 2D Spatial Distribution (slice snapshots) Adding extra parameters Before Training In order to add a new_parameter to be tracked with MLflow per your run, simply pass it to config as such: ModelConfig(new_parameter=value) . Since it will be initialized under ModelConfig().kwargs['new_parameter'] , the parameter name can be anything. You will see it in MLflow recorded as model - new_parameter . Internally, everything in ModelConfig().parameters gets recorded to MLflow. By default, all ModelConfig() variables, including kwargs are passed to it. Here is the implementation from the CNN3d estimator . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CNN3dConfig ( EstimatorConfig ): def __init__ ( self , n_epochs : int = 1 , patience : int = 10 , min_delta : float = 1e-5 , logdir : str = \"./logs/\" , lr : float = 1e-3 , min_lr = None , * args , ** kwargs ): self . n_epochs = n_epochs self . logdir = logdir self . patience = patience self . min_delta = min_delta self . lr = lr if min_lr == None : self . min_lr = lr * 1e-2 else : self . min_lr = min_lr self . kwargs = kwargs #everything in self.parameters will get recorded by MLflow #by default, all 'self' variables will get recorded self . parameters = { f 'model - { k } ' : v for k , v in self . __dict__ . items () if k != 'kwargs' } if bool ( self . kwargs ): self . parameters . update ({ f 'model - { k } ' : v for k , v in self . kwargs . items ()}) Note: MLflow doesn't like labels that contain / symbol. Please avoid or you might encounter an error. After Training or Evaluation If you want to perform some extra analysis on your model or predictions and record additional metrics after you have called Train.run() or Evaluation.run() , Sapsan has an interface to do so in 3 steps : Resume MLflow run Since MLflow run is closed at the end of Evaluation.run() , it will need to be resumed first before attempting to write to it. For that reason, both Train and Evaluate classes have a parameter run_id which contains the MLflow run_id. You can use it to resume the run, and record new metrics. Record new parameters To add extra parameters to the most recent Train or Evaluate entry in MLflow, simply use either the backend() interface or the standard MLflow interface. End the run In order to keep MLflow tidy, it is advised to call backend.end() after you are done. 1 2 3 4 5 6 7 8 9 eval = Evaluate ( ... ) cubes = eval . run () #do something with the prediction and/or target cube new_metric = np . amax ( cubes [ 'pred_cube' ] / cubes [ 'target_cube' ]) backend . resume ( run_id = eval . run_id ) backend . log_metric ( 'new_metric' , new_metric ) #or use backend.log_parameter() or backend.log_artifact() backend . end () Feel free to review the full API Reference: Backend (Tracking) for the full description of MLflow-related functions built into Sapsan.","title":"MLflow Tracking"},{"location":"tutorials/mlflow/#mlflow-tracking","text":"","title":"MLflow Tracking"},{"location":"tutorials/mlflow/#default-mlflow-tracking-in-sapsan","text":"","title":"Default MLflow Tracking in Sapsan"},{"location":"tutorials/mlflow/#starting-mlflow-server","text":"mlflow ui server will automatically start locally if a designated port is open. If not, Sapsan assumes the mLflow ui server is already running on that local port and will direct mlflow to write to it. Also you can start mlflow ui manually via: 1 mlflow ui --host localhost --port 9000","title":"Starting MLflow Server"},{"location":"tutorials/mlflow/#structure","text":"By default, Sapsan will keep the following structure in MLflow: Train 1 Evaluate 1 Evaluate 2 Train 2 Evaluate 1 Evaluate 2 where all evaluation runs are nested under the trained run entry. This way all evaluations are grouped together under the model that was just trained. Every Train checks for other active runs, terminates them, and starts a new run. At the end of the Train method, the run does not terminate, awaiting Evaluate runs to be nested under it. Thus, Evaluate runs start and end at the end of the method. However, one can still add extra metrics, artifacts and etc by resuming the previously closed run and writing to it, as discussed in the later section .","title":"Structure"},{"location":"tutorials/mlflow/#tracked-parameters","text":"Evaluation runs include the training model parameters and metrics to make it easier to parse through. Here is a complete list of what is tracked by default after running Train or Evaluate loop. Parameter Train Evaluate Everything passed to ModelConfig() (including new parameters passed to kwargs ) model - {parameter} - device, logdir, lr, min_delta, min_lr, n_epochs, patience data - {parameter} - features, features_label, target, target_label, axis, path, path, shuffle chkpnt - {parameter} - initial_size, sample to size, batch_size, batch_num, time, time_granularity Since Train metrics are recorded for Evaluate runs, they are prefixed as train - {metric} . Subsequently, all Evaluate metrics are written as eval - {metric} Metrics Train Evaluate eval - MSE Loss - Mean Squared Error (if the target is provided) eval - KS Stat - Kolmogorov-Smirnov Statistic (if the target is provided) train - final epoch - final training epoch All model metrics model.metrics() (provided by Catalyst and PyTorch) Runtime Artifacts Train Evaluate model_details.txt - model layer init & optimizer settings model_forward.txt - Model.forward() function runtime_log.html - loss vs. epoch training progress pdf_cdf.png - Probability Density Function (PDF) & Cummulative Distribution Function (CDF) plots slices_plot.png - 2D Spatial Distribution (slice snapshots)","title":"Tracked Parameters"},{"location":"tutorials/mlflow/#adding-extra-parameters","text":"","title":"Adding extra parameters"},{"location":"tutorials/mlflow/#before-training","text":"In order to add a new_parameter to be tracked with MLflow per your run, simply pass it to config as such: ModelConfig(new_parameter=value) . Since it will be initialized under ModelConfig().kwargs['new_parameter'] , the parameter name can be anything. You will see it in MLflow recorded as model - new_parameter . Internally, everything in ModelConfig().parameters gets recorded to MLflow. By default, all ModelConfig() variables, including kwargs are passed to it. Here is the implementation from the CNN3d estimator . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CNN3dConfig ( EstimatorConfig ): def __init__ ( self , n_epochs : int = 1 , patience : int = 10 , min_delta : float = 1e-5 , logdir : str = \"./logs/\" , lr : float = 1e-3 , min_lr = None , * args , ** kwargs ): self . n_epochs = n_epochs self . logdir = logdir self . patience = patience self . min_delta = min_delta self . lr = lr if min_lr == None : self . min_lr = lr * 1e-2 else : self . min_lr = min_lr self . kwargs = kwargs #everything in self.parameters will get recorded by MLflow #by default, all 'self' variables will get recorded self . parameters = { f 'model - { k } ' : v for k , v in self . __dict__ . items () if k != 'kwargs' } if bool ( self . kwargs ): self . parameters . update ({ f 'model - { k } ' : v for k , v in self . kwargs . items ()}) Note: MLflow doesn't like labels that contain / symbol. Please avoid or you might encounter an error.","title":"Before Training"},{"location":"tutorials/mlflow/#after-training-or-evaluation","text":"If you want to perform some extra analysis on your model or predictions and record additional metrics after you have called Train.run() or Evaluation.run() , Sapsan has an interface to do so in 3 steps : Resume MLflow run Since MLflow run is closed at the end of Evaluation.run() , it will need to be resumed first before attempting to write to it. For that reason, both Train and Evaluate classes have a parameter run_id which contains the MLflow run_id. You can use it to resume the run, and record new metrics. Record new parameters To add extra parameters to the most recent Train or Evaluate entry in MLflow, simply use either the backend() interface or the standard MLflow interface. End the run In order to keep MLflow tidy, it is advised to call backend.end() after you are done. 1 2 3 4 5 6 7 8 9 eval = Evaluate ( ... ) cubes = eval . run () #do something with the prediction and/or target cube new_metric = np . amax ( cubes [ 'pred_cube' ] / cubes [ 'target_cube' ]) backend . resume ( run_id = eval . run_id ) backend . log_metric ( 'new_metric' , new_metric ) #or use backend.log_parameter() or backend.log_artifact() backend . end () Feel free to review the full API Reference: Backend (Tracking) for the full description of MLflow-related functions built into Sapsan.","title":"After Training or Evaluation"},{"location":"tutorials/model_graph/","text":"Model Graph How to construct a graph of the model This a page describing in detail how to construct nice-looking graphs of your model automatically. Bug Temporarily doesn't work due to a compatability issue between ONNX and PyTorch>=1.12.0 Example 1 2 3 4 5 6 7 8 9 10 11 12 from sapsan.lib.estimator.cnn.cnn3d_estimator import CNN3d , CNN3dConfig from sapsan.utils.plot import model_graph from sapsan.lib.data import get_loader_shape # load your data into loaders estimator = CNN3d ( config = CNN3dConfig (), loaders = loaders ) shape_x , shape_y = get_loader_shape ( loaders ) model_graph ( model = estimator . model , shape = shape_x ) Considering that shape_x = (8,1,8,8,8) , the following graph will be produced: Details shape of the input data is in the format [N, C in , D b , H b , W b ]. You can either grab it from the loader as shown above or provide your own, as long as the number of channels C in matches the data your model was initialized with. transforms allow you to adjust the graph to your liking. For example, they can allow you to combine layers to be displayed in a single box, instead of separate. Please refer to the API of model_graph to see what options are available for transformations. Limitations model input param must be a PyTorch, TensorFlow, or Keras-with-TensorFlow-backend model. API for model_graph sapsan.utils.plot.model_graph(<i>model, shape: np.array, transforms</i>) Creates a graph of the ML model (needs graphviz to be installed). The method is based on hiddenlayer originally written by Waleed Abdulla. Parameters model (object) - initialized pytorch or tensorflow model shape (np.array) - shape of the input array in the form [N, C in , D b , H b , W b ], where C in =1 transforms (list[methods]) - a list of hiddenlayer transforms to be applied ( Fold, FoldId, Prune, PruneBranch, FoldDuplicates, Rename ). Default: 1 2 3 4 5 6 7 8 9 10 11 12 > import sapsan.utils.hiddenlayer as hl > transforms = [ hl . transforms . Fold ( \"Conv > MaxPool > Relu\" , \"ConvPoolRelu\" ), hl . transforms . Fold ( \"Conv > MaxPool\" , \"ConvPool\" ), hl . transforms . Prune ( \"Shape\" ), hl . transforms . Prune ( \"Constant\" ), hl . transforms . Prune ( \"Gather\" ), hl . transforms . Prune ( \"Unsqueeze\" ), hl . transforms . Prune ( \"Concat\" ), hl . transforms . Rename ( \"Cast\" , to = \"Input\" ), hl . transforms . FoldDuplicates () ] Return graph of a model Return type graphviz.Digraph object","title":"Model Graph"},{"location":"tutorials/model_graph/#model-graph","text":"","title":"Model Graph"},{"location":"tutorials/model_graph/#how-to-construct-a-graph-of-the-model","text":"This a page describing in detail how to construct nice-looking graphs of your model automatically. Bug Temporarily doesn't work due to a compatability issue between ONNX and PyTorch>=1.12.0","title":"How to construct a graph of the model"},{"location":"tutorials/model_graph/#example","text":"1 2 3 4 5 6 7 8 9 10 11 12 from sapsan.lib.estimator.cnn.cnn3d_estimator import CNN3d , CNN3dConfig from sapsan.utils.plot import model_graph from sapsan.lib.data import get_loader_shape # load your data into loaders estimator = CNN3d ( config = CNN3dConfig (), loaders = loaders ) shape_x , shape_y = get_loader_shape ( loaders ) model_graph ( model = estimator . model , shape = shape_x ) Considering that shape_x = (8,1,8,8,8) , the following graph will be produced:","title":"Example"},{"location":"tutorials/model_graph/#details","text":"shape of the input data is in the format [N, C in , D b , H b , W b ]. You can either grab it from the loader as shown above or provide your own, as long as the number of channels C in matches the data your model was initialized with. transforms allow you to adjust the graph to your liking. For example, they can allow you to combine layers to be displayed in a single box, instead of separate. Please refer to the API of model_graph to see what options are available for transformations.","title":"Details"},{"location":"tutorials/model_graph/#limitations","text":"model input param must be a PyTorch, TensorFlow, or Keras-with-TensorFlow-backend model.","title":"Limitations"},{"location":"tutorials/model_graph/#api-for-model_graph","text":"sapsan.utils.plot.model_graph(<i>model, shape: np.array, transforms</i>) Creates a graph of the ML model (needs graphviz to be installed). The method is based on hiddenlayer originally written by Waleed Abdulla. Parameters model (object) - initialized pytorch or tensorflow model shape (np.array) - shape of the input array in the form [N, C in , D b , H b , W b ], where C in =1 transforms (list[methods]) - a list of hiddenlayer transforms to be applied ( Fold, FoldId, Prune, PruneBranch, FoldDuplicates, Rename ). Default: 1 2 3 4 5 6 7 8 9 10 11 12 > import sapsan.utils.hiddenlayer as hl > transforms = [ hl . transforms . Fold ( \"Conv > MaxPool > Relu\" , \"ConvPoolRelu\" ), hl . transforms . Fold ( \"Conv > MaxPool\" , \"ConvPool\" ), hl . transforms . Prune ( \"Shape\" ), hl . transforms . Prune ( \"Constant\" ), hl . transforms . Prune ( \"Gather\" ), hl . transforms . Prune ( \"Unsqueeze\" ), hl . transforms . Prune ( \"Concat\" ), hl . transforms . Rename ( \"Cast\" , to = \"Input\" ), hl . transforms . FoldDuplicates () ] Return graph of a model Return type graphviz.Digraph object","title":"API for model_graph"},{"location":"tutorials/parallelgpu/","text":"Parallel GPU Training Automatic Sapsan relies on Catalyst to implement Distributed Data Parallel (DDP) . You can specify ddp=True in ModelConfig , which in turn will set ddp=True parameter for the Catalyst runner.train() . Let's take a look at how it could be done by adjusting cnn_example : cnn_example.ipynb 1 2 3 4 5 estimator = CNN3d ( config = CNN3dConfig ( n_epochs = 5 , patience = 10 , min_delta = 1e-5 , ddp = True ), loaders = loaders ) DDP is not supported on Jupyter Notebooks! You will have to prepare a script. Thus, it is advised to start off developing and testing your model on a single CPU or GPU in a jupyter notebook, then downloading it as a python script to run on multiple GPUs locally or on HPC. In addition, you will have to add the following statement to the beginning of your script in order for torch.multiprocessing to work correctly: 1 if __name__ == '__main__' : Even though Training will be performed on the GPUs, evaluation will be done on the CPU. Customizing For more information and further customization of your parallel setup, see DDP Tutorial from Catalyst . It might come in useful if you want, among other things, to take control over what portion of the data is copied onto which node. The runner itself, torch_backend.py , can be copied to the project directory and accessed when creating a new project by invoking --get_torch_backend or --gtb flag as such: 1 sapsan create --gtb -n {name} The torch_backend.py contains lots of important functions, but for customizing DDP you will need to focus on TorchBackend.torch_train() as shown below. Most likely you will need to adjust self.runner to either another Catalyst runner or your own, custom runner. Next, you will need to edit self.runner.train() parameters accordingly. torch_backend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class TorchBackend ( Estimator ): def __init__ ( self , config : EstimatorConfig , model ): super () . __init__ ( config ) self . runner = SupervisedRunner () # (1) . . . def torch_train ( self ): . . . self . runner . train ( model = model , criterion = self . loss_func , optimizer = self . optimizer , scheduler = self . scheduler , loaders = loaders , logdir = self . config . logdir , num_epochs = self . config . n_epochs , callbacks = [ EarlyStoppingCallback ( patience = self . config . patience , min_delta = self . config . min_delta , loader_key = self . loader_key , metric_key = self . metric_key , minimize = True ), SchedulerCallback ( loader_key = self . loader_key , metric_key = self . metric_key ,), SkipCheckpointCallback ( logdir = self . config . logdir ) ], verbose = False , check = False , engine = DeviceEngine ( self . device ), ddp = self . ddp # (2) ) Adjust the Runner here. Check Catalyst's documentation Controls automatic Distributed Data Parallel (DDP)","title":"Parallel GPU Training"},{"location":"tutorials/parallelgpu/#parallel-gpu-training","text":"","title":"Parallel GPU Training"},{"location":"tutorials/parallelgpu/#automatic","text":"Sapsan relies on Catalyst to implement Distributed Data Parallel (DDP) . You can specify ddp=True in ModelConfig , which in turn will set ddp=True parameter for the Catalyst runner.train() . Let's take a look at how it could be done by adjusting cnn_example : cnn_example.ipynb 1 2 3 4 5 estimator = CNN3d ( config = CNN3dConfig ( n_epochs = 5 , patience = 10 , min_delta = 1e-5 , ddp = True ), loaders = loaders ) DDP is not supported on Jupyter Notebooks! You will have to prepare a script. Thus, it is advised to start off developing and testing your model on a single CPU or GPU in a jupyter notebook, then downloading it as a python script to run on multiple GPUs locally or on HPC. In addition, you will have to add the following statement to the beginning of your script in order for torch.multiprocessing to work correctly: 1 if __name__ == '__main__' : Even though Training will be performed on the GPUs, evaluation will be done on the CPU.","title":"Automatic"},{"location":"tutorials/parallelgpu/#customizing","text":"For more information and further customization of your parallel setup, see DDP Tutorial from Catalyst . It might come in useful if you want, among other things, to take control over what portion of the data is copied onto which node. The runner itself, torch_backend.py , can be copied to the project directory and accessed when creating a new project by invoking --get_torch_backend or --gtb flag as such: 1 sapsan create --gtb -n {name} The torch_backend.py contains lots of important functions, but for customizing DDP you will need to focus on TorchBackend.torch_train() as shown below. Most likely you will need to adjust self.runner to either another Catalyst runner or your own, custom runner. Next, you will need to edit self.runner.train() parameters accordingly. torch_backend.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class TorchBackend ( Estimator ): def __init__ ( self , config : EstimatorConfig , model ): super () . __init__ ( config ) self . runner = SupervisedRunner () # (1) . . . def torch_train ( self ): . . . self . runner . train ( model = model , criterion = self . loss_func , optimizer = self . optimizer , scheduler = self . scheduler , loaders = loaders , logdir = self . config . logdir , num_epochs = self . config . n_epochs , callbacks = [ EarlyStoppingCallback ( patience = self . config . patience , min_delta = self . config . min_delta , loader_key = self . loader_key , metric_key = self . metric_key , minimize = True ), SchedulerCallback ( loader_key = self . loader_key , metric_key = self . metric_key ,), SkipCheckpointCallback ( logdir = self . config . logdir ) ], verbose = False , check = False , engine = DeviceEngine ( self . device ), ddp = self . ddp # (2) ) Adjust the Runner here. Check Catalyst's documentation Controls automatic Distributed Data Parallel (DDP)","title":"Customizing"},{"location":"tutorials/savenload/","text":"Save & Load Models All estimators in Sapsan depend either on torch_backend or sklearn_backend , depending on the model architecture. Both backends have save and load functions. Thus, no matter whether you are using included estimators or designing your own, both methods will be available. Loaded models can be used either for evaluation or to continue training. In the case of the latter either old or new config parameters can be set. Saving the Model To save the model, call: 1 estimator . save ( path = { save_path }) For PyTorch, the states of the model and optimizer will be saved, along with the last epoch and loss in {save_path}/model.pt . The config parameters will be saved in {save_path}/params.json For Sklearn, only the model itself will be saved, in {save_path}/model.pt Loading the Model Even though all Sapsan estimators have load method, you can use a dummy estimator to load your model. PyTorch Import load_estimator() to load your PyTorch model. You can pass new ModelConfig() parameters as well if you intend to continue training your model. 1 2 3 4 5 6 7 from sapsan.lib.estimator import load_estimator estimator = CNN3d ( config = CNN3dConfig ( n_epoch = 100 ), loaders = loaders ) loaded_estimator = load_estimator . load ({ path_to_model }, estimator = estimator ) Sklearn Sklearn uses a different interface, so you will need to call load_sklearn_estimator() 1 2 3 4 5 6 7 from sapsan.lib.estimator import load_sklearn_estimator estimator = KRR ( config = KRRConfig (), loaders = loaders ) loaded_estimator = load_sklearn . estimator . load ({ path_to_model }, estimator = estimator )","title":"Save & Load Models"},{"location":"tutorials/savenload/#save-load-models","text":"All estimators in Sapsan depend either on torch_backend or sklearn_backend , depending on the model architecture. Both backends have save and load functions. Thus, no matter whether you are using included estimators or designing your own, both methods will be available. Loaded models can be used either for evaluation or to continue training. In the case of the latter either old or new config parameters can be set.","title":"Save &amp; Load Models"},{"location":"tutorials/savenload/#saving-the-model","text":"To save the model, call: 1 estimator . save ( path = { save_path }) For PyTorch, the states of the model and optimizer will be saved, along with the last epoch and loss in {save_path}/model.pt . The config parameters will be saved in {save_path}/params.json For Sklearn, only the model itself will be saved, in {save_path}/model.pt","title":"Saving the Model"},{"location":"tutorials/savenload/#loading-the-model","text":"Even though all Sapsan estimators have load method, you can use a dummy estimator to load your model.","title":"Loading the Model"},{"location":"tutorials/savenload/#pytorch","text":"Import load_estimator() to load your PyTorch model. You can pass new ModelConfig() parameters as well if you intend to continue training your model. 1 2 3 4 5 6 7 from sapsan.lib.estimator import load_estimator estimator = CNN3d ( config = CNN3dConfig ( n_epoch = 100 ), loaders = loaders ) loaded_estimator = load_estimator . load ({ path_to_model }, estimator = estimator )","title":"PyTorch"},{"location":"tutorials/savenload/#sklearn","text":"Sklearn uses a different interface, so you will need to call load_sklearn_estimator() 1 2 3 4 5 6 7 from sapsan.lib.estimator import load_sklearn_estimator estimator = KRR ( config = KRRConfig (), loaders = loaders ) loaded_estimator = load_sklearn . estimator . load ({ path_to_model }, estimator = estimator )","title":"Sklearn"}]}